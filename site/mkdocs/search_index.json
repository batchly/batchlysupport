{
    "docs": [
        {
            "location": "/", 
            "text": "OVERVIEW\n\n\nWelcome to the documentation for Batchly - automated AWS cost reduction platform. This website aims to document every feature of Batchly from top-to-bottom, covering as much detail as possible. If you are just getting started with Batchly, it is highly recommended that you start with the \nGetting Started\n guide first and then return to this page.\n\n\nThe navigation will take you through each component of Batchly. Click on a navigation item to get started, or read more about why developers, executives and operators choose Batchly for their needs.", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "Welcome to the documentation for Batchly - automated AWS cost reduction platform. This website aims to document every feature of Batchly from top-to-bottom, covering as much detail as possible. If you are just getting started with Batchly, it is highly recommended that you start with the  Getting Started  guide first and then return to this page.  The navigation will take you through each component of Batchly. Click on a navigation item to get started, or read more about why developers, executives and operators choose Batchly for their needs.", 
            "title": "OVERVIEW"
        }, 
        {
            "location": "/about/whybatchly/", 
            "text": "Why Batchly?\n\n\nBatch.ly is a solution that fully automates AWS cost management (using spot instances and instance rightsizing using autoscaling) in a way that makes the ROI impossible to ignore.\n\n\nAWS has 11 regions with 2 to 3 AZ\u2019s in each region and 50 instance types and many instance categories (like Spot, Reserved, On-demand). Identifying the right infrastructure for your application/s from these options is a non trivial activity. It is complex, costly and most times suboptimal.\n\n\n\n\nBatchly takes out all the complexities associated with Cloud management by automating all the activities resulting in up to 90% reduction in AWS costs. \n\n\n\n\nWhy Batchly Benefits You\n\n\nBatchly enables both internet and traditional enterprises to automatically benefit from AWS cost and usage savings by optimizing workloads with spot instances and EC2 smart sizing. No matter what you use AWS for, Batchly helps you to reduce your cost (by up to 90%) in a frictionless manner.\n\n\nFeatures that customers love:\n\n\n\n\nAWS Spot Bid Optimization Seamlessly deploy, move, and manage workloads over high cost savings spot instances across any AWS region.\n\n\nEC2 Instance Smart Sizing Realize ultimate EC2 efficiencies with the automatic instance sizing for the right application / workload.\n\n\nSeamless Integration Easily work with pre-built apps for most common enterprise applications (JMeter, Spark, Hive, PIG, Jenkins, Elastic BeanStalk and more) and REST API support provides a seamless experience.\n\n\nFully Managed Benefit from cost and utilization optimizations from within your own AWS account while we do the heavy lifting.\n\n\nSecure We solved security with organic and strong AWS security controls.", 
            "title": "Why Batchly"
        }, 
        {
            "location": "/about/whybatchly/#why-batchly", 
            "text": "Batch.ly is a solution that fully automates AWS cost management (using spot instances and instance rightsizing using autoscaling) in a way that makes the ROI impossible to ignore.  AWS has 11 regions with 2 to 3 AZ\u2019s in each region and 50 instance types and many instance categories (like Spot, Reserved, On-demand). Identifying the right infrastructure for your application/s from these options is a non trivial activity. It is complex, costly and most times suboptimal.   Batchly takes out all the complexities associated with Cloud management by automating all the activities resulting in up to 90% reduction in AWS costs.", 
            "title": "Why Batchly?"
        }, 
        {
            "location": "/about/whybatchly/#why-batchly-benefits-you", 
            "text": "Batchly enables both internet and traditional enterprises to automatically benefit from AWS cost and usage savings by optimizing workloads with spot instances and EC2 smart sizing. No matter what you use AWS for, Batchly helps you to reduce your cost (by up to 90%) in a frictionless manner.  Features that customers love:   AWS Spot Bid Optimization Seamlessly deploy, move, and manage workloads over high cost savings spot instances across any AWS region.  EC2 Instance Smart Sizing Realize ultimate EC2 efficiencies with the automatic instance sizing for the right application / workload.  Seamless Integration Easily work with pre-built apps for most common enterprise applications (JMeter, Spark, Hive, PIG, Jenkins, Elastic BeanStalk and more) and REST API support provides a seamless experience.  Fully Managed Benefit from cost and utilization optimizations from within your own AWS account while we do the heavy lifting.  Secure We solved security with organic and strong AWS security controls.", 
            "title": "Why Batchly Benefits You"
        }, 
        {
            "location": "/user-guide/getting-started/", 
            "text": "Getting Started\n\n\nAdd AWS Account\n\n\nClick the Accounts link in the left sidebar. Here you provide a name for your account, specify the default AWS region you want to operate in and the access and secret keys for your account. (Before adding an account customer need to have their own AWS account.)\n\n\nThe following steps will demonstrate how to add an account:\n\n\n1.Click the \nAccounts\n link in the left sidebar.\n\n\n\n\n2.\nAccount Type:\n Add a new AWS account to batchly.\n\n\n\n\n3.\nAccount Name:\n Select a name to identify the account.\n\n\n4.\nDefault Region:\n Specify the default AWS region you want to operate in.\n\n\n\n\n5.\nAccess Key:\n Provide the AWS Access key.\n\n\n6.\nSecret Key:\n Provide the AWS Secret key of the corresponding Access Key.    \n\n\n7.\nAccount Manager:\n Assign a project manager for the project.\n\n\n\n\nCreate new project\n\n\nThe projects section lets you add a project under an AWS account. In the following steps will show how to add a new project to Batchly:\n\n\n1.Select \nproject\n link in left sidebar and Click on \nAdd New Project\n.\n\n\n\n\n2.\nAccount:\n Identifier of the Account in which the project resides.\n\n\n  \n\n\n3.\nName:\n Enter the Name of project.\n\n\n \n\n\n4.You can ensure that it runs within your VPC by clicking the Enable VPC checkbox and assign it to team member from this section\n\n\n  \n\n\n5.\nVPC Region:\n Set the AWS region where project need to be run.\n\n\n  \n\n\n6.\nVPC Id:\n Enter the VPC Id.\n\n\n  \n\n\n7.\nSubnet Id:\n Enter the subnet Id.\n\n\n8.\nProject Manager:\n Assign a project manager for the project.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/getting-started/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/getting-started/#add-aws-account", 
            "text": "Click the Accounts link in the left sidebar. Here you provide a name for your account, specify the default AWS region you want to operate in and the access and secret keys for your account. (Before adding an account customer need to have their own AWS account.)  The following steps will demonstrate how to add an account:  1.Click the  Accounts  link in the left sidebar.   2. Account Type:  Add a new AWS account to batchly.   3. Account Name:  Select a name to identify the account.  4. Default Region:  Specify the default AWS region you want to operate in.   5. Access Key:  Provide the AWS Access key.  6. Secret Key:  Provide the AWS Secret key of the corresponding Access Key.      7. Account Manager:  Assign a project manager for the project.", 
            "title": "Add AWS Account"
        }, 
        {
            "location": "/user-guide/getting-started/#create-new-project", 
            "text": "The projects section lets you add a project under an AWS account. In the following steps will show how to add a new project to Batchly:  1.Select  project  link in left sidebar and Click on  Add New Project .   2. Account:  Identifier of the Account in which the project resides.      3. Name:  Enter the Name of project.     4.You can ensure that it runs within your VPC by clicking the Enable VPC checkbox and assign it to team member from this section      5. VPC Region:  Set the AWS region where project need to be run.      6. VPC Id:  Enter the VPC Id.      7. Subnet Id:  Enter the subnet Id.  8. Project Manager:  Assign a project manager for the project.", 
            "title": "Create new project"
        }, 
        {
            "location": "/user-guide/creating-jobs/", 
            "text": "Creating Jobs\n\n\nJobs section is where you define your job. Creating a job is a simple three step processes. In the following three steps we will show how to add a new job in Batchly.\n\n\nCode (Upload your code):\n\n\nIn the code section first You need to build your packaged code and upload the zipped file.\n\n\n1.Select the \nJobs\n link in the left sidebar.\n\n\n\n\n2.\nJob Name:\n Enter the Job name and Project name under which the job should execute.\n\n\n\n\n3.\nProcessor:\n Package the code which includes business logic of your job, zip it and upload it.\n\n\n\n\n\n\n4.\nOperating System:\n specify which operating system you want it to run.\n\n\n\n\n5.\nPackage Type:\n Enter the package type of your application.\n\n\n\n\n6.\nCode Language:\n Select the language of your application.\n\n\n\n\n7.\nFile Name:\n Enter the package name or file name that contains your application.\n\n\nData Sources (Detail Input and Output location):\n\n\nIn this section you have to define the input and output data sources location. In Batchly we have following four request type.\n\n\n1.\nRequest type:\n Select the request type of the job.\n\n\n\n\n2.\nInput data Source:\n Select the correspoding data source of the job.\n\n\n3.\nResponse Type:\n Select the response type of the job.\n\n\nSLA (Setup Processing parameters):\n\n\n1.\nOperation mode:\n  Select the operation mode you want to run.\n\n\n\n\n2.\nRegion:\n Select the region where the job should execute.\n\n\n\n\n3.\nRun Type:\n Select the Demo mode to operate the run type.\n\n\n\n\n4.\nParameters:\n Add key-value pairs for configuring your processor. Add parameters will ask you the add the parameter name and parameter                          value.", 
            "title": "Creating Joule Jobs"
        }, 
        {
            "location": "/user-guide/creating-jobs/#creating-jobs", 
            "text": "Jobs section is where you define your job. Creating a job is a simple three step processes. In the following three steps we will show how to add a new job in Batchly.", 
            "title": "Creating Jobs"
        }, 
        {
            "location": "/user-guide/creating-jobs/#code-upload-your-code", 
            "text": "In the code section first You need to build your packaged code and upload the zipped file.  1.Select the  Jobs  link in the left sidebar.   2. Job Name:  Enter the Job name and Project name under which the job should execute.   3. Processor:  Package the code which includes business logic of your job, zip it and upload it.    4. Operating System:  specify which operating system you want it to run.   5. Package Type:  Enter the package type of your application.   6. Code Language:  Select the language of your application.   7. File Name:  Enter the package name or file name that contains your application.", 
            "title": "Code (Upload your code):"
        }, 
        {
            "location": "/user-guide/creating-jobs/#data-sources-detail-input-and-output-location", 
            "text": "In this section you have to define the input and output data sources location. In Batchly we have following four request type.  1. Request type:  Select the request type of the job.   2. Input data Source:  Select the correspoding data source of the job.  3. Response Type:  Select the response type of the job.", 
            "title": "Data Sources (Detail Input and Output location):"
        }, 
        {
            "location": "/user-guide/creating-jobs/#sla-setup-processing-parameters", 
            "text": "1. Operation mode:   Select the operation mode you want to run.   2. Region:  Select the region where the job should execute.   3. Run Type:  Select the Demo mode to operate the run type.   4. Parameters:  Add key-value pairs for configuring your processor. Add parameters will ask you the add the parameter name and parameter                          value.", 
            "title": "SLA (Setup Processing parameters):"
        }, 
        {
            "location": "/user-guide/account-management/", 
            "text": "Add AWS Account\n\n\nBefore adding an account customer need to have their own AWS account. The following steps will demonstrate how to add an account:\n\n\n1.Click the \nAccounts\n link in the left sidebar.\n\n\n\n\n2.\nAccount Type:\n Add a new AWS account to batchly.\n\n\n\n\n3.\nAccount Name:\n A name to identify the account.\n\n\n4.\nDefault Region:\n Specify the default AWS region you want to operate in.\n\n\n\n\n5.\nAccess Key:\n Provide the AWS Access key.\n\n\n6.\nSecret Key:\n Provide the AWS Secret key of the corresponding Access Key.    \n\n\n7.\nAccount Manager:\n Assign a project manager for the project.", 
            "title": "Account Management"
        }, 
        {
            "location": "/user-guide/account-management/#add-aws-account", 
            "text": "Before adding an account customer need to have their own AWS account. The following steps will demonstrate how to add an account:  1.Click the  Accounts  link in the left sidebar.   2. Account Type:  Add a new AWS account to batchly.   3. Account Name:  A name to identify the account.  4. Default Region:  Specify the default AWS region you want to operate in.   5. Access Key:  Provide the AWS Access key.  6. Secret Key:  Provide the AWS Secret key of the corresponding Access Key.      7. Account Manager:  Assign a project manager for the project.", 
            "title": "Add AWS Account"
        }, 
        {
            "location": "/api/overview/", 
            "text": "Batchly REST API\n\n\nEach customer of batch.ly gets their own customized URL for access.  Using the web console, you can generate new set of API access keys.  You need to safe keep the keys.  Batchly API SDK doesn\u2019t send the secret key in any request.  The authentication mechanism employed on the server side is HMAC which validates calls using request signature.  Server endpoint support both the JSON and XML data formats.  The SDK is configured to use the JSON data format.\n\n\nBefore making calls to the API, setup the base URI, access key and secret key in the configuration.\n\n\nEach of the major endpoints are grouped based on the high level entities.  For each high level entity, a controller is available.  Initialize the controller and call the appropriate methods.  All the constant values are available as Enumerations so that magic values need not be remembered.\n\n\nTo call any of the methods, create a new instance of the appropriate controller. Call the respective methods by creating the appropriate request objects. The following example shows the basic usage of the API to get list of accounts.\n\n\nLanguage : ruby\n\nrequire 'batchly_api'\n\nBatchlyApi::Configuration . BASE_URI = 'YOUR CUSTOMIZED ENDPOINT URL'\nBatchlyApi::Configuration . API_KEY =   'YOUR ACCESS KEY'\nBatchlyApi::Configuration . API_SECRET =   'YOUR SECRET KEY'\n\nctrl =   BatchlyApi::AccountsController . new\nresponse = ctrl. list_accounts", 
            "title": "Overview"
        }, 
        {
            "location": "/api/overview/#batchly-rest-api", 
            "text": "Each customer of batch.ly gets their own customized URL for access.  Using the web console, you can generate new set of API access keys.  You need to safe keep the keys.  Batchly API SDK doesn\u2019t send the secret key in any request.  The authentication mechanism employed on the server side is HMAC which validates calls using request signature.  Server endpoint support both the JSON and XML data formats.  The SDK is configured to use the JSON data format.  Before making calls to the API, setup the base URI, access key and secret key in the configuration.  Each of the major endpoints are grouped based on the high level entities.  For each high level entity, a controller is available.  Initialize the controller and call the appropriate methods.  All the constant values are available as Enumerations so that magic values need not be remembered.  To call any of the methods, create a new instance of the appropriate controller. Call the respective methods by creating the appropriate request objects. The following example shows the basic usage of the API to get list of accounts.  Language : ruby\n\nrequire 'batchly_api'\n\nBatchlyApi::Configuration . BASE_URI = 'YOUR CUSTOMIZED ENDPOINT URL'\nBatchlyApi::Configuration . API_KEY =   'YOUR ACCESS KEY'\nBatchlyApi::Configuration . API_SECRET =   'YOUR SECRET KEY'\n\nctrl =   BatchlyApi::AccountsController . new\nresponse = ctrl. list_accounts", 
            "title": "Batchly REST API"
        }, 
        {
            "location": "/api/createnewjob/", 
            "text": "Job\n\n\nA job is a reusable template that brings all data and code together with runtime parameters. The job when created includes the default SLA and region for operation. For each execution, you can vary the SLA and region.\n\n\nFor jobs that use AMI based processor, operation region is same as AMI region. Similarly jobs that use VPC projects, the region is limited to the VPC region.\n\n\nCreate\n\n\nCreating a job is essentially a composition of multiple API calls.\n\n\n\n\n\n\nDefine Processor: \n\nYou can create a new processor as defined in the previous section.\n\n\n\n\n\n\nDefine Source:\n\nDepending on your data store, you can call the appropriate API endpoints to define the data source.\n\n\n\n\n\n\n-- \nAWS S3: \n\nTo define a new S3 location, you need to call the API with bucket name and folder.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nREST endpoint\n\n\nPOST /api/DataSources/S3\n\n\n\n\n\n\n SDK method \n\n\nDataSourcesController \u2013 AddS3\n\n\n\n\n\n\nRequest\n\n\nAddS3DataSourceRequest\n\n\n\n\n\n\nResponse\n\n\n201 Created String\n\n\n\n\n\n\n\n\n--- \nAdd S3 Data Source Request\n\nTo create a new request to add S3 storage as data source\n\n\n\n\n\n\n\n\nProperties\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBucket\n\n\nName of the S3 bucket\n\n\n\n\n\n\nFolder\n\n\nFolder within the S3 bucket\n\n\n\n\n\n\n\n\n--\n Define Destination:\n\nDepending on your data store, you can call the appropriate API endpoints to define the destination. For processors that return a default response, destination is not mandatory.\n\n\n--\n AWS S3: \n\nTo define a new S3 location, you need to call the API with bucket name and folder.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nREST endpoint\n\n\nPOST /api/DataSources/S3\n\n\n\n\n\n\n SDK method \n\n\nDataSourcesController \u2013 AddS3\n\n\n\n\n\n\nRequest\n\n\nAddS3DataSourceRequest\n\n\n\n\n\n\nResponse\n\n\n201 Created String\n\n\n\n\n\n\n\n\n\n\n Add S3 Data Source Request\n\nTo create a new request to add S3 storage as data source\n\n\n\n\n\n\n\n\n\n\nProperties\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBucket\n\n\nName of the S3 bucket\n\n\n\n\n\n\nFolder\n\n\nFolder within the S3 bucket\n\n\n\n\n\n\n\n\n\n\n\n\nDefine a job specific Parameter group\n\nThis is an optional step where you can define a new parameter group that uses different runtime parameters. The procedure to create a new parameter group is defined above.\n\n\n\n\n\n\nCompose Job\n\nAfter all the required entity identifiers are available, you can call the create job endpoint to create a new job.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nREST endpoint\n\n\nPOST /api/Jobs\n\n\n\n\n\n\nSDK method\n\n\nJobsController \u2013 AddJob\n\n\n\n\n\n\nRequest\n\n\nCreateJobRequest\n\n\n\n\n\n\nResponse\n\n\n201 Created JobModel\n\n\n\n\n\n\n\n\n-- \nCreate Job Request\n\nRequest to create a new job\n\n\n\n\n\n\n\n\nProperties\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nName of the job\n\n\n\n\n\n\nProject Id\n\n\nIdentifier of the project under which the job should execute\n\n\n\n\n\n\nProcessor Id\n\n\nIdentifier of the processor that processes the data\n\n\n\n\n\n\nData Source Id\n\n\nIdentifier of the Data Source from which data is extracted for processing\n\n\n\n\n\n\nDestination Id\n\n\nIdentifier to the Data Source into which processed data is sent\n\n\n\n\n\n\nSchedule\n\n\nSchedule information of Job if it should be managed by Scheduler\n\n\n\n\n\n\nRegion\n\n\nDefault Region in which the job should execute. You can change region for each run when calling \"Execute\" endpoint.\n\n\n\n\n\n\nSLA\n\n\nDefault SLA for the job execution. You can change SLA for each run when calling the \"Execute\" endpoint.\n\n\n\n\n\n\nParameters\n\n\nParameter name and values that should be sent to the processor. The values can be changed for each run when calling the \"Execute\" endpoint.", 
            "title": "Create New Job"
        }, 
        {
            "location": "/api/createnewjob/#job", 
            "text": "A job is a reusable template that brings all data and code together with runtime parameters. The job when created includes the default SLA and region for operation. For each execution, you can vary the SLA and region.  For jobs that use AMI based processor, operation region is same as AMI region. Similarly jobs that use VPC projects, the region is limited to the VPC region.", 
            "title": "Job"
        }, 
        {
            "location": "/api/createnewjob/#create", 
            "text": "Creating a job is essentially a composition of multiple API calls.    Define Processor:  \nYou can create a new processor as defined in the previous section.    Define Source: \nDepending on your data store, you can call the appropriate API endpoints to define the data source.    --  AWS S3:  \nTo define a new S3 location, you need to call the API with bucket name and folder.           REST endpoint  POST /api/DataSources/S3     SDK method   DataSourcesController \u2013 AddS3    Request  AddS3DataSourceRequest    Response  201 Created String     ---  Add S3 Data Source Request \nTo create a new request to add S3 storage as data source     Properties  Description      Bucket  Name of the S3 bucket    Folder  Folder within the S3 bucket     --  Define Destination: \nDepending on your data store, you can call the appropriate API endpoints to define the destination. For processors that return a default response, destination is not mandatory.  --  AWS S3:  \nTo define a new S3 location, you need to call the API with bucket name and folder.           REST endpoint  POST /api/DataSources/S3     SDK method   DataSourcesController \u2013 AddS3    Request  AddS3DataSourceRequest    Response  201 Created String       Add S3 Data Source Request \nTo create a new request to add S3 storage as data source      Properties  Description      Bucket  Name of the S3 bucket    Folder  Folder within the S3 bucket       Define a job specific Parameter group \nThis is an optional step where you can define a new parameter group that uses different runtime parameters. The procedure to create a new parameter group is defined above.    Compose Job \nAfter all the required entity identifiers are available, you can call the create job endpoint to create a new job.             REST endpoint  POST /api/Jobs    SDK method  JobsController \u2013 AddJob    Request  CreateJobRequest    Response  201 Created JobModel     --  Create Job Request \nRequest to create a new job     Properties  Description      Name  Name of the job    Project Id  Identifier of the project under which the job should execute    Processor Id  Identifier of the processor that processes the data    Data Source Id  Identifier of the Data Source from which data is extracted for processing    Destination Id  Identifier to the Data Source into which processed data is sent    Schedule  Schedule information of Job if it should be managed by Scheduler    Region  Default Region in which the job should execute. You can change region for each run when calling \"Execute\" endpoint.    SLA  Default SLA for the job execution. You can change SLA for each run when calling the \"Execute\" endpoint.    Parameters  Parameter name and values that should be sent to the processor. The values can be changed for each run when calling the \"Execute\" endpoint.", 
            "title": "Create"
        }, 
        {
            "location": "/usecases/ocr-processing/", 
            "text": "This section provides the necessary information for processing files using an OCR like Tesseract.\n\n\nData Source\n\n\nBatchly currently supports maintaining Image files in S3.  You can specify a bucket and a folder as data source.  Batchly enumerates the entire folder to get the list of files and begins processing.\n\n\nInterface Model\n\n\nTesseract has a nugget package for use within .Net systems which makes it extremely easy to build processors for Batchly. Here\u2019s the steps for creating a new processor\n\n\n\n\n\n\nCreate a new class library project\n\n\n\n\n\n\nUsing Package Manager, add the tesseract nuget package\n\n\n\n\n\n\nAdd reference to the Net.Batchly.Extension dll\n\n\n\n\n\n\nCreate a new class and implement the IProcessor Interface\n\n\n\n\n\n\nWrite how a single file is processed within the Process method.\n\n\n\n\n\n\nThe following image shows a simple processor for Batchly written using the tesseract engine.\n\n\n\nScript Model\n\n\nIf you wish to create a script for processing, Batchly supports the following method. Create a script that accept a json file that contains all the input information, invoke a library like Tesseract and send back a json response post processing.", 
            "title": "OCR Processing"
        }, 
        {
            "location": "/usecases/ocr-processing/#data-source", 
            "text": "Batchly currently supports maintaining Image files in S3.  You can specify a bucket and a folder as data source.  Batchly enumerates the entire folder to get the list of files and begins processing.", 
            "title": "Data Source"
        }, 
        {
            "location": "/usecases/ocr-processing/#interface-model", 
            "text": "Tesseract has a nugget package for use within .Net systems which makes it extremely easy to build processors for Batchly. Here\u2019s the steps for creating a new processor    Create a new class library project    Using Package Manager, add the tesseract nuget package    Add reference to the Net.Batchly.Extension dll    Create a new class and implement the IProcessor Interface    Write how a single file is processed within the Process method.    The following image shows a simple processor for Batchly written using the tesseract engine.", 
            "title": "Interface Model"
        }, 
        {
            "location": "/usecases/ocr-processing/#script-model", 
            "text": "If you wish to create a script for processing, Batchly supports the following method. Create a script that accept a json file that contains all the input information, invoke a library like Tesseract and send back a json response post processing.", 
            "title": "Script Model"
        }, 
        {
            "location": "/usecases/video-transcoding/", 
            "text": "How To Run Video Transcoding (FFMpeg) Jobs In AWS Using Batchly\n\n\nVideo processing (transcoding, encoding, transsizing) workloads are extremely resource hungry and often require high CPU server farms which are complex to run and maintain. Instead of making these capex investments which will only be utilized Batchly can be used to run these jobs as per schedule or on-demand at a fraction of the cost.\n\n\nThis article we look at how easy it is to setup and run your transcoding job on Batchly. For this example, we will encode 10 files of varying sizes (totalling 16 GB) into 720p HD resolution with an SLA of 7 hours (Side note: Core tenet of Batchly is the SLA model \u2013 You set the time, cost or both and Batchly strictly adheres to it)\n\n\nStep 0:\n For the purpose of this article, we assume the raw files to be in Amazon S3. If you have it in-house or in another hosting provider, it can be synced to S3 using scripts or third party file uploaders like Asphera which is outside the purview of this article.\n\n\nStep 1:\n Select \u201cNew Batchly Job\u201d in the topbar. Enter a job name, select a project and Batchly FFMpeg Processor. FFMpeg is a popular open source solution to resize \n convert a host of audio and video formats.\n\n\n\n\nStep 2:\n Enter the input and output sources. In this case, the files are present in \u201cbucket-name\u201d under a folder called \u201cfolder\u201d and outputs would be put into \u201coutput-folder\u201d.\n\n\n\n\nStep 3:\n Choose \u201cManual Operation\u201d or \u201cScheduled Operation\u201d. For this test, we will choose manual operation to start the job instantly. FFMpeg requires two arguments: OUTPUT_FORMAT and Arguments. Since we are converting the raw files to 720p MP4, use the following\n\n\n\n\nOUTPUT_FORMAT:\n .MP4\n\n\nArguments:\n -codec:v libx264 -crf 18 -b:v 1000k -maxrate 1000k -bufsize 1835k -vf scale=-2:720 -threads 0 -codec:a copy\n\n\nNote:\n The arguments variable internally sends the value to the ffmpeg command\n\n\nStep 4:\n Save and start the job.\n\n\nDuring the processing you can go back to the Monitor Run screen to check the progress. Once the run is completed, Batchly provides a comprehensive report about the run. This can be accessed by clicking on \u201cRuns\u201d in the left side bar and click the \u201cSummary\u201d button on the run you want to review.\n\n\nRun Summary", 
            "title": "Video Transcoding"
        }, 
        {
            "location": "/usecases/video-transcoding/#how-to-run-video-transcoding-ffmpeg-jobs-in-aws-using-batchly", 
            "text": "Video processing (transcoding, encoding, transsizing) workloads are extremely resource hungry and often require high CPU server farms which are complex to run and maintain. Instead of making these capex investments which will only be utilized Batchly can be used to run these jobs as per schedule or on-demand at a fraction of the cost.  This article we look at how easy it is to setup and run your transcoding job on Batchly. For this example, we will encode 10 files of varying sizes (totalling 16 GB) into 720p HD resolution with an SLA of 7 hours (Side note: Core tenet of Batchly is the SLA model \u2013 You set the time, cost or both and Batchly strictly adheres to it)  Step 0:  For the purpose of this article, we assume the raw files to be in Amazon S3. If you have it in-house or in another hosting provider, it can be synced to S3 using scripts or third party file uploaders like Asphera which is outside the purview of this article.  Step 1:  Select \u201cNew Batchly Job\u201d in the topbar. Enter a job name, select a project and Batchly FFMpeg Processor. FFMpeg is a popular open source solution to resize   convert a host of audio and video formats.   Step 2:  Enter the input and output sources. In this case, the files are present in \u201cbucket-name\u201d under a folder called \u201cfolder\u201d and outputs would be put into \u201coutput-folder\u201d.   Step 3:  Choose \u201cManual Operation\u201d or \u201cScheduled Operation\u201d. For this test, we will choose manual operation to start the job instantly. FFMpeg requires two arguments: OUTPUT_FORMAT and Arguments. Since we are converting the raw files to 720p MP4, use the following   OUTPUT_FORMAT:  .MP4  Arguments:  -codec:v libx264 -crf 18 -b:v 1000k -maxrate 1000k -bufsize 1835k -vf scale=-2:720 -threads 0 -codec:a copy  Note:  The arguments variable internally sends the value to the ffmpeg command  Step 4:  Save and start the job.  During the processing you can go back to the Monitor Run screen to check the progress. Once the run is completed, Batchly provides a comprehensive report about the run. This can be accessed by clicking on \u201cRuns\u201d in the left side bar and click the \u201cSummary\u201d button on the run you want to review.", 
            "title": "How To Run Video Transcoding (FFMpeg) Jobs In AWS Using Batchly"
        }, 
        {
            "location": "/usecases/video-transcoding/#run-summary", 
            "text": "", 
            "title": "Run Summary"
        }
    ]
}