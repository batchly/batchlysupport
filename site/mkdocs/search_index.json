{
    "docs": [
        {
            "location": "/", 
            "text": "OVERVIEW\n\n\nWelcome to the documentation for Batchly - the automated AWS cost-reduction platform. This website aims to document every feature of Batchly from top to bottom, covering as much detail as possible. If you are just getting started with Batchly, it is highly recommended that you begin with the \nGetting Started\n guide first and then return to this page.\n\n\nThe navigation will take you through each component of Batchly. Click on a navigation item to get started, or read more about why developers, executives, and operators choose Batchly for their needs.\n\n\nTable of Contents\n\n\n\n\nOverview\n\n\nWhy Batchly\n\n\nBatchly User Guide\n\n\nGetting Started with Batchly\n\n\nUser Management in Batchly\n\n\nCreate IAM access in your AWS account for Batchly\n\n\nSecurity Policy\n\n\n\n\n\n\nApp Specific Support Pages\n\n\nBig Data Apps - Spark, Hadoop, Hive, Pig\n\n\nJMeter - Load Testing Framework\n\n\nFFMpeg - Video Transcoding Application\n\n\nImageMagick - Image Processing Application\n\n\nAuto Scaling\n \n\n\nElastic Load Balancer\n\n\n\n\n\n\nBatchly API Guide\n\n\nBatchly REST API Overview\n\n\nAPI Definitions for common tasks\n\n\nApp Specific API Definitions", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "Welcome to the documentation for Batchly - the automated AWS cost-reduction platform. This website aims to document every feature of Batchly from top to bottom, covering as much detail as possible. If you are just getting started with Batchly, it is highly recommended that you begin with the  Getting Started  guide first and then return to this page.  The navigation will take you through each component of Batchly. Click on a navigation item to get started, or read more about why developers, executives, and operators choose Batchly for their needs.", 
            "title": "OVERVIEW"
        }, 
        {
            "location": "/#table-of-contents", 
            "text": "Overview  Why Batchly  Batchly User Guide  Getting Started with Batchly  User Management in Batchly  Create IAM access in your AWS account for Batchly  Security Policy    App Specific Support Pages  Big Data Apps - Spark, Hadoop, Hive, Pig  JMeter - Load Testing Framework  FFMpeg - Video Transcoding Application  ImageMagick - Image Processing Application  Auto Scaling    Elastic Load Balancer    Batchly API Guide  Batchly REST API Overview  API Definitions for common tasks  App Specific API Definitions", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/whybatchly/", 
            "text": "Why Batchly?\n\n\nBatchly is a solution that fully automates AWS cost management (using spot instances and instance rightsizing using autoscaling) in a way that makes the ROI impossible to ignore.\n\n\nAWS has 11 regions with 2 to 3 AZs in each region, 50 instance types, and many instance categories (like Spot, Reserved, and On-demand). Identifying the right infrastructure for your application(s) from these options is not a trivial activity. It is complex, costly, and most times suboptimal.\n\n\nBatchly takes out all the complexities associated with Cloud management by automating all the activities, resulting in up to 90% reduction in AWS costs.\n\n\n\n\nBatchly takes out all the complexities associated with Cloud management by automating all the activities resulting in up to 90% reduction in AWS costs. \n\n\n\n\nWhy Batchly Benefits You\n\n\nBatchly enables both internet and traditional enterprises to automatically benefit from AWS cost and usage savings by optimizing workloads with spot instances and EC2 smart sizing. No matter what you use AWS for, Batchly helps you to reduce your cost (by up to 90%) in a frictionless manner.\n\n\nFeatures that customers love:\n\n\n\n\n\n\nAWS spot bid optimization seamlessly deploys, moves, and manages workloads over high cost savings spot instances across any AWS region.\n\n\n\n\n\n\nEC2 instance smart sizing realizes ultimate EC2 efficiencies with the automatic instance sizing for the right application / workload.\n\n\n\n\n\n\nSeamless integration easily works with pre-built apps for the most common enterprise applications (JMeter, Spark, Hive, PIG, Jenkins, Elastic BeanStalk, and more) and REST API support provides a smooth experience.\n\n\n\n\n\n\nFully managed benefit from cost and utilization optimizations from within your own AWS account while we do the heavy lifting.\n\n\n\n\n\n\nSecured with organic and strong AWS security controls.", 
            "title": "Why Batchly"
        }, 
        {
            "location": "/whybatchly/#why-batchly", 
            "text": "Batchly is a solution that fully automates AWS cost management (using spot instances and instance rightsizing using autoscaling) in a way that makes the ROI impossible to ignore.  AWS has 11 regions with 2 to 3 AZs in each region, 50 instance types, and many instance categories (like Spot, Reserved, and On-demand). Identifying the right infrastructure for your application(s) from these options is not a trivial activity. It is complex, costly, and most times suboptimal.  Batchly takes out all the complexities associated with Cloud management by automating all the activities, resulting in up to 90% reduction in AWS costs.   Batchly takes out all the complexities associated with Cloud management by automating all the activities resulting in up to 90% reduction in AWS costs.", 
            "title": "Why Batchly?"
        }, 
        {
            "location": "/whybatchly/#why-batchly-benefits-you", 
            "text": "Batchly enables both internet and traditional enterprises to automatically benefit from AWS cost and usage savings by optimizing workloads with spot instances and EC2 smart sizing. No matter what you use AWS for, Batchly helps you to reduce your cost (by up to 90%) in a frictionless manner.  Features that customers love:    AWS spot bid optimization seamlessly deploys, moves, and manages workloads over high cost savings spot instances across any AWS region.    EC2 instance smart sizing realizes ultimate EC2 efficiencies with the automatic instance sizing for the right application / workload.    Seamless integration easily works with pre-built apps for the most common enterprise applications (JMeter, Spark, Hive, PIG, Jenkins, Elastic BeanStalk, and more) and REST API support provides a smooth experience.    Fully managed benefit from cost and utilization optimizations from within your own AWS account while we do the heavy lifting.    Secured with organic and strong AWS security controls.", 
            "title": "Why Batchly Benefits You"
        }, 
        {
            "location": "/user-guide/getting-started/", 
            "text": "Getting Started\n\n\nTo get started with Batchly, go to the \nRegistration page\n and \ncreate a new Batchly Account\n.\n\n\nTo Register for Batchly, enter the following details:\n\n\n\n\n\n\nEnter your \nFull Name\n\n\n\n\n\n\nEnter your \nOrganization Name\n\n\n\n\n\n\nEnter your Organization Name or any desired name to set \ndomain name\n\n\n\n\n\n\nEnter your valid \nEmail Address\n\n\n\n\n\n\nEnter your valid \nPhone Number\n\n\n\n\n\n\nEnter your \nPassword\n\n\n\n\n\n\nClick \nCreate Account\n button.You may be asked to select the Captcha to let us know that you are human.\n\n\n\n\n\n\n\n\nFirst step after signing up - Check your Email\n\n\nAfter signing up, you will receive an email on your registered email address for verification. After verification completion, you can successfully sign in to Batchly.\n\n\nSign in Process - Wizard\n\n\nWhen you sign up for the very first time from your customized domain (for example : http:// yxz.batchly.net), you will go through the Wizard page. Here you will be asked to follow a few steps in order to set up everything, which you will be required to use on an app later.\n\n\nStep 1:\n Create an account.\n\n\nStep 2:\n Create a project.\n\n\nStep 3:\n Select any app.\n\n\n\n\nAdd AWS Account\n\n\nImportant:\n \n\n\n\n\n\n\nTo create an Account, Batchly requires your role \nAWS Role ARN\n. \n\n\n\n\n\n\nIn case \nAWS Role ARN\n already exists then you might not be able to create new \nAWS Role ARN\n.(You may use the same Role ARN or create a new one)\n\n\n\n\n\n\nTo use Batchly Apps, Account creation is mandatory. For that, click the \nAdd AWS Account\n and you will be redirected to another page where following details needs to be given to add an account.\n\n\nStep 1:\n Click on \nLaunch Stack\n to launch the CloudFormation template.\nIt will redirect you to the AWS website. (Please Log in to your AWS account if you're not logged in. )\n\n\n\n\nStep 2:\n After login into AWS account, you will be redirected to the CloudFormation page.Click on the \nNext\n Button in the \nSelect Template\n screen.\n\n\n\n\nStep 3:\n Click on the \nNext\n Button in the \nSpecify Details\n screen. \n\n\n\n\nStep 4:\n Click on the \nNext\n Button in the \nOptions\n screen.\n\n\n\n\nStep 5:\n Select both the checkboxes in the \nReview\n screen (it would give this CloudFormation template access to create IAM Resources) and Click the \nCreate\n button. A stack gets created.\n\n\n\n\nStep 6:\n Once the stack gets created, Go to \nOutputs\n tab and copy the Role ARN value and paste it in \nAWS Role ARN\n field in the Batchly's Account screen.\n\n\n\n\nStep 7:\n Enter Account Name.\n\n\nStep 8:\n Select the Region.\n\n\n\n\nStep 9:\n Cick on the button \nAdd Account\n button.\n\n\nNote:\n Before adding an account, the customer needs to have their own AWS account.\n\n\nAdd Project\n\n\nImportant:\n \n\n\n\n\n\n\nBefore adding a project, the customer needs to have their own AWS account.\n\n\n\n\n\n\nWhen you create an account, a default project will be created without VPC. So, creating a project is not mandatory. This can be done later.\n\n\n\n\n\n\nThe projects section lets you add a project under an AWS account. The following steps will show how to add a new project:\n\n\n\n\n\n\nEnter Project Name.\n\n\n\n\n\n\nSelect the Account under which you want to add the project.\n\n\n\n\n\n\nSelect the Cloud region.\n\n\n\n\n\n\nSelect the \nVirtual Private Cloud\n. (This is optional)\n\nNote:\n If you wish to work on \nJMeter\n app then this VPC will be mandatory.\n\n\n\n\n\n\nSelect the VPC information. (This needs to be done only if you have selected VPC)\n\n\n\n\n\n\nSelect the subnet information. (This needs to be done only if you have selected VPC)\n\n\n\n\n\n\n\n\nSelect App\n\n\nOnce you're done with adding account and project, you can select any app you want to work on.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/getting-started/#getting-started", 
            "text": "To get started with Batchly, go to the  Registration page  and  create a new Batchly Account .  To Register for Batchly, enter the following details:    Enter your  Full Name    Enter your  Organization Name    Enter your Organization Name or any desired name to set  domain name    Enter your valid  Email Address    Enter your valid  Phone Number    Enter your  Password    Click  Create Account  button.You may be asked to select the Captcha to let us know that you are human.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/getting-started/#first-step-after-signing-up-check-your-email", 
            "text": "After signing up, you will receive an email on your registered email address for verification. After verification completion, you can successfully sign in to Batchly.", 
            "title": "First step after signing up - Check your Email"
        }, 
        {
            "location": "/user-guide/getting-started/#sign-in-process-wizard", 
            "text": "When you sign up for the very first time from your customized domain (for example : http:// yxz.batchly.net), you will go through the Wizard page. Here you will be asked to follow a few steps in order to set up everything, which you will be required to use on an app later.  Step 1:  Create an account.  Step 2:  Create a project.  Step 3:  Select any app.", 
            "title": "Sign in Process - Wizard"
        }, 
        {
            "location": "/user-guide/getting-started/#add-aws-account", 
            "text": "Important:      To create an Account, Batchly requires your role  AWS Role ARN .     In case  AWS Role ARN  already exists then you might not be able to create new  AWS Role ARN .(You may use the same Role ARN or create a new one)    To use Batchly Apps, Account creation is mandatory. For that, click the  Add AWS Account  and you will be redirected to another page where following details needs to be given to add an account.  Step 1:  Click on  Launch Stack  to launch the CloudFormation template.\nIt will redirect you to the AWS website. (Please Log in to your AWS account if you're not logged in. )   Step 2:  After login into AWS account, you will be redirected to the CloudFormation page.Click on the  Next  Button in the  Select Template  screen.   Step 3:  Click on the  Next  Button in the  Specify Details  screen.    Step 4:  Click on the  Next  Button in the  Options  screen.   Step 5:  Select both the checkboxes in the  Review  screen (it would give this CloudFormation template access to create IAM Resources) and Click the  Create  button. A stack gets created.   Step 6:  Once the stack gets created, Go to  Outputs  tab and copy the Role ARN value and paste it in  AWS Role ARN  field in the Batchly's Account screen.   Step 7:  Enter Account Name.  Step 8:  Select the Region.   Step 9:  Cick on the button  Add Account  button.  Note:  Before adding an account, the customer needs to have their own AWS account.", 
            "title": "Add AWS Account"
        }, 
        {
            "location": "/user-guide/getting-started/#add-project", 
            "text": "Important:      Before adding a project, the customer needs to have their own AWS account.    When you create an account, a default project will be created without VPC. So, creating a project is not mandatory. This can be done later.    The projects section lets you add a project under an AWS account. The following steps will show how to add a new project:    Enter Project Name.    Select the Account under which you want to add the project.    Select the Cloud region.    Select the  Virtual Private Cloud . (This is optional) Note:  If you wish to work on  JMeter  app then this VPC will be mandatory.    Select the VPC information. (This needs to be done only if you have selected VPC)    Select the subnet information. (This needs to be done only if you have selected VPC)", 
            "title": "Add Project"
        }, 
        {
            "location": "/user-guide/getting-started/#select-app", 
            "text": "Once you're done with adding account and project, you can select any app you want to work on.", 
            "title": "Select App"
        }, 
        {
            "location": "/user-guide/user-management/", 
            "text": "Change Password\n\n\nYou can change your Batchly password at anytime.\n\n\n\n\n\n\nSign in to your Bacthly account.\n\n\n\n\n\n\nClick on the name given on the top-right corner.\n\n\n\n\n\n\nSelect \nChange Password\n.\n\n\n\n\n\n\n\n\nEdit Profile Settings\n\n\nYou can change your profile settings ( \nEmail\n, \nName\n, \nRole\n ) at any time.\n\n\n\n\n\n\nYou can click on the name given on the top-right corner from the header.\n\n\n\n\n\n\nSelect \nEdit Password\n.\n\n\n\n\n\n\n\n\nSettings\n\n\nFrom the settings, you can generate \nAPI Keys\n. Also, you can \nrevoke\n those keys when not required.\n\n\n\n\n\n\nYou can click on the name given on the top-right corner from the header.\n\n\n\n\n\n\nSelect \nSettings\n.\n\n\n\n\n\n\n\n\nUsers \n(For Administrator Account Only)\n\n\nAs Administrator in Batchly, you have permissions to add and delete your colleagues. He/She can also restrict or permit access to specific apps in Batchly.\n\n\nNote:\n By default, all users have access to all apps in Batchly.\n\n\nTo remove app permissions for a specific user, follow the below steps:\n\n\n\n\nGo to Users screen, from the Profile link in the top header.\n\n\n\n\n\n\n\n\nIn the Users screen, click \"Add User\" button.\n\n\n\n\n\n\n\n\nIn the \"Add User\" screen, go to \"App Permissions\" section, check the Apps you want to give access to and click \"Add User\".\n\n\n\n\n\n\nThis will create a new user, with access to specific apps.", 
            "title": "User Management"
        }, 
        {
            "location": "/user-guide/user-management/#change-password", 
            "text": "You can change your Batchly password at anytime.    Sign in to your Bacthly account.    Click on the name given on the top-right corner.    Select  Change Password .", 
            "title": "Change Password"
        }, 
        {
            "location": "/user-guide/user-management/#edit-profile-settings", 
            "text": "You can change your profile settings (  Email ,  Name ,  Role  ) at any time.    You can click on the name given on the top-right corner from the header.    Select  Edit Password .", 
            "title": "Edit Profile Settings"
        }, 
        {
            "location": "/user-guide/user-management/#settings", 
            "text": "From the settings, you can generate  API Keys . Also, you can  revoke  those keys when not required.    You can click on the name given on the top-right corner from the header.    Select  Settings .", 
            "title": "Settings"
        }, 
        {
            "location": "/user-guide/user-management/#users-for-administrator-account-only", 
            "text": "As Administrator in Batchly, you have permissions to add and delete your colleagues. He/She can also restrict or permit access to specific apps in Batchly.  Note:  By default, all users have access to all apps in Batchly.  To remove app permissions for a specific user, follow the below steps:   Go to Users screen, from the Profile link in the top header.     In the Users screen, click \"Add User\" button.     In the \"Add User\" screen, go to \"App Permissions\" section, check the Apps you want to give access to and click \"Add User\".    This will create a new user, with access to specific apps.", 
            "title": "Users (For Administrator Account Only)"
        }, 
        {
            "location": "/user-guide/iam-access/", 
            "text": "Steps to create IAM Access\n\n\nBatchly provides an easy way to add your AWS account. All it needs is for you to provide your AWS Master Key and Access Key, and Batchly automatically creates a separate IAM access with privileges to launch EC2 instances (Spot and On-demand), as well as monitor them.\n\n\nHowever, if for some reason you don\u2019t want to provide your AWS keys, you can create an IAM user for Batchly by following the steps given below:\n\n\n\n\n\n\nGo to AWS IAM dashboard\n\n\n\n\n\n\nIn the left hand pane, click on user.\n\n\n\n\n\n\nGive the user a name, then select Generate to give an access key to each user and click Create.\n\n\n\n\n\n\n\n\nDownload the credentials. At the bottom right corner, there\u2019s a download credentials button.\n\n\n\n\nIn the Users listing, click on the user and in the next screen at the bottom, click on Inline Policies.\n\n\n\n\n\n\nSelect Custom Policy and it opens up a policy editor. Give the policy a name, such as  Batchly-Policy, and add the policy below.\n\n\n\n\n\n\n{\n    \nVersion\n: \n2012-10-17\n,\n    \nStatement\n: [\n        {\n            \nSid\n: \nStmt1455011811000\n,\n            \nEffect\n: \nAllow\n,\n            \nAction\n: [\n                \niam:GetUser\n,\n                \niam:AddRoleToInstanceProfile\n,\n                \niam:CreateInstanceProfile\n,\n                \niam:CreateRole\n,\n                \niam:GetRole\n,\n                \niam:ListInstanceProfiles\n,\n                \niam:ListRoles\n,\n                \niam:PutRolePolicy\n,\n                \niam:RemoveRoleFromInstanceProfile\n,\n                \niam:UpdateAssumeRolePolicy\n,\n                \niam:PassRole\n,\n                \niam:GetInstanceProfile\n\n            ],\n            \nResource\n: [\n        \narn:aws:iam::XXXXXXXXXXXXX:role/batchly/\n\n                \narn:aws:iam::XXXXXXXXXXXXX:role/batchly/batchly-trusted-role\n,\n        \narn:aws:iam::XXXXXXXXXXXXX:instance-profile/*\n\n        \narn:aws:iam::XXXXXXXXXXXXX:policy/batchly-access-policy\n,\n        \narn:aws:iam::XXXXXXXXXXXXX:user/USERNAME\n\n            ]\n        }\n    ]\n}\n\n\n\n\nNote:\n In the JSON, replace the XXXXXXXXXXXXX with the account number of the account in which you are creating the user. Replace the USERNAME with the username in the above policy statement.\n\n\nNext, apply the policy and use the credentials to add the AWS account in Batchly.\n\n\nPlease execute the above steps and share the credential information with us. This would enable us to add the role. The permission list clearly specifies the permission expected for adding the new role.\n\n\nOnce the account is added in Batchly, you can safely remove the user. In addition, once the account is added, you can also validate the newly added role and its permission list.", 
            "title": "Create IAM Access"
        }, 
        {
            "location": "/user-guide/iam-access/#steps-to-create-iam-access", 
            "text": "Batchly provides an easy way to add your AWS account. All it needs is for you to provide your AWS Master Key and Access Key, and Batchly automatically creates a separate IAM access with privileges to launch EC2 instances (Spot and On-demand), as well as monitor them.  However, if for some reason you don\u2019t want to provide your AWS keys, you can create an IAM user for Batchly by following the steps given below:    Go to AWS IAM dashboard    In the left hand pane, click on user.    Give the user a name, then select Generate to give an access key to each user and click Create.     Download the credentials. At the bottom right corner, there\u2019s a download credentials button.   In the Users listing, click on the user and in the next screen at the bottom, click on Inline Policies.    Select Custom Policy and it opens up a policy editor. Give the policy a name, such as  Batchly-Policy, and add the policy below.    {\n     Version :  2012-10-17 ,\n     Statement : [\n        {\n             Sid :  Stmt1455011811000 ,\n             Effect :  Allow ,\n             Action : [\n                 iam:GetUser ,\n                 iam:AddRoleToInstanceProfile ,\n                 iam:CreateInstanceProfile ,\n                 iam:CreateRole ,\n                 iam:GetRole ,\n                 iam:ListInstanceProfiles ,\n                 iam:ListRoles ,\n                 iam:PutRolePolicy ,\n                 iam:RemoveRoleFromInstanceProfile ,\n                 iam:UpdateAssumeRolePolicy ,\n                 iam:PassRole ,\n                 iam:GetInstanceProfile \n            ],\n             Resource : [\n         arn:aws:iam::XXXXXXXXXXXXX:role/batchly/ \n                 arn:aws:iam::XXXXXXXXXXXXX:role/batchly/batchly-trusted-role ,\n         arn:aws:iam::XXXXXXXXXXXXX:instance-profile/* \n         arn:aws:iam::XXXXXXXXXXXXX:policy/batchly-access-policy ,\n         arn:aws:iam::XXXXXXXXXXXXX:user/USERNAME \n            ]\n        }\n    ]\n}  Note:  In the JSON, replace the XXXXXXXXXXXXX with the account number of the account in which you are creating the user. Replace the USERNAME with the username in the above policy statement.  Next, apply the policy and use the credentials to add the AWS account in Batchly.  Please execute the above steps and share the credential information with us. This would enable us to add the role. The permission list clearly specifies the permission expected for adding the new role.  Once the account is added in Batchly, you can safely remove the user. In addition, once the account is added, you can also validate the newly added role and its permission list.", 
            "title": "Steps to create IAM Access"
        }, 
        {
            "location": "/user-guide/security-policy/", 
            "text": "Batchly Security Policy\n\n\nAs a \u201cCloud First\u201d company, we at Batchly take security very seriously and it has always been at the heart of our decisions \n workflows from day one.\n\n\nAWS Account Security\n\n\nBatchly runs within your secure AWS account. It uses IAM Roles and a set of permissions associated with it to access \n manage AWS infrastructure. We DO NOT store your AWS Access and Secret keys. \n\n\nIAM Role and Permissions:\n\n\nWe adhere to AWS recommended best practices for assuming a role within your AWS account.\n(Refer: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.htm)\n\n\nTo start using Batchly you are required to create a role that Batchly can assume within your AWS account. We have published an exhaustive list of permissions that Batchly requires to manage your AWS environment in the most cost effective manner. We DO NOT require * (All) permissions and only take fine grained permissions for different AWS services.\n\n\nThe following are the permissions that are required across services:\n\n\nAWS:EC2\n\n\nec2:CreateSecurityGroup\nec2:CreateSpotDatafeedSubscription\nec2:CancelSpotInstanceRequests \nec2:CreateTags \nec2:DeleteTags\nec2:CreateVolume \nec2:attach* \nec2:Describe* \nec2:RequestSpotInstances \nec2:RunInstances \nec2:GetConsoleOutput\nec2:AuthorizeSecurityGroup*\n\n**On Resources Created by Batchly**\n\nec2:StartInstances\nec2:StopInstances\nec2:TerminateInstances\nec2:DeleteVolume        \nec2:DeleteSecurityGroup\n\n\n\n\nAWS:S3\n\n\ns3:AbortMultipartUpload\ns3:Get*\ns3:List*\ns3:Put*\ns3:RestoreObject\ns3:CreateBucket\n\n\n\n\nAWS:SQS\n\n\nsqs:CreateQueue\nsqs:GetQueueAttributes\nsqs:GetQueueUrl\nsqs:ListQueues\nsqs:SetQueueAttributes\n\n\n\n\nAWS:CloudWatch\n\n\ncloudwatch:DescribeAlarmHistory\ncloudwatch:DescribeAlarms\ncloudwatch:DescribeAlarmsForMetric\ncloudwatch:DisableAlarmActions\ncloudwatch:EnableAlarmActions\ncloudwatch:GetMetricData\ncloudwatch:GetMetricStatistics\ncloudwatch:ListMetrics\ncloudwatch:PutMetricAlarm\ncloudwatch:PutMetricData\ncloudwatch:SetAlarmState\n\n\n\n\nAWS:Autoscaling\n\n\nautoscaling:Describe*\nautoscaling:AttachLoadBalancers\nautoscaling:CreateOrUpdateTags\nautoscaling:DetachLoadBalancers\nautoscaling:AttachInstances\nautoscaling:DetachInstances\nautoscaling:SetDesiredCapacity\nautoscaling:TerminateInstanceInAutoScalingGroup\nautoscaling:UpdateAutoScalingGroup\n\n\n\n\nAWS:EMR\n\n\nelasticmapreduce:AddTags        \nelasticmapreduce:AddJobFlowSteps        \nelasticmapreduce:DescribeJobFlows       \nelasticmapreduce:DescribeCluster        \nelasticmapreduce:DescribeStep           \nelasticmapreduce:ListBootstrapActions   \nelasticmapreduce:ListClusters       \nelasticmapreduce:ListInstanceGroups     \nelasticmapreduce:ListInstances      \nelasticmapreduce:ListSteps\nelasticmapreduce:ModifyInstanceGroups   \nelasticmapreduce:RunJobFlow\n\n**On Resources Created by Batchly**\n\nelasticmapreduce:RemoveTags\nelasticmapreduce:SetTerminationProtection\nelasticmapreduce:SetVisibleToAllUsers   \nelasticmapreduce:TerminateJobFlows\n\n\n\n\nAWS:ElasticBeanstalk\n\n\nelasticbeanstalk:Describe*\nelasticbeanstalk:RequestEnvironmentInfo\nelasticbeanstalk:RetrieveEnvironmentInfo\nelasticbeanstalk:ValidateConfigurationSettings\nelasticbeanstalk:RestartAppServer\n\n\n\n\nOthers\n\n\necs:Describe*\necs:List*\necs:DeregisterContainerInstance\necs:RegisterTaskDefinition\nelasticloadbalancing:ConfigureHealthCheck\nelasticloadbalancing:DeregisterInstancesFromLoadBalancer\nelasticloadbalancing:Describe*\nelasticloadbalancing:RegisterInstancesWithLoadBalancer\niam:ListRoles\niam:ListInstanceProfiles\niam:ListInstanceProfilesForRole\niam:PassRole\nsts:AssumeRole\nlogs:CreateLogGroup\nlogs:PutRetentionPolicy\n\n\n\n\nResources Key / Password Management\n\n\nWe have a strict Time-bound key rotation policy for managing resource passwords and keys that are used internally. In practice these are rotated at regular intervals to contain damages due to any extremely rare incident of a data breach.\n\n\nBatchly Account Security\n\n\nAs a customer you are provided with your own Batchly account. We provide the following ways for you to interface with Batchly:\n\n\n\n\nBatchly Console\n\n\nBatchly API/SDKs\n\n\n\n\nBatchly API / SDKs:\n\n\nBatchly API backend only processes authenticated and signed requests from the user. Batchly uses HMAC-SHA1 authentication. All responses are signed to ensure that the origin of the response is indeed Batchly and no one else. \n\n\nBatchly SDKs use APIs internally and take care of signing and authentication for you.\n\n\nBatchly Internal Infrastructure\n\n\nWe have a High Availability (HA) setup for our backend infrastructure and hence is tolerant to failures across Availability Zones and Regions.\n\n\nThe Batchly infrastructure works within its own Virtual Private cloud (VPC) and therefore is isolated from the public internet. Access to any resources within the VPC from the public internet is carefully modulated using appropriate Security Groups and Access Control Lists (ACLs).\n\n\nWe have separate Environments (Dev QA and Production) each serving a specific purpose in the product\u2019s Development Life Cycle. Exhaustive testing and Quality assurances are undertaken in the QA environment before any feature is moved live.", 
            "title": "Security Policy"
        }, 
        {
            "location": "/user-guide/security-policy/#batchly-security-policy", 
            "text": "As a \u201cCloud First\u201d company, we at Batchly take security very seriously and it has always been at the heart of our decisions   workflows from day one.", 
            "title": "Batchly Security Policy"
        }, 
        {
            "location": "/user-guide/security-policy/#aws-account-security", 
            "text": "Batchly runs within your secure AWS account. It uses IAM Roles and a set of permissions associated with it to access   manage AWS infrastructure. We DO NOT store your AWS Access and Secret keys.", 
            "title": "AWS Account Security"
        }, 
        {
            "location": "/user-guide/security-policy/#iam-role-and-permissions", 
            "text": "We adhere to AWS recommended best practices for assuming a role within your AWS account.\n(Refer: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.htm)  To start using Batchly you are required to create a role that Batchly can assume within your AWS account. We have published an exhaustive list of permissions that Batchly requires to manage your AWS environment in the most cost effective manner. We DO NOT require * (All) permissions and only take fine grained permissions for different AWS services.  The following are the permissions that are required across services:", 
            "title": "IAM Role and Permissions:"
        }, 
        {
            "location": "/user-guide/security-policy/#awsec2", 
            "text": "ec2:CreateSecurityGroup\nec2:CreateSpotDatafeedSubscription\nec2:CancelSpotInstanceRequests \nec2:CreateTags \nec2:DeleteTags\nec2:CreateVolume \nec2:attach* \nec2:Describe* \nec2:RequestSpotInstances \nec2:RunInstances \nec2:GetConsoleOutput\nec2:AuthorizeSecurityGroup*\n\n**On Resources Created by Batchly**\n\nec2:StartInstances\nec2:StopInstances\nec2:TerminateInstances\nec2:DeleteVolume        \nec2:DeleteSecurityGroup", 
            "title": "AWS:EC2"
        }, 
        {
            "location": "/user-guide/security-policy/#awss3", 
            "text": "s3:AbortMultipartUpload\ns3:Get*\ns3:List*\ns3:Put*\ns3:RestoreObject\ns3:CreateBucket", 
            "title": "AWS:S3"
        }, 
        {
            "location": "/user-guide/security-policy/#awssqs", 
            "text": "sqs:CreateQueue\nsqs:GetQueueAttributes\nsqs:GetQueueUrl\nsqs:ListQueues\nsqs:SetQueueAttributes", 
            "title": "AWS:SQS"
        }, 
        {
            "location": "/user-guide/security-policy/#awscloudwatch", 
            "text": "cloudwatch:DescribeAlarmHistory\ncloudwatch:DescribeAlarms\ncloudwatch:DescribeAlarmsForMetric\ncloudwatch:DisableAlarmActions\ncloudwatch:EnableAlarmActions\ncloudwatch:GetMetricData\ncloudwatch:GetMetricStatistics\ncloudwatch:ListMetrics\ncloudwatch:PutMetricAlarm\ncloudwatch:PutMetricData\ncloudwatch:SetAlarmState", 
            "title": "AWS:CloudWatch"
        }, 
        {
            "location": "/user-guide/security-policy/#awsautoscaling", 
            "text": "autoscaling:Describe*\nautoscaling:AttachLoadBalancers\nautoscaling:CreateOrUpdateTags\nautoscaling:DetachLoadBalancers\nautoscaling:AttachInstances\nautoscaling:DetachInstances\nautoscaling:SetDesiredCapacity\nautoscaling:TerminateInstanceInAutoScalingGroup\nautoscaling:UpdateAutoScalingGroup", 
            "title": "AWS:Autoscaling"
        }, 
        {
            "location": "/user-guide/security-policy/#awsemr", 
            "text": "elasticmapreduce:AddTags        \nelasticmapreduce:AddJobFlowSteps        \nelasticmapreduce:DescribeJobFlows       \nelasticmapreduce:DescribeCluster        \nelasticmapreduce:DescribeStep           \nelasticmapreduce:ListBootstrapActions   \nelasticmapreduce:ListClusters       \nelasticmapreduce:ListInstanceGroups     \nelasticmapreduce:ListInstances      \nelasticmapreduce:ListSteps\nelasticmapreduce:ModifyInstanceGroups   \nelasticmapreduce:RunJobFlow\n\n**On Resources Created by Batchly**\n\nelasticmapreduce:RemoveTags\nelasticmapreduce:SetTerminationProtection\nelasticmapreduce:SetVisibleToAllUsers   \nelasticmapreduce:TerminateJobFlows", 
            "title": "AWS:EMR"
        }, 
        {
            "location": "/user-guide/security-policy/#awselasticbeanstalk", 
            "text": "elasticbeanstalk:Describe*\nelasticbeanstalk:RequestEnvironmentInfo\nelasticbeanstalk:RetrieveEnvironmentInfo\nelasticbeanstalk:ValidateConfigurationSettings\nelasticbeanstalk:RestartAppServer", 
            "title": "AWS:ElasticBeanstalk"
        }, 
        {
            "location": "/user-guide/security-policy/#others", 
            "text": "ecs:Describe*\necs:List*\necs:DeregisterContainerInstance\necs:RegisterTaskDefinition\nelasticloadbalancing:ConfigureHealthCheck\nelasticloadbalancing:DeregisterInstancesFromLoadBalancer\nelasticloadbalancing:Describe*\nelasticloadbalancing:RegisterInstancesWithLoadBalancer\niam:ListRoles\niam:ListInstanceProfiles\niam:ListInstanceProfilesForRole\niam:PassRole\nsts:AssumeRole\nlogs:CreateLogGroup\nlogs:PutRetentionPolicy", 
            "title": "Others"
        }, 
        {
            "location": "/user-guide/security-policy/#resources-key-password-management", 
            "text": "We have a strict Time-bound key rotation policy for managing resource passwords and keys that are used internally. In practice these are rotated at regular intervals to contain damages due to any extremely rare incident of a data breach.", 
            "title": "Resources Key / Password Management"
        }, 
        {
            "location": "/user-guide/security-policy/#batchly-account-security", 
            "text": "As a customer you are provided with your own Batchly account. We provide the following ways for you to interface with Batchly:   Batchly Console  Batchly API/SDKs", 
            "title": "Batchly Account Security"
        }, 
        {
            "location": "/user-guide/security-policy/#batchly-api-sdks", 
            "text": "Batchly API backend only processes authenticated and signed requests from the user. Batchly uses HMAC-SHA1 authentication. All responses are signed to ensure that the origin of the response is indeed Batchly and no one else.   Batchly SDKs use APIs internally and take care of signing and authentication for you.", 
            "title": "Batchly API / SDKs:"
        }, 
        {
            "location": "/user-guide/security-policy/#batchly-internal-infrastructure", 
            "text": "We have a High Availability (HA) setup for our backend infrastructure and hence is tolerant to failures across Availability Zones and Regions.  The Batchly infrastructure works within its own Virtual Private cloud (VPC) and therefore is isolated from the public internet. Access to any resources within the VPC from the public internet is carefully modulated using appropriate Security Groups and Access Control Lists (ACLs).  We have separate Environments (Dev QA and Production) each serving a specific purpose in the product\u2019s Development Life Cycle. Exhaustive testing and Quality assurances are undertaken in the QA environment before any feature is moved live.", 
            "title": "Batchly Internal Infrastructure"
        }, 
        {
            "location": "/app-pages/spark/", 
            "text": "Getting started with Big Data Apps\n\n\nBig Data has following apps which are almost similar to each other in term of using it (or information required to run these apps): \n\nHive\n, \nPig\n, \nSpark\n, \nHadoop\n\n\nThis demonstration will show on how to use Spark app.You may select any other Big data app instead of Spark.(since , all these apps are similar)\n\n\nStep 1:\n  Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.\n\n\nStep 2:\n You will be redirected to Batchly Dashboard. Next, click on the \nApp Store\n located in the header.\n\n\n\n\nStep 3:\n You will be redirected to the App store which has the apps supported on Batchly. To run Spark, click the \nGet Started\n button in the Spark App.\n\n\n\n\nStep 4:\n Now, to run Spark job, fill all the required given text fields. There are following text fields to be filled: \n\n\nJob Name:\n You can give any desired name to your job. \n\n\nProject:\n Select the associated project to run the job.\n\n\nCluster Name:\n Select the cluster name. \n\n\nDeploy Mode:\n Select the deploy mode.\n\n\nSpark-submit Options:\n Enter Spark-submit if you have any.\n\n\nApplication Location:\n Enter Application Location.\n\n\nArguments:\n Enter Arguments if you have any.\n\n\nAction on Failure:\n Select the Action on Failure.\n\n\nInstance Type:\n Select the Instance Type\n\n\n\n\nStep 5:\n Click on the \nAdd Job\n button once you are done with filling all the details. This action will save your job and is available to see later on the \nJobs\n page.\n\n\nStep 6:\n On successful job addition, you would get a popup where you can either start your job immediately (by clicking \nExecute the Job\n) or schedule your job to run later (by clicking on the button \nSchedule the Job\n).\n\n\n\n\nStep 7:\n You can monitor the job progress using the \nJob Run Details\n page.", 
            "title": "Big Data Apps"
        }, 
        {
            "location": "/app-pages/spark/#getting-started-with-big-data-apps", 
            "text": "Big Data has following apps which are almost similar to each other in term of using it (or information required to run these apps):  Hive ,  Pig ,  Spark ,  Hadoop  This demonstration will show on how to use Spark app.You may select any other Big data app instead of Spark.(since , all these apps are similar)  Step 1:   Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.  Step 2:  You will be redirected to Batchly Dashboard. Next, click on the  App Store  located in the header.   Step 3:  You will be redirected to the App store which has the apps supported on Batchly. To run Spark, click the  Get Started  button in the Spark App.   Step 4:  Now, to run Spark job, fill all the required given text fields. There are following text fields to be filled:   Job Name:  You can give any desired name to your job.   Project:  Select the associated project to run the job.  Cluster Name:  Select the cluster name.   Deploy Mode:  Select the deploy mode.  Spark-submit Options:  Enter Spark-submit if you have any.  Application Location:  Enter Application Location.  Arguments:  Enter Arguments if you have any.  Action on Failure:  Select the Action on Failure.  Instance Type:  Select the Instance Type   Step 5:  Click on the  Add Job  button once you are done with filling all the details. This action will save your job and is available to see later on the  Jobs  page.  Step 6:  On successful job addition, you would get a popup where you can either start your job immediately (by clicking  Execute the Job ) or schedule your job to run later (by clicking on the button  Schedule the Job ).   Step 7:  You can monitor the job progress using the  Job Run Details  page.", 
            "title": "Getting started with Big Data Apps"
        }, 
        {
            "location": "/app-pages/jmeter/", 
            "text": "Let's get started with JMeter\n\n\nStep 1:\n  Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.\n\n\nStep 2:\n You will be redirected to Batchly Dashboard. Next, click on the \nApp Store\n located in the header.\n\n\n\n\nStep 3:\n You will be redirected to the App store which has the apps supported on Batchly. To run JMeter, click the \nGet Started\n button in the JMeter App.\n\n\n\n\nStep 4:\n Now, to run JMeter job, fill all the required given text fields. There are following text fields to be filled: \n\n\n\n\n\n\nJob Name:\n You can give any desired name to your job.\n\n\n\n\n\n\nProject:\n Select the associated project to run the JMeter job.\n\n\n\n\n\n\nTest Plan:\n Upload the JMeter test plan (JMX file) from your local system which you want to execute.\n\n\n\n\n\n\nOutput Location:\n Give the Amazon S3 bucket output location.\n\n\n\n\n\n\nInstance Count:\n Give the number of instance on which you want to run your test job.\n\n\n\n\n\n\nInstance Type:\n Select the Amazon EC2 instance type on which you want to run your test plan.\n\n\n\n\n\n\n\n\nStep 5:\n Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.\n\n\nStep 6:\n On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).\n\n\n\n\nStep 7:\n You can monitor the job progress using the Job Run Details page.", 
            "title": "JMeter"
        }, 
        {
            "location": "/app-pages/jmeter/#lets-get-started-with-jmeter", 
            "text": "Step 1:   Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.  Step 2:  You will be redirected to Batchly Dashboard. Next, click on the  App Store  located in the header.   Step 3:  You will be redirected to the App store which has the apps supported on Batchly. To run JMeter, click the  Get Started  button in the JMeter App.   Step 4:  Now, to run JMeter job, fill all the required given text fields. There are following text fields to be filled:     Job Name:  You can give any desired name to your job.    Project:  Select the associated project to run the JMeter job.    Test Plan:  Upload the JMeter test plan (JMX file) from your local system which you want to execute.    Output Location:  Give the Amazon S3 bucket output location.    Instance Count:  Give the number of instance on which you want to run your test job.    Instance Type:  Select the Amazon EC2 instance type on which you want to run your test plan.     Step 5:  Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.  Step 6:  On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).   Step 7:  You can monitor the job progress using the Job Run Details page.", 
            "title": "Let's get started with JMeter"
        }, 
        {
            "location": "/app-pages/ffmpeg/", 
            "text": "Let's get started with FFmpeg\n\n\nStep 1:\n  Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.\n\n\nStep 2:\n You will be redirected to Batchly Dashboard. Next, click on the \nApp Store\n located in the header.\n\n\n\n\nStep 3:\n You will be redirected to the App store which has the apps supported on Batchly. To run JMeter, click the \nGet Started\n button in the FFmpeg App.\n\n\n\n\nStep 4:\n Now, to run FFmpeg job, fill all the required given text fields. There are following text fields to be filled:\n\n\nJob Name:\n You can give any desired name to your job.\n\n\nProject:\n Select the associated project to run the job.\n\n\nDefault AWS Region:\n Select the AWS region.\n\n\nTime Limit:\n Give the time limit to run a job in hours.\n\n\nProfile:\n Select the profile type in which you want to transcode.\n\n\nFilename Suffix:\n Give any desired suffix to differentiate input file from the output file.\n\n\nSource S3 Bucket Name:\n Give the Amazon S3 bucket Bucket name.\n\n\nFolder Name:\n Give the folder name, if available.\n\n\nDestination S3 Bucket Name:\n Give the Amazon S3 Bucket name.\n\n\nFolder Name:\n Give the folder name, if available.\n\n\n\n\nStep 5:\n Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.\n\n\nStep 6:\n On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).\n\n\n\n\nStep 7:\n You can monitor the job progress using the Job Run Details page.", 
            "title": "FFmpeg"
        }, 
        {
            "location": "/app-pages/ffmpeg/#lets-get-started-with-ffmpeg", 
            "text": "Step 1:   Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.  Step 2:  You will be redirected to Batchly Dashboard. Next, click on the  App Store  located in the header.   Step 3:  You will be redirected to the App store which has the apps supported on Batchly. To run JMeter, click the  Get Started  button in the FFmpeg App.   Step 4:  Now, to run FFmpeg job, fill all the required given text fields. There are following text fields to be filled:  Job Name:  You can give any desired name to your job.  Project:  Select the associated project to run the job.  Default AWS Region:  Select the AWS region.  Time Limit:  Give the time limit to run a job in hours.  Profile:  Select the profile type in which you want to transcode.  Filename Suffix:  Give any desired suffix to differentiate input file from the output file.  Source S3 Bucket Name:  Give the Amazon S3 bucket Bucket name.  Folder Name:  Give the folder name, if available.  Destination S3 Bucket Name:  Give the Amazon S3 Bucket name.  Folder Name:  Give the folder name, if available.   Step 5:  Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.  Step 6:  On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).   Step 7:  You can monitor the job progress using the Job Run Details page.", 
            "title": "Let's get started with FFmpeg"
        }, 
        {
            "location": "/app-pages/imagemagick/", 
            "text": "Let's get started with ImageMagick\n\n\nStep 1:\n  Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.\n\n\nStep 2:\n You will be redirected to Batchly Dashboard. Next, click on the \nApp Store\n located in the header.\n\n\n\n\nStep 3:\n You will be redirected to the App store which has the apps supported on Batchly. To run ImageMagick, click the \nGet Started\n button in the \nImageMagick\n App.\n\n\n\n\nStep 4:\n Now, to run ImageMagick job, fill all the required given text fields.\n\n\nJob Name:\n You can give any desired name to your job.\n\n\nProject:\n Select the associated project to run the job.\n\n\n\n\nDefault AWS Region:\n Select the AWS region.\n\n\nSource S3 Bucket Name:\n Give the Amazon S3 bucket Bucket name.\n\n\nFolder Name:\n Give the folder name, if available.\n\n\nDestination S3 Bucket Name:\n Give the Amazon S3 Bucket name.\n\n\nFolder Name:\n Give the folder name, if available.\n\n\n\n\nTime Limit:\n Give the time limit to run a job in hours.\n\n\nDefault AWS Region:\n Select the AWS region.\n\n\nRESIZE:\n Give any desired size for the output file.\n\n\n\n\nStep 5:\n Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the \nJobs\n page.\n\n\nStep 6:\n On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).\n\n\nStep 7:\n You can monitor the job progress using the Job Run Details page.", 
            "title": "ImageMagick"
        }, 
        {
            "location": "/app-pages/imagemagick/#lets-get-started-with-imagemagick", 
            "text": "Step 1:   Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.  Step 2:  You will be redirected to Batchly Dashboard. Next, click on the  App Store  located in the header.   Step 3:  You will be redirected to the App store which has the apps supported on Batchly. To run ImageMagick, click the  Get Started  button in the  ImageMagick  App.   Step 4:  Now, to run ImageMagick job, fill all the required given text fields.  Job Name:  You can give any desired name to your job.  Project:  Select the associated project to run the job.   Default AWS Region:  Select the AWS region.  Source S3 Bucket Name:  Give the Amazon S3 bucket Bucket name.  Folder Name:  Give the folder name, if available.  Destination S3 Bucket Name:  Give the Amazon S3 Bucket name.  Folder Name:  Give the folder name, if available.   Time Limit:  Give the time limit to run a job in hours.  Default AWS Region:  Select the AWS region.  RESIZE:  Give any desired size for the output file.   Step 5:  Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the  Jobs  page.  Step 6:  On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).  Step 7:  You can monitor the job progress using the Job Run Details page.", 
            "title": "Let's get started with ImageMagick"
        }, 
        {
            "location": "/app-pages/autoscaling/", 
            "text": "Getting started with AWS Auto Scaling using Batchly\n\n\nAWS Auto Scaling helps you maintain application availability and allows you to scale your Amazon EC2 capacity up or down automatically according to conditions you define. With Batchly App for Auto Scaling, you can now run your AWS Auto Scaling groups through Batchly and bring down your EC2 costs significantly with the usage of right spot instances without compromising on your application availability. \n\n\nStep 1:\n  Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.\n\n\nStep 2:\n You will be redirected to Batchly Dashboard. Next, click on the App Store located in the header.\n\n\n\n\nStep 3:\n You will be redirected to the \nApp store\n which has the apps supported on Batchly. To run Elastic Load Balancer app, click the Get Started button.\n\n\n\n\nStep 4:\n Now, to run Auto Scaling job, fill all the required given text fields. There are following text fields to be filled:\n\n\nJob Name:\n You can give any desired name to your job.\n\n\nProject:\n Select the associated project to run the job. The project is associated with VPC and Region in which the job has to run.\n\n\nAuto Scaling Group name:\n Select the AWS Auto Scaling group from the drop down.\n\n\nMin OnDemand instances:\n Specify the AMI ID to be used to launch instancesSpecify the minimum number of ondemand instances that should run as part of the Auto Scaling group.\n\n\nMax Instance Count:\n Specify the maximum number of instances that should run as part of the Auto Scaling group.\n\n\nDesired Count:\n Specify the desired number of instances. Auto Scaling ensures that your group has this many instances.\n\n\n\n\nStep 5:\n Click on the \nAdd Job\n button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.\n\n\nStep 6:\n On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).\n\n\n\n\nStep 7:\n You can monitor the job progress using the Job Run Details page.", 
            "title": "Auto Scaling"
        }, 
        {
            "location": "/app-pages/autoscaling/#getting-started-with-aws-auto-scaling-using-batchly", 
            "text": "AWS Auto Scaling helps you maintain application availability and allows you to scale your Amazon EC2 capacity up or down automatically according to conditions you define. With Batchly App for Auto Scaling, you can now run your AWS Auto Scaling groups through Batchly and bring down your EC2 costs significantly with the usage of right spot instances without compromising on your application availability.   Step 1:   Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.  Step 2:  You will be redirected to Batchly Dashboard. Next, click on the App Store located in the header.   Step 3:  You will be redirected to the  App store  which has the apps supported on Batchly. To run Elastic Load Balancer app, click the Get Started button.   Step 4:  Now, to run Auto Scaling job, fill all the required given text fields. There are following text fields to be filled:  Job Name:  You can give any desired name to your job.  Project:  Select the associated project to run the job. The project is associated with VPC and Region in which the job has to run.  Auto Scaling Group name:  Select the AWS Auto Scaling group from the drop down.  Min OnDemand instances:  Specify the AMI ID to be used to launch instancesSpecify the minimum number of ondemand instances that should run as part of the Auto Scaling group.  Max Instance Count:  Specify the maximum number of instances that should run as part of the Auto Scaling group.  Desired Count:  Specify the desired number of instances. Auto Scaling ensures that your group has this many instances.   Step 5:  Click on the  Add Job  button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.  Step 6:  On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).   Step 7:  You can monitor the job progress using the Job Run Details page.", 
            "title": "Getting started with AWS Auto Scaling using Batchly"
        }, 
        {
            "location": "/app-pages/elasticloadbalancer/", 
            "text": "Getting started with Batchly Elastic Load Balancer\n\n\nStep 1:\n  Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.\n\n\nStep 2:\n You will be redirected to Batchly Dashboard. Next, click on the App Store located in the header.\n\n\n\n\nStep 3:\n You will be redirected to the \nApp store\n which has the apps supported on Batchly. To run Elastic Load Balancer app, click the Get Started button.\n\n\n\n\nStep 4:\n Now, to run Elastic Load Balancer job, fill all the required given text fields. There are following text fields to be filled:\n\n\nJob Name:\n You can give any desired name to your job.\n\n\nProject:\n Select the associated project to run the job. The project is associated with VPC and Region in which the job has to run.\n\n\nElastic Load Balancer:\n Select the Elastic Load Balancer from the drop down.\n\n\nAMI ID:\n Specify the AMI ID to be used to launch instances.\n\n\nSecurity Group:\n Specify the security group to be associated with instances.\n\n\nLaunch script:\n Specify the user data script to be used while launching the instances.\n\n\nInstance Type:\n Specify the instance type to be used to launch instances.\n\n\nMax Instance Count:\n Specify the maximum number of instances that should run as part of the autoscaling group.\n\n\nDesired Count:\n Specify the desired number of instances.\n\n\n Scaling Rules:\n Specify the metric on which the to scale, upper threshold , lower threshold and scale down factors. \n\n\n\n\nStep 5:\n Click on the \nAdd Job\n button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.\n\n\nStep 6:\n On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).\n\n\n\n\nStep 7:\n You can monitor the job progress using the Job Run Details page.", 
            "title": "Elastic Load Balancer"
        }, 
        {
            "location": "/app-pages/elasticloadbalancer/#getting-started-with-batchly-elastic-load-balancer", 
            "text": "Step 1:   Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.  Step 2:  You will be redirected to Batchly Dashboard. Next, click on the App Store located in the header.   Step 3:  You will be redirected to the  App store  which has the apps supported on Batchly. To run Elastic Load Balancer app, click the Get Started button.   Step 4:  Now, to run Elastic Load Balancer job, fill all the required given text fields. There are following text fields to be filled:  Job Name:  You can give any desired name to your job.  Project:  Select the associated project to run the job. The project is associated with VPC and Region in which the job has to run.  Elastic Load Balancer:  Select the Elastic Load Balancer from the drop down.  AMI ID:  Specify the AMI ID to be used to launch instances.  Security Group:  Specify the security group to be associated with instances.  Launch script:  Specify the user data script to be used while launching the instances.  Instance Type:  Specify the instance type to be used to launch instances.  Max Instance Count:  Specify the maximum number of instances that should run as part of the autoscaling group.  Desired Count:  Specify the desired number of instances.   Scaling Rules:  Specify the metric on which the to scale, upper threshold , lower threshold and scale down factors.    Step 5:  Click on the  Add Job  button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.  Step 6:  On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).   Step 7:  You can monitor the job progress using the Job Run Details page.", 
            "title": "Getting started with Batchly Elastic Load Balancer"
        }, 
        {
            "location": "/api/overview/", 
            "text": "Table of Contents\n\n\n\n\nBatchly REST API Overview\n\n\nAPI Definitions for common tasks\n\n\nApp Specific API Definitions\n\n\n\n\nBatchly REST API\n\n\nEach customer of batchly gets their own customized URL for access. Using the web console, you can generate new set of API access keys.  You need to safe keep the keys.  Batchly API SDK doesn\u2019t send the secret key in any request. Batchly supports two types of API authentication mechanisms -  HMAC which validates calls using request signature and ApiKey based.\n\n\nBefore making calls to the API, setup the base URI, access key and secret key in the configuration.\n\n\nEnd Point\n\n\nTo call API requests to Batchly.net, please send HTTP(S) post requests to:\n\n\nhttps://\ncustomerdomain\n.batchly.net/api/\n\n\n\n\nAuthentication\n\n\n ApiKey \n\n\nA user's unique authentication key string. This authentication key will be shared with your team when the system is configured for your usage. This key is mandatory and needs to be sent as part of every API call. The Key should be sent along in the Request Header as below.\n\n\nAuthorization: ApiKey AccessKey:SecretKey\n\n\n\n\nSupport\n\n\nPlease reach out to \nsupport@batchy.net\n to get your API Keys if you do not have the keys and/or receiving authentication errors.", 
            "title": "API Overview"
        }, 
        {
            "location": "/api/overview/#table-of-contents", 
            "text": "Batchly REST API Overview  API Definitions for common tasks  App Specific API Definitions", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/api/overview/#batchly-rest-api", 
            "text": "Each customer of batchly gets their own customized URL for access. Using the web console, you can generate new set of API access keys.  You need to safe keep the keys.  Batchly API SDK doesn\u2019t send the secret key in any request. Batchly supports two types of API authentication mechanisms -  HMAC which validates calls using request signature and ApiKey based.  Before making calls to the API, setup the base URI, access key and secret key in the configuration.", 
            "title": "Batchly REST API"
        }, 
        {
            "location": "/api/overview/#end-point", 
            "text": "To call API requests to Batchly.net, please send HTTP(S) post requests to:  https:// customerdomain .batchly.net/api/", 
            "title": "End Point"
        }, 
        {
            "location": "/api/overview/#authentication", 
            "text": "ApiKey   A user's unique authentication key string. This authentication key will be shared with your team when the system is configured for your usage. This key is mandatory and needs to be sent as part of every API call. The Key should be sent along in the Request Header as below.  Authorization: ApiKey AccessKey:SecretKey", 
            "title": "Authentication"
        }, 
        {
            "location": "/api/overview/#support", 
            "text": "Please reach out to  support@batchy.net  to get your API Keys if you do not have the keys and/or receiving authentication errors.", 
            "title": "Support"
        }, 
        {
            "location": "/api/definition/", 
            "text": "Batchly API supports the following entities:\n\n\n\n\nJobs\n\n\nRuns\n\n\nProjects\n\n\nAccounts\n\n\n\n\nJobs API\n\n\nJobs are calls to perform various operations on the actual dataset. This call varies from one app to another. Though it is app-specific, each job requires jobname, Project, SLA, input location and output location to be mentioned.\n\n\nBatchly responds immediately with a job ID so your application can track the progress of the job.\n\n\nActions Supported\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGET\n\n\napi/Jobs\n\n\nReturns all the jobs associated with your Batchly account\n\n\n\n\n\n\n\n\nTo run individual jobs, call the API associated with the respective app. \nApp API Documentation\n\n\nRuns API\n\n\nRuns are individual instances of the job in Batchly. Runs API allows you to list and delete the runs in your Batchly account.\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGET\n\n\n/api/Runs\n\n\nGet all the runs associated with your batchly account\n\n\n\n\n\n\nGET\n\n\n/api/Runs/{id}\n\n\nGet the details of the run for the given id\n\n\n\n\n\n\nDELETE\n\n\n/api/Runs/{id}\n\n\nDelete the run details from your account\n\n\n\n\n\n\n\n\nProjects API\n\n\nProjects define the boundary for your jobs to run in. Projects creates a sandbox environment associated with an AWS account, a cloud region within that AWS account and (optionally) associate a virtual private cloud with it.\n\n\nProjects\u2019 calls allow you to create, edit or view projects associated with your Batchly account.\n\n\nActions Supported\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDELETE\n\n\n/api/Projects/{id}\n\n\nDelete the project with the given id\n\n\n\n\n\n\nGET\n\n\n/api/Projects/{id}\n\n\nGet the details of the project for the given id\n\n\n\n\n\n\nPUT\n\n\n/api/Projects/{id}\n\n\nUpdate the project details for the given id\n\n\n\n\n\n\nPOST\n\n\n/api/Projects/Add\n\n\nCreate a new project\n\n\n\n\n\n\nGET\n\n\n/api/Projects\n\n\nGet all the projects associated with your batchly account\n\n\n\n\n\n\n\n\nAccounts API\n\n\nAttach your AWS account to batchly using the accounts API. Accounts calls allow you to create, edit or view AWS accounts associated with your Batchly account.\n\n\nActions Supported\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPOST\n\n\n/api/Accounts/AWS\n\n\nAdd new AWS account to your batchly account\n\n\n\n\n\n\nDELETE\n\n\n/api/Accounts/{id}\n\n\nDelete the AWS account associated with this id\n\n\n\n\n\n\nGET\n\n\n/api/Accounts/{id}\n\n\nGet the AWS account details related to this id\n\n\n\n\n\n\nPUT\n\n\n/api/Accounts/{id}\n\n\nUpdate the details for this id\n\n\n\n\n\n\nGET\n\n\n/api/Accounts\n\n\nGet all the AWS accounts associated with your batchly account", 
            "title": "Definition"
        }, 
        {
            "location": "/api/definition/#jobs-api", 
            "text": "Jobs are calls to perform various operations on the actual dataset. This call varies from one app to another. Though it is app-specific, each job requires jobname, Project, SLA, input location and output location to be mentioned.  Batchly responds immediately with a job ID so your application can track the progress of the job.  Actions Supported     HTTP Method  Endpoint  Description      GET  api/Jobs  Returns all the jobs associated with your Batchly account     To run individual jobs, call the API associated with the respective app.  App API Documentation", 
            "title": "Jobs API"
        }, 
        {
            "location": "/api/definition/#runs-api", 
            "text": "Runs are individual instances of the job in Batchly. Runs API allows you to list and delete the runs in your Batchly account.     HTTP Method  Endpoint  Description      GET  /api/Runs  Get all the runs associated with your batchly account    GET  /api/Runs/{id}  Get the details of the run for the given id    DELETE  /api/Runs/{id}  Delete the run details from your account", 
            "title": "Runs API"
        }, 
        {
            "location": "/api/definition/#projects-api", 
            "text": "Projects define the boundary for your jobs to run in. Projects creates a sandbox environment associated with an AWS account, a cloud region within that AWS account and (optionally) associate a virtual private cloud with it.  Projects\u2019 calls allow you to create, edit or view projects associated with your Batchly account.  Actions Supported     HTTP Method  Endpoint  Description      DELETE  /api/Projects/{id}  Delete the project with the given id    GET  /api/Projects/{id}  Get the details of the project for the given id    PUT  /api/Projects/{id}  Update the project details for the given id    POST  /api/Projects/Add  Create a new project    GET  /api/Projects  Get all the projects associated with your batchly account", 
            "title": "Projects API"
        }, 
        {
            "location": "/api/definition/#accounts-api", 
            "text": "Attach your AWS account to batchly using the accounts API. Accounts calls allow you to create, edit or view AWS accounts associated with your Batchly account.  Actions Supported     HTTP Method  Endpoint  Description      POST  /api/Accounts/AWS  Add new AWS account to your batchly account    DELETE  /api/Accounts/{id}  Delete the AWS account associated with this id    GET  /api/Accounts/{id}  Get the AWS account details related to this id    PUT  /api/Accounts/{id}  Update the details for this id    GET  /api/Accounts  Get all the AWS accounts associated with your batchly account", 
            "title": "Accounts API"
        }, 
        {
            "location": "/api/app-api/", 
            "text": "App Specific APIs\n\n\nBatchly works through Apps. There are marketplace apps such as FFMpeg for video transcoding, and JMeter for load testing. You may also upload your own private app to run in your Batchly account.\n\n\nNevertheless, each app has its own set of APIs to perform the basic operations. The request headers, however, differ based on the app you want to create.\n\n\nCommon Actions Supported By All Apps\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPOST\n\n\napi/apps/{APP_NAME}/{id}/Execute\n\n\nExecute the job\n\n\n\n\n\n\nPOST\n\n\napi/apps/{APP_NAME}\n\n\nCreate a new job\n\n\n\n\n\n\nPUT\n\n\napi/apps/{APP_NAME}/{id}\n\n\nEdit a job with updated information.\n\n\n\n\n\n\nGET\n\n\napi/apps/{APP_NAME}/{id}\n\n\nGet details for a job\n\n\n\n\n\n\nDELETE\n\n\napi/apps/{APP_NAME}/{id}\n\n\nDelete a job\n\n\n\n\n\n\nGET\n\n\napi/apps/{APP_NAME}/jobs\n\n\nLists all jobs\n\n\n\n\n\n\n\n\nApp Specific Request headers\n\n\n\n\nSpark App\n\n\nAPP_NAME : Spark\n\n\nRequest Header\n\n\n{\n  \nName\n: \u201cSpark Job\u201d,\n  \nProjectId\n: \u201cP-XXXXX\u201d,\n  \nClusterId\n: \u201cj-XXXXX\u201d,\n  \nDeployMode\n: \u201cCluster\u201d,\n  \nSparkSubmitOptions\n: \u201copt1 opt2 opt3\u201d,\n  \nApplicationLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_SPARK_JAR\u201d,\n  \nArguments\n: \u201carg1 arg2 arg3\u201d,\n  \nActionOnFailure\n: \u201cCONTINUE\u201d,\n  \nInstanceTypes\n: [\nc4.large\n, \nm3.large\n]\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nClusterId\n\n\nString\n\n\nCluster Id where the job should execute\n\n\nYes\n\n\n\n\n\n\nDeployMode\n\n\nInteger\n\n\n1 (for Cluster mode), 2 (for Client mode)\n\n\nYes\n\n\n\n\n\n\nSparkSubmitOptions\n\n\nString\n\n\nSpecify other options for spark-submit\n\n\nNo\n\n\n\n\n\n\nApplicationLocation\n\n\nString\n\n\nS3 location where the Spark jar resides\n\n\nYes\n\n\n\n\n\n\nArguments\n\n\nString\n\n\nAny arguments that the script requires\n\n\nNo\n\n\n\n\n\n\nActionOnFailure\n\n\nString\n\n\nCONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER\n\n\nYes\n\n\n\n\n\n\nInstanceTypes\n\n\nString[]\n\n\nThe instance types to be launched\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n   Id: \nW-XXXXX\n,\n   Name: \nSpark Job\n,\n   Runs: 1,\n   IsScheduled: \nFalse\n,\n   IsExecuting: \nTrue | False\n,\n   Engine: \nMapReduce\n\n}   \n\n\n\n\n\n\nHadoopStreaming App\n\n\nAPP_NAME: HadoopStreaming\n\n\nRequest Header\n\n\n{\n  \nName\n: \u201cHadoopStreaming Job\u201d,\n  \nProjectId\n: \u201cP-XXXXX\u201d,\n  \nClusterId\n: \u201cj-XXXXX\u201d,\n  \nMapperLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_MAP_FUNCTION\u201d,\n  \nReducerLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_REDUCE_FUNCTION\u201d,\n  \nInputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n  \nOutputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n  \nArguments\n: \u201carg1 arg2 arg3\u201d,\n  \nActionOnFailure\n: \u201cCONTINUE\u201d,\n  \nInstanceTypes\n: [\nc4.large\n, \nm3.large\n]\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nClusterId\n\n\nString\n\n\nCluster Id where the job should execute\n\n\nYes\n\n\n\n\n\n\nMapperLocation\n\n\nString\n\n\nS3 location of the map function or the name of the Hadoop streaming command to run\n\n\nYes\n\n\n\n\n\n\nReducerLocation\n\n\nString\n\n\nS3 location of the reduce function or the name of the Hadoop streaming command to run\n\n\nYes\n\n\n\n\n\n\nInputLocation\n\n\nString\n\n\nS3 location where the dataset resides\n\n\nYes\n\n\n\n\n\n\nOutputLocation\n\n\nString\n\n\nS3 location where the results are written\n\n\nYes\n\n\n\n\n\n\nArguments\n\n\nString\n\n\nAny arguments that the script requires\n\n\nNo\n\n\n\n\n\n\nActionOnFailure\n\n\nString\n\n\nCONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER\n\n\nYes\n\n\n\n\n\n\nInstanceTypes\n\n\nString[]\n\n\nThe instance types to be launched\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n   Id: \nW-XXXXX\n,\n   Name: \nHadoopStreaming Job\n,\n   Runs: 1,\n   IsScheduled: \nFalse\n,\n   IsExecuting: \nTrue | False\n,\n   Engine: \nMapReduce\n\n}   \n\n\n\n\n\n\nHadoopCustom App\n\n\nAPP_NAME: Hadoop\n\n\nRequest Header\n\n\n{\n  \nName\n: \u201cHadoop Job\u201d,\n  \nProjectId\n: \u201cP-XXXXX\u201d,\n  \nClusterId\n: \u201cj-XXXXX\u201d,\n  \nJarLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_JAR\u201d,\n  \nArguments\n: \u201carg1 arg2 arg3\u201d,\n  \nActionOnFailure\n: \u201cCONTINUE\u201d,\n  \nInstanceTypes\n: [\nc4.large\n, \nm3.large\n]\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nClusterId\n\n\nString\n\n\nCluster Id where the job should execute\n\n\nYes\n\n\n\n\n\n\nJarLocation\n\n\nString\n\n\nA path into S3 or a fully qualified java class in the classpath\n\n\nYes\n\n\n\n\n\n\nArguments\n\n\nString\n\n\nAny arguments that the script requires\n\n\nNo\n\n\n\n\n\n\nActionOnFailure\n\n\nString\n\n\nCONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER\n\n\nYes\n\n\n\n\n\n\nInstanceTypes\n\n\nString\n\n\nThe instance types to be launched\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n   Id: \nW-XXXXX\n,\n   Name: \nHadoop Job\n,\n   Runs: 1,\n   IsScheduled: \nFalse\n,\n   IsExecuting: \nTrue | False\n,\n   Engine: \nMapReduce\n\n}   \n\n\n\n\n\n\nHive App\n\n\nAPP_NAME: Hive\n\n\nRequest Header\n\n\n{\n  \nName\n: \u201cHive Job\u201d,\n  \nProjectId\n: \u201cP-XXXXX\u201d,\n  \nClusterId\n: \u201cj-XXXXX\u201d,\n  \nScriptLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_SCRIPT\u201d,\n  \nInputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n  \nOutputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n  \nArguments\n: \u201carg1 arg2 arg3\u201d,\n  \nActionOnFailure\n: \u201cCONTINUE\u201d,\n  \nInstanceTypes\n: [\nc4.large\n, \nm3.large\n]\n}    \n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nClusterId\n\n\nString\n\n\nCluster Id where the job should execute\n\n\nYes\n\n\n\n\n\n\nScriptLocation\n\n\nString\n\n\nS3 location where the script resides\n\n\nYes\n\n\n\n\n\n\nInputLocation\n\n\nString\n\n\nS3 location where the dataset resides\n\n\nNo\n\n\n\n\n\n\nOutputLocation\n\n\nString\n\n\nS3 location where results are written\n\n\nNo\n\n\n\n\n\n\nArguments\n\n\nString\n\n\nAny arguments that the script requires\n\n\nNo\n\n\n\n\n\n\nActionOnFailure\n\n\nString\n\n\nCONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER\n\n\nYes\n\n\n\n\n\n\nInstanceTypes\n\n\nString[]\n\n\nThe instance types to be launched\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n   Id: \nW-XXXXX\n,\n   Name: \nHive Job\n,\n   Runs: 1,\n   IsScheduled: \nFalse\n,\n   IsExecuting: \nTrue | False\n,\n   Engine: \nMapReduce\n\n}   \n\n\n\n\n\n\nPig App\n\n\nAPP_NAME: PIG\n\n\nRequest Header\n\n\n{\n  \nName\n: \u201cPig Job\u201d,\n  \nProjectId\n: \u201cP-XXXXX\u201d,\n  \nClusterId\n: \u201cj-XXXXX\u201d,\n  \nScriptLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_SCRIPT\u201d,\n  \nInputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n  \nOutputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n  \nArguments\n: \u201carg1 arg2 arg3\u201d,\n  \nActionOnFailure\n: \u201cCONTINUE\u201d,\n  \nInstanceTypes\n: [\nc4.large\n, \nm3.large\n]\n} \n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nClusterId\n\n\nString\n\n\nCluster Id where the job should execute\n\n\nYes\n\n\n\n\n\n\nScriptLocation\n\n\nString\n\n\nS3 location where the script resides\n\n\nYes\n\n\n\n\n\n\nInputLocation\n\n\nString\n\n\nS3 location where the dataset resides\n\n\nNo\n\n\n\n\n\n\nOutputLocation\n\n\nString\n\n\nS3 location where results are written\n\n\nNo\n\n\n\n\n\n\nArguments\n\n\nString\n\n\nAny arguments that the script requires\n\n\nNo\n\n\n\n\n\n\nActionOnFailure\n\n\nString\n\n\nCONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER\n\n\nYes\n\n\n\n\n\n\nInstanceTypes\n\n\nString[]\n\n\nThe instance types to be launched\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n   Id: \nW-XXXXX\n,\n   Name: \nPig Job\n,\n   Runs: 1,\n   IsScheduled: \nFalse\n,\n   IsExecuting: \nTrue | False\n,\n   Engine: \nMapReduce\n\n}   \n\n\n\n\n\n\nJMeter - Load Testing App\n\n\nAPP_NAME: JMeter\n\n\nRequest Header\n\n\n{\n  \nName\n: \nstring\n,\n  \nProjectId\n: \nstring\n,\n  \nDestinationLocation\n: \nstring\n,\n  \nTestPlan\n: \nstring\n,\n  \nInstanceType\n: \nstring\n,\n  \nInstanceCount\n: 0\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nTestPlan\n\n\nInteger\n\n\nJMX test plan you want to execture\n\n\nYes\n\n\n\n\n\n\nDestinationLocation\n\n\nString\n\n\nLocation where you want the output files to go to\n\n\nYes\n\n\n\n\n\n\nInstanceType\n\n\nString\n\n\nInstance type you want Batchly to launch\n\n\nYes\n\n\n\n\n\n\nInstanceCount\n\n\nInteger\n\n\nNumber of instances of the InstanceType you want Batchly to launch and manage\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n  \nRequestId\n: \nstring\n,\n  \nData\n: {\n    \nRuns\n: 0,\n    \nIsScheduled\n: true,\n    \nIsExecuting\n: true,\n    \nEngine\n: \nAll\n,\n    \nAppName\n: \nstring\n,\n    \nIsPrivateApp\n: true,\n    \nId\n: \nstring\n,\n    \nName\n: \nstring\n\n  },\n  \nErrors\n: {},\n  \nContinuationToken\n: \nstring\n\n}\n\n\n\n\n\n\nFFMpeg - Video Transcoding App\n\n\nAPP_NAME: FFMpeg\n\n\nRequest Header\n\n\n{\n  \nName\n: \nstring\n,\n  \nProjectId\n: \nstring\n,\n  \nRegion\n: \nstring\n,\n  \nTimeLimit\n: 0,\n  \nProfileId\n: \nstring\n,\n  \nFileNameSuffix\n: \nstring\n,\n  \nSourceLocation\n: \nstring\n,\n  \nDestinationLocation\n: \nstring\n\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nRegion\n\n\nString\n\n\nAWS region where your job will run\n\n\nYes\n\n\n\n\n\n\nTimeLimit\n\n\nInteger\n\n\nTime limit when your job needs to complete\n\n\nYes\n\n\n\n\n\n\nProfileId\n\n\nString\n\n\nCommon video profiles for MP4 and HLS\n\n\nYes\n\n\n\n\n\n\nFileNameSuffix\n\n\nString\n\n\nSuffix to identify the transcoded output file\n\n\nNo\n\n\n\n\n\n\nSourceLocation\n\n\nString\n\n\nLocation where all the input files reside\n\n\nYes\n\n\n\n\n\n\nDestinationLocation\n\n\nString\n\n\nLocation where you want the output files to go to\n\n\nYes\n\n\n\n\n\n\n\n\nExample result\n\n\n{\n  \nRequestId\n: \nstring\n,\n  \nData\n: {\n    \nRuns\n: 0,\n    \nIsScheduled\n: true,\n    \nIsExecuting\n: true,\n    \nEngine\n: \nAll\n,\n    \nAppName\n: \nstring\n,\n    \nIsPrivateApp\n: true,\n    \nId\n: \nstring\n,\n    \nName\n: \nstring\n\n  },\n  \nErrors\n: {},\n  \nContinuationToken\n: \nstring\n\n}\n\n\n\n\n\n\nImageMagick - Image processing App\n\n\nAPP_NAME: ImageMagick\n\n\nRequest Header\n\n\n{\n  \nName\n: \nstring\n,\n  \nProjectId\n: \nstring\n,\n  \nSourceLocation\n: \nstring\n,\n  \nDestinationLocation\n: \nstring\n,\n  \nRegion\n: \nstring\n,\n  \nTimeLimit\n: 0,\n  \nResize\n: \nstring\n\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nRegion\n\n\nString\n\n\nAWS region where your job will run\n\n\nYes\n\n\n\n\n\n\nTimeLimit\n\n\nInteger\n\n\nTime limit when your job needs to complete\n\n\nYes\n\n\n\n\n\n\nResize\n\n\nString\n\n\nImage height and width to which the image needs to be converted to e.g. 100x100\n\n\nYes\n\n\n\n\n\n\nSourceLocation\n\n\nString\n\n\nLocation where all the input files reside\n\n\nYes\n\n\n\n\n\n\nDestinationLocation\n\n\nString\n\n\nLocation where you want the output files to go to\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n  \nRequestId\n: \nstring\n,\n  \nData\n: {\n    \nRuns\n: 0,\n    \nIsScheduled\n: true,\n    \nIsExecuting\n: true,\n    \nEngine\n: \nAll\n,\n    \nAppName\n: \nstring\n,\n    \nIsPrivateApp\n: true,\n    \nId\n: \nstring\n,\n    \nName\n: \nstring\n\n  },\n  \nErrors\n: {},\n  \nContinuationToken\n: \nstring\n\n}\n\n\n\n\n\n\nTesseract - Optical Character Recognition App\n\n\nAPP_NAME: Tesseract\n\n\nRequest Header\n\n\n{\n  \nName\n: \nstring\n,\n  \nProjectId\n: \nstring\n,\n  \nSourceLocation\n: \nstring\n,\n  \nDestinationLocation\n: \nstring\n,\n  \nRegion\n: \nstring\n,\n  \nTimeLimit\n: 0\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nRegion\n\n\nString\n\n\nAWS region where your job will run\n\n\nYes\n\n\n\n\n\n\nTimeLimit\n\n\nInteger\n\n\nTime limit when your job needs to complete\n\n\nYes\n\n\n\n\n\n\nSourceLocation\n\n\nString\n\n\nLocation where all the input files reside\n\n\nYes\n\n\n\n\n\n\nDestinationLocation\n\n\nString\n\n\nLocation where you want the output files to go to\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n  \nRuns\n: 0,\n  \nIsScheduled\n: true,\n  \nIsExecuting\n: true,\n  \nEngine\n: \nAll\n,\n  \nAppName\n: \nstring\n,\n  \nIsPrivateApp\n: true,\n  \nId\n: \nstring\n,\n  \nName\n: \nstring\n\n}", 
            "title": "App Specific APIs"
        }, 
        {
            "location": "/api/app-api/#app-specific-apis", 
            "text": "Batchly works through Apps. There are marketplace apps such as FFMpeg for video transcoding, and JMeter for load testing. You may also upload your own private app to run in your Batchly account.  Nevertheless, each app has its own set of APIs to perform the basic operations. The request headers, however, differ based on the app you want to create.  Common Actions Supported By All Apps     HTTP Method  Endpoint  Description      POST  api/apps/{APP_NAME}/{id}/Execute  Execute the job    POST  api/apps/{APP_NAME}  Create a new job    PUT  api/apps/{APP_NAME}/{id}  Edit a job with updated information.    GET  api/apps/{APP_NAME}/{id}  Get details for a job    DELETE  api/apps/{APP_NAME}/{id}  Delete a job    GET  api/apps/{APP_NAME}/jobs  Lists all jobs", 
            "title": "App Specific APIs"
        }, 
        {
            "location": "/api/app-api/#app-specific-request-headers", 
            "text": "", 
            "title": "App Specific Request headers"
        }, 
        {
            "location": "/api/app-api/#spark-app", 
            "text": "APP_NAME : Spark  Request Header  {\n   Name : \u201cSpark Job\u201d,\n   ProjectId : \u201cP-XXXXX\u201d,\n   ClusterId : \u201cj-XXXXX\u201d,\n   DeployMode : \u201cCluster\u201d,\n   SparkSubmitOptions : \u201copt1 opt2 opt3\u201d,\n   ApplicationLocation : \u201cs3://BUCKET_NAME/PATH_TO_SPARK_JAR\u201d,\n   Arguments : \u201carg1 arg2 arg3\u201d,\n   ActionOnFailure : \u201cCONTINUE\u201d,\n   InstanceTypes : [ c4.large ,  m3.large ]\n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    ClusterId  String  Cluster Id where the job should execute  Yes    DeployMode  Integer  1 (for Cluster mode), 2 (for Client mode)  Yes    SparkSubmitOptions  String  Specify other options for spark-submit  No    ApplicationLocation  String  S3 location where the Spark jar resides  Yes    Arguments  String  Any arguments that the script requires  No    ActionOnFailure  String  CONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER  Yes    InstanceTypes  String[]  The instance types to be launched  Yes     Success Response  {\n   Id:  W-XXXXX ,\n   Name:  Spark Job ,\n   Runs: 1,\n   IsScheduled:  False ,\n   IsExecuting:  True | False ,\n   Engine:  MapReduce \n}", 
            "title": "Spark App"
        }, 
        {
            "location": "/api/app-api/#hadoopstreaming-app", 
            "text": "APP_NAME: HadoopStreaming  Request Header  {\n   Name : \u201cHadoopStreaming Job\u201d,\n   ProjectId : \u201cP-XXXXX\u201d,\n   ClusterId : \u201cj-XXXXX\u201d,\n   MapperLocation : \u201cs3://BUCKET_NAME/PATH_TO_MAP_FUNCTION\u201d,\n   ReducerLocation : \u201cs3://BUCKET_NAME/PATH_TO_REDUCE_FUNCTION\u201d,\n   InputLocation : \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n   OutputLocation : \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n   Arguments : \u201carg1 arg2 arg3\u201d,\n   ActionOnFailure : \u201cCONTINUE\u201d,\n   InstanceTypes : [ c4.large ,  m3.large ]\n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    ClusterId  String  Cluster Id where the job should execute  Yes    MapperLocation  String  S3 location of the map function or the name of the Hadoop streaming command to run  Yes    ReducerLocation  String  S3 location of the reduce function or the name of the Hadoop streaming command to run  Yes    InputLocation  String  S3 location where the dataset resides  Yes    OutputLocation  String  S3 location where the results are written  Yes    Arguments  String  Any arguments that the script requires  No    ActionOnFailure  String  CONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER  Yes    InstanceTypes  String[]  The instance types to be launched  Yes     Success Response  {\n   Id:  W-XXXXX ,\n   Name:  HadoopStreaming Job ,\n   Runs: 1,\n   IsScheduled:  False ,\n   IsExecuting:  True | False ,\n   Engine:  MapReduce \n}", 
            "title": "HadoopStreaming App"
        }, 
        {
            "location": "/api/app-api/#hadoopcustom-app", 
            "text": "APP_NAME: Hadoop  Request Header  {\n   Name : \u201cHadoop Job\u201d,\n   ProjectId : \u201cP-XXXXX\u201d,\n   ClusterId : \u201cj-XXXXX\u201d,\n   JarLocation : \u201cs3://BUCKET_NAME/PATH_TO_JAR\u201d,\n   Arguments : \u201carg1 arg2 arg3\u201d,\n   ActionOnFailure : \u201cCONTINUE\u201d,\n   InstanceTypes : [ c4.large ,  m3.large ]\n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    ClusterId  String  Cluster Id where the job should execute  Yes    JarLocation  String  A path into S3 or a fully qualified java class in the classpath  Yes    Arguments  String  Any arguments that the script requires  No    ActionOnFailure  String  CONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER  Yes    InstanceTypes  String  The instance types to be launched  Yes     Success Response  {\n   Id:  W-XXXXX ,\n   Name:  Hadoop Job ,\n   Runs: 1,\n   IsScheduled:  False ,\n   IsExecuting:  True | False ,\n   Engine:  MapReduce \n}", 
            "title": "HadoopCustom App"
        }, 
        {
            "location": "/api/app-api/#hive-app", 
            "text": "APP_NAME: Hive  Request Header  {\n   Name : \u201cHive Job\u201d,\n   ProjectId : \u201cP-XXXXX\u201d,\n   ClusterId : \u201cj-XXXXX\u201d,\n   ScriptLocation : \u201cs3://BUCKET_NAME/PATH_TO_SCRIPT\u201d,\n   InputLocation : \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n   OutputLocation : \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n   Arguments : \u201carg1 arg2 arg3\u201d,\n   ActionOnFailure : \u201cCONTINUE\u201d,\n   InstanceTypes : [ c4.large ,  m3.large ]\n}         Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    ClusterId  String  Cluster Id where the job should execute  Yes    ScriptLocation  String  S3 location where the script resides  Yes    InputLocation  String  S3 location where the dataset resides  No    OutputLocation  String  S3 location where results are written  No    Arguments  String  Any arguments that the script requires  No    ActionOnFailure  String  CONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER  Yes    InstanceTypes  String[]  The instance types to be launched  Yes     Success Response  {\n   Id:  W-XXXXX ,\n   Name:  Hive Job ,\n   Runs: 1,\n   IsScheduled:  False ,\n   IsExecuting:  True | False ,\n   Engine:  MapReduce \n}", 
            "title": "Hive App"
        }, 
        {
            "location": "/api/app-api/#pig-app", 
            "text": "APP_NAME: PIG  Request Header  {\n   Name : \u201cPig Job\u201d,\n   ProjectId : \u201cP-XXXXX\u201d,\n   ClusterId : \u201cj-XXXXX\u201d,\n   ScriptLocation : \u201cs3://BUCKET_NAME/PATH_TO_SCRIPT\u201d,\n   InputLocation : \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n   OutputLocation : \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n   Arguments : \u201carg1 arg2 arg3\u201d,\n   ActionOnFailure : \u201cCONTINUE\u201d,\n   InstanceTypes : [ c4.large ,  m3.large ]\n}      Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    ClusterId  String  Cluster Id where the job should execute  Yes    ScriptLocation  String  S3 location where the script resides  Yes    InputLocation  String  S3 location where the dataset resides  No    OutputLocation  String  S3 location where results are written  No    Arguments  String  Any arguments that the script requires  No    ActionOnFailure  String  CONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER  Yes    InstanceTypes  String[]  The instance types to be launched  Yes     Success Response  {\n   Id:  W-XXXXX ,\n   Name:  Pig Job ,\n   Runs: 1,\n   IsScheduled:  False ,\n   IsExecuting:  True | False ,\n   Engine:  MapReduce \n}", 
            "title": "Pig App"
        }, 
        {
            "location": "/api/app-api/#jmeter-load-testing-app", 
            "text": "APP_NAME: JMeter  Request Header  {\n   Name :  string ,\n   ProjectId :  string ,\n   DestinationLocation :  string ,\n   TestPlan :  string ,\n   InstanceType :  string ,\n   InstanceCount : 0\n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    TestPlan  Integer  JMX test plan you want to execture  Yes    DestinationLocation  String  Location where you want the output files to go to  Yes    InstanceType  String  Instance type you want Batchly to launch  Yes    InstanceCount  Integer  Number of instances of the InstanceType you want Batchly to launch and manage  Yes     Success Response  {\n   RequestId :  string ,\n   Data : {\n     Runs : 0,\n     IsScheduled : true,\n     IsExecuting : true,\n     Engine :  All ,\n     AppName :  string ,\n     IsPrivateApp : true,\n     Id :  string ,\n     Name :  string \n  },\n   Errors : {},\n   ContinuationToken :  string \n}", 
            "title": "JMeter - Load Testing App"
        }, 
        {
            "location": "/api/app-api/#ffmpeg-video-transcoding-app", 
            "text": "APP_NAME: FFMpeg  Request Header  {\n   Name :  string ,\n   ProjectId :  string ,\n   Region :  string ,\n   TimeLimit : 0,\n   ProfileId :  string ,\n   FileNameSuffix :  string ,\n   SourceLocation :  string ,\n   DestinationLocation :  string \n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    Region  String  AWS region where your job will run  Yes    TimeLimit  Integer  Time limit when your job needs to complete  Yes    ProfileId  String  Common video profiles for MP4 and HLS  Yes    FileNameSuffix  String  Suffix to identify the transcoded output file  No    SourceLocation  String  Location where all the input files reside  Yes    DestinationLocation  String  Location where you want the output files to go to  Yes     Example result  {\n   RequestId :  string ,\n   Data : {\n     Runs : 0,\n     IsScheduled : true,\n     IsExecuting : true,\n     Engine :  All ,\n     AppName :  string ,\n     IsPrivateApp : true,\n     Id :  string ,\n     Name :  string \n  },\n   Errors : {},\n   ContinuationToken :  string \n}", 
            "title": "FFMpeg - Video Transcoding App"
        }, 
        {
            "location": "/api/app-api/#imagemagick-image-processing-app", 
            "text": "APP_NAME: ImageMagick  Request Header  {\n   Name :  string ,\n   ProjectId :  string ,\n   SourceLocation :  string ,\n   DestinationLocation :  string ,\n   Region :  string ,\n   TimeLimit : 0,\n   Resize :  string \n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    Region  String  AWS region where your job will run  Yes    TimeLimit  Integer  Time limit when your job needs to complete  Yes    Resize  String  Image height and width to which the image needs to be converted to e.g. 100x100  Yes    SourceLocation  String  Location where all the input files reside  Yes    DestinationLocation  String  Location where you want the output files to go to  Yes     Success Response  {\n   RequestId :  string ,\n   Data : {\n     Runs : 0,\n     IsScheduled : true,\n     IsExecuting : true,\n     Engine :  All ,\n     AppName :  string ,\n     IsPrivateApp : true,\n     Id :  string ,\n     Name :  string \n  },\n   Errors : {},\n   ContinuationToken :  string \n}", 
            "title": "ImageMagick - Image processing App"
        }, 
        {
            "location": "/api/app-api/#tesseract-optical-character-recognition-app", 
            "text": "APP_NAME: Tesseract  Request Header  {\n   Name :  string ,\n   ProjectId :  string ,\n   SourceLocation :  string ,\n   DestinationLocation :  string ,\n   Region :  string ,\n   TimeLimit : 0\n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    Region  String  AWS region where your job will run  Yes    TimeLimit  Integer  Time limit when your job needs to complete  Yes    SourceLocation  String  Location where all the input files reside  Yes    DestinationLocation  String  Location where you want the output files to go to  Yes     Success Response  {\n   Runs : 0,\n   IsScheduled : true,\n   IsExecuting : true,\n   Engine :  All ,\n   AppName :  string ,\n   IsPrivateApp : true,\n   Id :  string ,\n   Name :  string \n}", 
            "title": "Tesseract - Optical Character Recognition App"
        }, 
        {
            "location": "/media-api/media-definition/", 
            "text": "Table of Contents\n\n\n\n\nBatchly Media Overview\n\n\nMedia Operations API\n\n\nMedia Status Codes\n\n\n\n\nBatchly Media API Overview\n\n\nBatchly Media enables both digital producers and publishing companies to automaticallybenefit from AWS cost and usage savings by optimizing media workloads with spot instances and EC2 smart sizing. Batchly is 20x times cheaper than other popular cloud transcoders.\n\n\nFeatures that customers love:\n\n\nInstance Smart Sizing\n Realize ultimate EC2 efficiencies with automatic instance sizing based on the file type, file size and output profiles.\n\n\nParallelization\n Process each profile in parallel to give you fast transcode times and quick time-to-market.\n\n\nAWS Spot Bid Optimization\n Seamlessly deploy, move, and manage workloads over high cost savings spot instances across any AWS region.\n\n\nSeamless Integration\n Integrate to any third party CMS solution using Batchly Media REST APIs.\n\n\nFully Managed\n Benefit from cost and utilization optimizations from within your own AWS account while we do the heavy lifting. Add to it, you don\u2019t pay for ingress and egress of your video content.\n\n\nWhy Batchly Media?\n\n\nThe global Video-On-Demand (VOD) market is projected to reach US$91B by 2020, largely driven by revolution in mobile technologyandfreedomtoviewitacrossmultipledevices,controloverviewingtimeand varietyofchoice.\n\n\nFor publishers and content owners, VODenabling their entire content libraryrequires transcoding it into multiple bitrates and resolutions (profiles) to suit their demography. This is a expensive operation often involving transcoding multiple renditions for different geographies, channels and devices. In addition, cloud transcoders require you to move data into theirserverswhichadds toyouringressandegresscostsand mightnotcomplywithyourlicensingpolicies.\n\n\nTechnical Details\n\n\n\n\nAVC (H.264) and HEVC Codec Support \n\n\nHLS \n MPEG-DASH Packaging\n\n\nRuns in your AWS account\n\n\nBlack band removal\n\n\nThumbnail generation\n\n\nStitch multiple videos\n\n\nREST APIs\n\n\nWebhook callbacks for success, failure and progress\n\n\nMetadata info for original and transcoded files\n\n\nSet AWS ACL rules for output files\n\n\n\n\nAPI Definition\n\n\nThe Batchly Media Api supports the following entities\n\n\n\n\nProfiles\n\n\nJobs\n\n\n\n\nProfiles are a way to group all the parameters that are required for successful processing of a file.  This may include the width, height, aspect ratio, etc\u2026\n\n\nBatchly Media Api has few pre built profiles that are available for performing media operations.  \n\n\nJobs are calls to perform various operations on actual files.  This call involves the following data to be sent as part of the request.\n\n\n\n\nThe source location of your video.\n\n\nJob notification location (via webhook)\n\n\nOne or multiple custom or preset desired encoding recipes.\n\n\nOne or multiple delivery points for your video.\n\n\n\n\nBatchly responds immediately with a media ID so your application can track the progress of the job.\n\n\nEnd Point\n\n\nTo send API requests to Batchly.net, please send HTTP(S) post requests to:\n\n\nhttps://media.batchly.net/\n\n\n\n\nAuthentication\n\n\nApiKey\n\nA user's unique authentication key string. This authentication key will be shared with your team when the system is configured for your usage. This key is mandatory and needs to be sent as part of every Api Call.  The Key should be sent along in the Request Header as below.\n\n\nAuthorization: ApiKey AccessKey:SecretKey", 
            "title": "Media API Definition"
        }, 
        {
            "location": "/media-api/media-definition/#table-of-contents", 
            "text": "Batchly Media Overview  Media Operations API  Media Status Codes", 
            "title": "Table of Contents"
        }, 
        {
            "location": "/media-api/media-definition/#batchly-media-api-overview", 
            "text": "Batchly Media enables both digital producers and publishing companies to automaticallybenefit from AWS cost and usage savings by optimizing media workloads with spot instances and EC2 smart sizing. Batchly is 20x times cheaper than other popular cloud transcoders.  Features that customers love:  Instance Smart Sizing  Realize ultimate EC2 efficiencies with automatic instance sizing based on the file type, file size and output profiles.  Parallelization  Process each profile in parallel to give you fast transcode times and quick time-to-market.  AWS Spot Bid Optimization  Seamlessly deploy, move, and manage workloads over high cost savings spot instances across any AWS region.  Seamless Integration  Integrate to any third party CMS solution using Batchly Media REST APIs.  Fully Managed  Benefit from cost and utilization optimizations from within your own AWS account while we do the heavy lifting. Add to it, you don\u2019t pay for ingress and egress of your video content.", 
            "title": "Batchly Media API Overview"
        }, 
        {
            "location": "/media-api/media-definition/#why-batchly-media", 
            "text": "The global Video-On-Demand (VOD) market is projected to reach US$91B by 2020, largely driven by revolution in mobile technologyandfreedomtoviewitacrossmultipledevices,controloverviewingtimeand varietyofchoice.  For publishers and content owners, VODenabling their entire content libraryrequires transcoding it into multiple bitrates and resolutions (profiles) to suit their demography. This is a expensive operation often involving transcoding multiple renditions for different geographies, channels and devices. In addition, cloud transcoders require you to move data into theirserverswhichadds toyouringressandegresscostsand mightnotcomplywithyourlicensingpolicies.", 
            "title": "Why Batchly Media?"
        }, 
        {
            "location": "/media-api/media-definition/#technical-details", 
            "text": "AVC (H.264) and HEVC Codec Support   HLS   MPEG-DASH Packaging  Runs in your AWS account  Black band removal  Thumbnail generation  Stitch multiple videos  REST APIs  Webhook callbacks for success, failure and progress  Metadata info for original and transcoded files  Set AWS ACL rules for output files", 
            "title": "Technical Details"
        }, 
        {
            "location": "/media-api/media-definition/#api-definition", 
            "text": "The Batchly Media Api supports the following entities   Profiles  Jobs   Profiles are a way to group all the parameters that are required for successful processing of a file.  This may include the width, height, aspect ratio, etc\u2026  Batchly Media Api has few pre built profiles that are available for performing media operations.    Jobs are calls to perform various operations on actual files.  This call involves the following data to be sent as part of the request.   The source location of your video.  Job notification location (via webhook)  One or multiple custom or preset desired encoding recipes.  One or multiple delivery points for your video.   Batchly responds immediately with a media ID so your application can track the progress of the job.", 
            "title": "API Definition"
        }, 
        {
            "location": "/media-api/media-definition/#end-point", 
            "text": "To send API requests to Batchly.net, please send HTTP(S) post requests to:  https://media.batchly.net/", 
            "title": "End Point"
        }, 
        {
            "location": "/media-api/media-definition/#authentication", 
            "text": "ApiKey \nA user's unique authentication key string. This authentication key will be shared with your team when the system is configured for your usage. This key is mandatory and needs to be sent as part of every Api Call.  The Key should be sent along in the Request Header as below.  Authorization: ApiKey AccessKey:SecretKey", 
            "title": "Authentication"
        }, 
        {
            "location": "/media-api/media-operations/", 
            "text": "Media Operations Supported\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGET\n\n\napi/Profiles\n\n\nReturns all the profiles available in batch.ly\n\n\n\n\n\n\nGET\n\n\napi/Profiles/{id}\n\n\nReturns parameters of a particular profile\n\n\n\n\n\n\nPOST\n\n\napi/Profiles\n\n\nCreate a new private profile for your usage.\n\n\n\n\n\n\nPUT\n\n\napi/Profiles/{id}\n\n\nEdit a given private profile with updated information\n\n\n\n\n\n\nDELETE\n\n\napi/Profiles/{id}\n\n\nRemoves a private profile.\n\n\n\n\n\n\nPOST\n\n\napi/Jobs\n\n\nCreates a new transcoding job with the formats specified in the JSON request\n\n\n\n\n\n\nGET\n\n\napi/Jobs/{id}\n\n\nGet the status of a specific job\n\n\n\n\n\n\nGET\n\n\napi/Jobs/{id}?profile={profileName}\n\n\nGet the status of a specific transcoding within a media job\n\n\n\n\n\n\n\n\nOperations\n\n\nThe following Operations are supported as part of the Create Job call.  The rules associated with entities are described below.\n\n\n\n\n\n\n\n\nOperation\n\n\nSources\n\n\nProfiles\n\n\nDestinations\n\n\nRemarks\n\n\n\n\n\n\n\n\n\n\nTranscode\n\n\nOne Path\n\n\nOne or More Profiles\n\n\nOne Destination\n\n\nThe source file is transcoded to the given profiles.  The destination file name is appended with profile name when creating output\n\n\n\n\n\n\nStitch\n\n\nAt Least Two Paths\n\n\nNone\n\n\nOne Destination\n\n\nOperation combines the input files to create the destination output\n\n\n\n\n\n\nHls\n\n\nOne Path\n\n\nAt Least Two Profiles\n\n\nOne Destination\n\n\nThe source is converted into many profiles and a destination .m3u8 file created with the appropriate references\n\n\n\n\n\n\nGetMetadata\n\n\nOne or more Paths\n\n\nNA\n\n\nNA\n\n\nIndividual source path meta data is sent back as webhooks to the given callback url\n\n\n\n\n\n\nGenerateThumbnails\n\n\nOne Path\n\n\nOne or More Profiles\n\n\nOne Destination\n\n\nSource file is used to snapshot to create thumbnails based on profiles.  Destination file format is used and appended with index of images.\n\n\n\n\n\n\n\n\nSupport for additional operations are currently in development and the document will be updated to reflect the same in near future.\n\n\nProfile Settings\n\n\nTranscode and Hls Settings\n\n\nBatchly has in-built profiles for most of the common MP4 and HLS output formats. However, you have flexibility to add or modify the values.\n\n\n\n\n\n\n\n\nField Name\n\n\nAllowed Values\n\n\n\n\n\n\n\n\n\n\nformat\n\n\nmp4, hls\n\n\n\n\n\n\nvideo_codec\n\n\nlibx264\n\n\n\n\n\n\nvideo_bitrate\n\n\nInteger e.g represent 100k as 100000\n\n\n\n\n\n\nminrate\n\n\nInteger e.g represent 100k as 100000\n\n\n\n\n\n\nmaxrate\n\n\nInteger e.g represent 100k as 100000\n\n\n\n\n\n\nbufsize\n\n\nInteger e.g represent 100k as 100000\n\n\n\n\n\n\nvideo_width\n\n\nInteger e.g 1084\n\n\n\n\n\n\nvideo_height\n\n\nInteger e.g 720\n\n\n\n\n\n\nframerate\n\n\nInteger e.g. 15, 30\n\n\n\n\n\n\ngroup_of_pictures\n\n\nInteger. Typically, double the framerate\n\n\n\n\n\n\nvideo_preset\n\n\nfast, medium, slow, slower, veryslow, placebo. Default - Medium\n\n\n\n\n\n\nprofile\n\n\nhigh, main, baseline\n\n\n\n\n\n\nlevel\n\n\n3.0, 3.1, 4.0, 4.1, 4.2\n\n\n\n\n\n\nvideo_disable\n\n\nyes, no\n\n\n\n\n\n\nvideo_disable_subtitle\n\n\nyes,no\n\n\n\n\n\n\naudio_codec\n\n\naac,ac3\n\n\n\n\n\n\naudio_channels\n\n\ne.g 2\n\n\n\n\n\n\naudio_bitrate\n\n\nInteger e.g represent 100k as 100000\n\n\n\n\n\n\naudio_samplerate\n\n\nE.g 44100, 48000\n\n\n\n\n\n\nx264opts\n\n\nH.264 specific options\n\n\n\n\n\n\nmovflags\n\n\nfaststart moves the video info to header\n\n\n\n\n\n\nhls_time\n\n\nsegment length. Recommended value 9\n\n\n\n\n\n\nhls_list_size\n\n\nSet to 0 for all VOD playlist entries\n\n\n\n\n\n\nhls_flags\n\n\nIf set to 'single_file', all segments will be stored in 1 ts file\n\n\n\n\n\n\n\n\nThumbnail Settings\n\n\nYou can create custom thumbnail profiles by using the following field values.\n\n\n\n\n\n\n\n\nField Name\n\n\nAllowed Values\n\n\n\n\n\n\n\n\n\n\nthumbnail_time\n\n\nIn seconds. E.g.: 10 for generating thumbnail every 10 seconds\n\n\n\n\n\n\nthumbnail_width\n\n\nIn pixels. E.g.: 50 for generating a thumbnail of width 50px\n\n\n\n\n\n\nthumbnail_height\n\n\nIn pixels E.g.: 50 for generating a thumbnail of height 50px\n\n\n\n\n\n\n\n\nModels\n\n\nProfile\n\n\nFor both profile create and update, the following json needs to be sent. \n\n\n\n\nFor create, perform a HTTP POST request to /api/Profiles \n\n\nFor Update perform a HTTP PUT request to /api/Profiles/{id} endpoint\n\n\n\n\n{\n  \nName\n: \nunique_profile_name_with_no_spaces\n,\n  \nType\n: \nTranscode | Hls | GenerateThumbnails\n,\n  \nParameters\n: {\n    \nkey\n: \nvalue\n,\n    \nkey2\n: \nvalue2\n\n  }\n}\n\n\n\n\nJob\n\n\n{\n  \nName\n: \nName\n,\n  \nSla\n: 2,\n  \nNotification\n: {\n      \nSuccessCallback\n: \ninternal url\n,\n      \nErrorCallback\n: \ninternal url\n\n    },\n  \nOperations\n: [\n    {\n      \nType\n: \nStitch\n,\n      \nProfiles\n: [],\n      \nSources\n: [\n        {\n          \nType\n: \nS3\n,\n          \nPath\n: \nhttps://input1\n\n        },\n        {\n          \nType\n: \nS3\n,\n          \nPath\n: \nhttps://input2\n\n        }\n      ],\n      \nDestinations\n: [\n        {\n          \nType\n: \nS3\n,\n          \nPath\n: \nhttps://output\n\n        }\n      ]\n    },\n    {\n      \nType\n: \nTranscode\n,\n      \nProfiles\n: [\ngeneric-hls-144p\n],\n      \nSources\n: [\n        {\n          \nType\n: \nS3\n,\n          \nPath\n: \nhttps://input\n\n        }\n      ],\n      \nDestinations\n: [\n        {\n          \nType\n: \nS3\n,\n          \nPath\n: \nhttps://output\n\n        }\n      ], \n      \nMetadata\n: {\n          \nProbe\n: true,\n          \nCallback\n: \ncallback url\n\n        } \n    },\n    {\n      \nType\n: \nHls\n,\n      \nProfiles\n: [\ngeneric-hls-240p\n, \nmy-custom-480p\n],\n      \nSources\n: [\n        {\n          \nType\n: \nS3\n,\n          \nPath\n: \nhttps://input\n\n        }\n      ],\n      \nDestinations\n: [\n        {\n          \nType\n: \nS3\n,\n          \nPath\n: \nhttps://output\n,\n          \nAcl\n: \nprivate|public-read|public-read-write|authenticated-read|bucket-owner-read|bucket-owner-full-control\n,\n        },\n      {\n      \nType\n: \nGenerateThumbnails\n,\n      \nProfiles\n: [\nthumb_10_50_50\n],\n      \nSources\n: [\n        {\n          \nType\n: \nS3\n,\n          \nPath\n: \nhttps://input\n\n        }\n      ],\n      \nDestinations\n: [\n        {\n          \nType\n: \nS3\n,\n          \nPath\n: \nhttps://output/thumb.jpg\n\n        }\n      ]\n    }\n  ]\n}\n\n\n\n\n\nCallbacks\n\n\nSuccess\n\n\n{\n  \nProcessId\n: \nstring\n,\n  \nStartedOn\n: \nstart date time in universal time format\n,\n  \nCompletedOn\n: \ncompletion date time in universal time format\n\n}\n\n\n\n\nStatus Callbacks\n\n\nThere are 2 types of status callbacks, one for the overall progress and the other for individual progresses for each of the profiles selected.\n\n\n\n\nConsolidated Callback\n\n\n\n\nhttps://media.batchly.net/api/jobs/{id}\n\n\n\n\nThe response would be\n\n\nProcessID\n: \nstring\n\n\nStartedOn\n:\nstart date time in universal time format\n,\n\nCompletedOn\n:\ncompletion date time in universal time format\n,\n\nProgress\n: \npercentage\n\n\nStatusCode\n:\noverall job status\n,|2|3|4|\n\nStatus\n:\nstatus message\n |Started|Success|Error|\n\n\n\n\n\n\nIndividual profile status syntax\n\n\n\n\nhttps://media.batchly.net/api/jobs/{id}?profile={profileName}\n\n\n\n\nThe response for each profile would be:\n\n\nProcessID\n: \nstring\n\n\nStartedOn\n:\nstart date time in universal time format\n,\n\nCompletedOn\n:\ncompletion date time in universal time format\n,\n\nProgress\n:\nin percentage\n,\n\nStatus\n:\nstatus message\n,   \n\nSubStatus\n:\nsubstatus message\n,\n\nFileName\n: \noutput filename\n\n\n\n\n\nError\n\n\n{\n  \nProcessId\n: \nstring\n,\n  \nStartedOn\n: \nstart date time in universal time format\n,\n  \nCompletedOn\n: \ncompletion date time in universal time format\n,\n  \nErrorCode\n: \nsystem error code\n,\n  \nErrorMessage\n: \nfailure reason\n\n}\n\n\n\n\nMetadata\n\n\n{\n  \nProcessId\n: \nstring\n,\n  \nSource\n: \nfile path\n,\n  \nMetadata\n: \nprobe result in json string\n\n}\n\n\n\n\nProceed to \nMedia Status Codes\n or back to \nMedia Overview", 
            "title": "Media Operations"
        }, 
        {
            "location": "/media-api/media-operations/#media-operations-supported", 
            "text": "HTTP Method  Endpoint  Description      GET  api/Profiles  Returns all the profiles available in batch.ly    GET  api/Profiles/{id}  Returns parameters of a particular profile    POST  api/Profiles  Create a new private profile for your usage.    PUT  api/Profiles/{id}  Edit a given private profile with updated information    DELETE  api/Profiles/{id}  Removes a private profile.    POST  api/Jobs  Creates a new transcoding job with the formats specified in the JSON request    GET  api/Jobs/{id}  Get the status of a specific job    GET  api/Jobs/{id}?profile={profileName}  Get the status of a specific transcoding within a media job", 
            "title": "Media Operations Supported"
        }, 
        {
            "location": "/media-api/media-operations/#operations", 
            "text": "The following Operations are supported as part of the Create Job call.  The rules associated with entities are described below.     Operation  Sources  Profiles  Destinations  Remarks      Transcode  One Path  One or More Profiles  One Destination  The source file is transcoded to the given profiles.  The destination file name is appended with profile name when creating output    Stitch  At Least Two Paths  None  One Destination  Operation combines the input files to create the destination output    Hls  One Path  At Least Two Profiles  One Destination  The source is converted into many profiles and a destination .m3u8 file created with the appropriate references    GetMetadata  One or more Paths  NA  NA  Individual source path meta data is sent back as webhooks to the given callback url    GenerateThumbnails  One Path  One or More Profiles  One Destination  Source file is used to snapshot to create thumbnails based on profiles.  Destination file format is used and appended with index of images.     Support for additional operations are currently in development and the document will be updated to reflect the same in near future.", 
            "title": "Operations"
        }, 
        {
            "location": "/media-api/media-operations/#profile-settings", 
            "text": "", 
            "title": "Profile Settings"
        }, 
        {
            "location": "/media-api/media-operations/#transcode-and-hls-settings", 
            "text": "Batchly has in-built profiles for most of the common MP4 and HLS output formats. However, you have flexibility to add or modify the values.     Field Name  Allowed Values      format  mp4, hls    video_codec  libx264    video_bitrate  Integer e.g represent 100k as 100000    minrate  Integer e.g represent 100k as 100000    maxrate  Integer e.g represent 100k as 100000    bufsize  Integer e.g represent 100k as 100000    video_width  Integer e.g 1084    video_height  Integer e.g 720    framerate  Integer e.g. 15, 30    group_of_pictures  Integer. Typically, double the framerate    video_preset  fast, medium, slow, slower, veryslow, placebo. Default - Medium    profile  high, main, baseline    level  3.0, 3.1, 4.0, 4.1, 4.2    video_disable  yes, no    video_disable_subtitle  yes,no    audio_codec  aac,ac3    audio_channels  e.g 2    audio_bitrate  Integer e.g represent 100k as 100000    audio_samplerate  E.g 44100, 48000    x264opts  H.264 specific options    movflags  faststart moves the video info to header    hls_time  segment length. Recommended value 9    hls_list_size  Set to 0 for all VOD playlist entries    hls_flags  If set to 'single_file', all segments will be stored in 1 ts file", 
            "title": "Transcode and Hls Settings"
        }, 
        {
            "location": "/media-api/media-operations/#thumbnail-settings", 
            "text": "You can create custom thumbnail profiles by using the following field values.     Field Name  Allowed Values      thumbnail_time  In seconds. E.g.: 10 for generating thumbnail every 10 seconds    thumbnail_width  In pixels. E.g.: 50 for generating a thumbnail of width 50px    thumbnail_height  In pixels E.g.: 50 for generating a thumbnail of height 50px", 
            "title": "Thumbnail Settings"
        }, 
        {
            "location": "/media-api/media-operations/#models", 
            "text": "", 
            "title": "Models"
        }, 
        {
            "location": "/media-api/media-operations/#profile", 
            "text": "For both profile create and update, the following json needs to be sent.    For create, perform a HTTP POST request to /api/Profiles   For Update perform a HTTP PUT request to /api/Profiles/{id} endpoint   {\n   Name :  unique_profile_name_with_no_spaces ,\n   Type :  Transcode | Hls | GenerateThumbnails ,\n   Parameters : {\n     key :  value ,\n     key2 :  value2 \n  }\n}", 
            "title": "Profile"
        }, 
        {
            "location": "/media-api/media-operations/#job", 
            "text": "{\n   Name :  Name ,\n   Sla : 2,\n   Notification : {\n       SuccessCallback :  internal url ,\n       ErrorCallback :  internal url \n    },\n   Operations : [\n    {\n       Type :  Stitch ,\n       Profiles : [],\n       Sources : [\n        {\n           Type :  S3 ,\n           Path :  https://input1 \n        },\n        {\n           Type :  S3 ,\n           Path :  https://input2 \n        }\n      ],\n       Destinations : [\n        {\n           Type :  S3 ,\n           Path :  https://output \n        }\n      ]\n    },\n    {\n       Type :  Transcode ,\n       Profiles : [ generic-hls-144p ],\n       Sources : [\n        {\n           Type :  S3 ,\n           Path :  https://input \n        }\n      ],\n       Destinations : [\n        {\n           Type :  S3 ,\n           Path :  https://output \n        }\n      ], \n       Metadata : {\n           Probe : true,\n           Callback :  callback url \n        } \n    },\n    {\n       Type :  Hls ,\n       Profiles : [ generic-hls-240p ,  my-custom-480p ],\n       Sources : [\n        {\n           Type :  S3 ,\n           Path :  https://input \n        }\n      ],\n       Destinations : [\n        {\n           Type :  S3 ,\n           Path :  https://output ,\n           Acl :  private|public-read|public-read-write|authenticated-read|bucket-owner-read|bucket-owner-full-control ,\n        },\n      {\n       Type :  GenerateThumbnails ,\n       Profiles : [ thumb_10_50_50 ],\n       Sources : [\n        {\n           Type :  S3 ,\n           Path :  https://input \n        }\n      ],\n       Destinations : [\n        {\n           Type :  S3 ,\n           Path :  https://output/thumb.jpg \n        }\n      ]\n    }\n  ]\n}", 
            "title": "Job"
        }, 
        {
            "location": "/media-api/media-operations/#callbacks", 
            "text": "Success  {\n   ProcessId :  string ,\n   StartedOn :  start date time in universal time format ,\n   CompletedOn :  completion date time in universal time format \n}  Status Callbacks  There are 2 types of status callbacks, one for the overall progress and the other for individual progresses for each of the profiles selected.   Consolidated Callback   https://media.batchly.net/api/jobs/{id}  The response would be  ProcessID :  string  StartedOn : start date time in universal time format , CompletedOn : completion date time in universal time format , Progress :  percentage  StatusCode : overall job status ,|2|3|4| Status : status message  |Started|Success|Error|   Individual profile status syntax   https://media.batchly.net/api/jobs/{id}?profile={profileName}  The response for each profile would be:  ProcessID :  string  StartedOn : start date time in universal time format , CompletedOn : completion date time in universal time format , Progress : in percentage , Status : status message ,    SubStatus : substatus message , FileName :  output filename   Error  {\n   ProcessId :  string ,\n   StartedOn :  start date time in universal time format ,\n   CompletedOn :  completion date time in universal time format ,\n   ErrorCode :  system error code ,\n   ErrorMessage :  failure reason \n}  Metadata  {\n   ProcessId :  string ,\n   Source :  file path ,\n   Metadata :  probe result in json string \n}  Proceed to  Media Status Codes  or back to  Media Overview", 
            "title": "Callbacks"
        }, 
        {
            "location": "/media-api/media-codes/", 
            "text": "Media Codes\n\n\nStatus Values\n\n\n\n\n\n\n\n\nCode\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\n101\n\n\nJob Starting\n\n\n\n\n\n\n102\n\n\nJob Processing\n\n\n\n\n\n\n103\n\n\nJob Error\n\n\n\n\n\n\n104\n\n\nJob Success\n\n\n\n\n\n\n110\n\n\nUnknown\n\n\n\n\n\n\n\n\nSubstatus Values\n\n\n\n\n\n\n\n\nCode\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\n400\n\n\nOperation is starting\n\n\n\n\n\n\n500\n\n\nFile Downloading\n\n\n\n\n\n\n600\n\n\nFile Uploading\n\n\n\n\n\n\n700\n\n\nFile transcoding\n\n\n\n\n\n\n800\n\n\nFile stitching\n\n\n\n\n\n\n900\n\n\nDetecting blackband values\n\n\n\n\n\n\n901\n\n\nRemove blackband\n\n\n\n\n\n\n1000\n\n\nCreate master playlist (for Hls)\n\n\n\n\n\n\n1100\n\n\nCreate thumbnails\n\n\n\n\n\n\n1200\n\n\nOperation completed\n\n\n\n\n\n\n\n\nError Code Values\n\n\n\n\n\n\n\n\nCode\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\n80\n\n\nIncorrect input profile parameters\n\n\n\n\n\n\n91\n\n\nSQS setup failed in AWS account\n\n\n\n\n\n\n501\n\n\nS3 bucket not found or incorrect access permission\n\n\n\n\n\n\n502\n\n\nFile download failed\n\n\n\n\n\n\n601\n\n\nFile upload failed\n\n\n\n\n\n\n701\n\n\nTranscode operation failed\n\n\n\n\n\n\n801\n\n\nStitching operation failed\n\n\n\n\n\n\n901\n\n\nMetadata operation setup failed\n\n\n\n\n\n\n902\n\n\nMetadata operation failed\n\n\n\n\n\n\n903\n\n\nFailed to get output bitrate\n\n\n\n\n\n\n904\n\n\nFailed to get metadata\n\n\n\n\n\n\n905\n\n\nFailed to create master playlist (for Hls)\n\n\n\n\n\n\n906\n\n\nFailed to detect blackband\n\n\n\n\n\n\n907\n\n\nFailed to remove blackband", 
            "title": "Media Status Codes"
        }, 
        {
            "location": "/media-api/media-codes/#media-codes", 
            "text": "", 
            "title": "Media Codes"
        }, 
        {
            "location": "/media-api/media-codes/#status-values", 
            "text": "Code  Meaning      101  Job Starting    102  Job Processing    103  Job Error    104  Job Success    110  Unknown", 
            "title": "Status Values"
        }, 
        {
            "location": "/media-api/media-codes/#substatus-values", 
            "text": "Code  Meaning      400  Operation is starting    500  File Downloading    600  File Uploading    700  File transcoding    800  File stitching    900  Detecting blackband values    901  Remove blackband    1000  Create master playlist (for Hls)    1100  Create thumbnails    1200  Operation completed", 
            "title": "Substatus Values"
        }, 
        {
            "location": "/media-api/media-codes/#error-code-values", 
            "text": "Code  Meaning      80  Incorrect input profile parameters    91  SQS setup failed in AWS account    501  S3 bucket not found or incorrect access permission    502  File download failed    601  File upload failed    701  Transcode operation failed    801  Stitching operation failed    901  Metadata operation setup failed    902  Metadata operation failed    903  Failed to get output bitrate    904  Failed to get metadata    905  Failed to create master playlist (for Hls)    906  Failed to detect blackband    907  Failed to remove blackband", 
            "title": "Error Code Values"
        }
    ]
}