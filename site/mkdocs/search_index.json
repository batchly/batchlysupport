{
    "docs": [
        {
            "location": "/", 
            "text": "OVERVIEW\n\n\nWelcome to the documentation for Batchly - the automated AWS cost-reduction platform. This website aims to document every feature of Batchly from top to bottom, covering as much detail as possible. If you are just getting started with Batchly, it is highly recommended that you begin with the \nGetting Started\n guide first and then return to this page.\n\n\nThe navigation will take you through each component of Batchly. Click on a navigation item to get started, or read more about why developers, executives, and operators choose Batchly for their needs.", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "Welcome to the documentation for Batchly - the automated AWS cost-reduction platform. This website aims to document every feature of Batchly from top to bottom, covering as much detail as possible. If you are just getting started with Batchly, it is highly recommended that you begin with the  Getting Started  guide first and then return to this page.  The navigation will take you through each component of Batchly. Click on a navigation item to get started, or read more about why developers, executives, and operators choose Batchly for their needs.", 
            "title": "OVERVIEW"
        }, 
        {
            "location": "/about/whybatchly/", 
            "text": "Why Batchly?\n\n\nBatchly is a solution that fully automates AWS cost management (using spot instances and instance rightsizing using autoscaling) in a way that makes the ROI impossible to ignore.\n\n\nAWS has 11 regions with 2 to 3 AZs in each region, 50 instance types, and many instance categories (like Spot, Reserved, and On-demand). Identifying the right infrastructure for your application(s) from these options is not a trivial activity. It is complex, costly, and most times suboptimal.\n\n\nBatchly takes out all the complexities associated with Cloud management by automating all the activities, resulting in up to 90% reduction in AWS costs.\n\n\n\n\nBatchly takes out all the complexities associated with Cloud management by automating all the activities resulting in up to 90% reduction in AWS costs. \n\n\n\n\nWhy Batchly Benefits You\n\n\nBatchly enables both internet and traditional enterprises to automatically benefit from AWS cost and usage savings by optimizing workloads with spot instances and EC2 smart sizing. No matter what you use AWS for, Batchly helps you to reduce your cost (by up to 90%) in a frictionless manner.\n\n\nFeatures that customers love:\n\n\n\n\n\n\nAWS spot bid optimization seamlessly deploys, moves, and manages workloads over high cost savings spot instances across any AWS region.\n\n\n\n\n\n\nEC2 instance smart sizing realizes ultimate EC2 efficiencies with the automatic instance sizing for the right application / workload.\n\n\n\n\n\n\nSeamless integration easily works with pre-built apps for the most common enterprise applications (JMeter, Spark, Hive, PIG, Jenkins, Elastic BeanStalk, and more) and REST API support provides a smooth experience.\n\n\n\n\n\n\nFully managed benefit from cost and utilization optimizations from within your own AWS account while we do the heavy lifting.\n\n\n\n\n\n\nSecured with organic and strong AWS security controls.", 
            "title": "Why Batchly"
        }, 
        {
            "location": "/about/whybatchly/#why-batchly", 
            "text": "Batchly is a solution that fully automates AWS cost management (using spot instances and instance rightsizing using autoscaling) in a way that makes the ROI impossible to ignore.  AWS has 11 regions with 2 to 3 AZs in each region, 50 instance types, and many instance categories (like Spot, Reserved, and On-demand). Identifying the right infrastructure for your application(s) from these options is not a trivial activity. It is complex, costly, and most times suboptimal.  Batchly takes out all the complexities associated with Cloud management by automating all the activities, resulting in up to 90% reduction in AWS costs.   Batchly takes out all the complexities associated with Cloud management by automating all the activities resulting in up to 90% reduction in AWS costs.", 
            "title": "Why Batchly?"
        }, 
        {
            "location": "/about/whybatchly/#why-batchly-benefits-you", 
            "text": "Batchly enables both internet and traditional enterprises to automatically benefit from AWS cost and usage savings by optimizing workloads with spot instances and EC2 smart sizing. No matter what you use AWS for, Batchly helps you to reduce your cost (by up to 90%) in a frictionless manner.  Features that customers love:    AWS spot bid optimization seamlessly deploys, moves, and manages workloads over high cost savings spot instances across any AWS region.    EC2 instance smart sizing realizes ultimate EC2 efficiencies with the automatic instance sizing for the right application / workload.    Seamless integration easily works with pre-built apps for the most common enterprise applications (JMeter, Spark, Hive, PIG, Jenkins, Elastic BeanStalk, and more) and REST API support provides a smooth experience.    Fully managed benefit from cost and utilization optimizations from within your own AWS account while we do the heavy lifting.    Secured with organic and strong AWS security controls.", 
            "title": "Why Batchly Benefits You"
        }, 
        {
            "location": "/user-guide/getting-started/", 
            "text": "Getting Started\n\n\nTo get started with Batchly, go to the \nRegistration page\n and \ncreate a new Batchly Account\n.\n\n\nTo Register for Batchly, enter the following details :\n\n\n\n\n\n\nEnter your \nFull Name\n\n\n\n\n\n\nEnter your \nOrganization Name\n\n\n\n\n\n\nEnter your Organization Name or any desired name to set \ndomain name\n\n\n\n\n\n\nEnter your valid \nEmail Address\n\n\n\n\n\n\nEnter your valid \nPhone Number\n\n\n\n\n\n\nEnter your \nPassword\n\n\n\n\n\n\nClick \nCreate Account\n button.You may be asked to select the Captcha to let us know that you are human.\n\n\n\n\n\n\n\n\nFirst step after signing up - Check your Email\n\n\nAfter signing up, you will receive an email on your registered email address for verification. After verification completion, you can successfully sign in to Batchly.\n\n\nSign in Process - Wizard\n\n\nWhen you sign up for the very first time from your customized domain (for example : http:// yxz.batchly.net), you will go through the Wizard page. Here you will be asked to follow a few steps in order to set up everything, which you will be required to use on an app later.\n\n\nStep 1:\n Create an account.\n\n\nStep 2:\n Create a project.\n\n\nStep 3:\n Select any app.\n\n\n\n\nAdd AWS Account\n\n\nClick the \nAdd AWS Account\n and you will be redirected to another page where following details needs to be given to add an account.\n\n\n\n\n\n\nEnter Account Name.\n\n\n\n\n\n\nSelect the Region.\n\n\n\n\n\n\nEnter your Access Key.\n\n\n\n\n\n\nEnter your Secret Key.\n\n\n\n\n\n\nNote:\n Before adding an account, the customer needs to have their own AWS account.\n\n\n\n\nAdd Project\n\n\nImportant:\n \n\n\n\n\n\n\nBefore adding a project, the customer needs to have their own AWS account.\n\n\n\n\n\n\nWhen you create an account, a default project will be created without VPC. So, creating a project is not mandatory. This can be done later.\n\n\n\n\n\n\nThe projects section lets you add a project under an AWS account. The following steps will show how to add a new project:\n\n\n\n\n\n\nEnter Project Name.\n\n\n\n\n\n\nSelect the Account under which you want to add the project.\n\n\n\n\n\n\nSelect the Cloud region.\n\n\n\n\n\n\nSelect the \nVirtual Private Cloud\n. (This is optional.)\n\nNote:\n If you wish to work on \nJMeter\n app then this VPC will be mandatory.\n\n\n\n\n\n\nSelect the VPC information. (This needs to be done only if you have selected VPC)\n\n\n\n\n\n\nSelect the subnet information. (This needs to be done only if you have selected VPC)\n\n\n\n\n\n\n\n\nSelect App\n\n\nOnce you're done with adding account and project, you can select any app you want to work on.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/getting-started/#getting-started", 
            "text": "To get started with Batchly, go to the  Registration page  and  create a new Batchly Account .  To Register for Batchly, enter the following details :    Enter your  Full Name    Enter your  Organization Name    Enter your Organization Name or any desired name to set  domain name    Enter your valid  Email Address    Enter your valid  Phone Number    Enter your  Password    Click  Create Account  button.You may be asked to select the Captcha to let us know that you are human.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/user-guide/getting-started/#first-step-after-signing-up-check-your-email", 
            "text": "After signing up, you will receive an email on your registered email address for verification. After verification completion, you can successfully sign in to Batchly.", 
            "title": "First step after signing up - Check your Email"
        }, 
        {
            "location": "/user-guide/getting-started/#sign-in-process-wizard", 
            "text": "When you sign up for the very first time from your customized domain (for example : http:// yxz.batchly.net), you will go through the Wizard page. Here you will be asked to follow a few steps in order to set up everything, which you will be required to use on an app later.  Step 1:  Create an account.  Step 2:  Create a project.  Step 3:  Select any app.", 
            "title": "Sign in Process - Wizard"
        }, 
        {
            "location": "/user-guide/getting-started/#add-aws-account", 
            "text": "Click the  Add AWS Account  and you will be redirected to another page where following details needs to be given to add an account.    Enter Account Name.    Select the Region.    Enter your Access Key.    Enter your Secret Key.    Note:  Before adding an account, the customer needs to have their own AWS account.", 
            "title": "Add AWS Account"
        }, 
        {
            "location": "/user-guide/getting-started/#add-project", 
            "text": "Important:      Before adding a project, the customer needs to have their own AWS account.    When you create an account, a default project will be created without VPC. So, creating a project is not mandatory. This can be done later.    The projects section lets you add a project under an AWS account. The following steps will show how to add a new project:    Enter Project Name.    Select the Account under which you want to add the project.    Select the Cloud region.    Select the  Virtual Private Cloud . (This is optional.) Note:  If you wish to work on  JMeter  app then this VPC will be mandatory.    Select the VPC information. (This needs to be done only if you have selected VPC)    Select the subnet information. (This needs to be done only if you have selected VPC)", 
            "title": "Add Project"
        }, 
        {
            "location": "/user-guide/getting-started/#select-app", 
            "text": "Once you're done with adding account and project, you can select any app you want to work on.", 
            "title": "Select App"
        }, 
        {
            "location": "/user-guide/user-management/", 
            "text": "Change Password\n\n\nYou can change your Batchly password at anytime.\n\n\n\n\n\n\nSign in to your Bacthly account.\n\n\n\n\n\n\nClick on the name given on the top-right corner.\n\n\n\n\n\n\nSelect \nChange Password\n.\n\n\n\n\n\n\n\n\nEdit Profile Settings\n\n\nYou can change your profile settings ( \nEmail\n, \nName\n, \nRole\n ) at any time.\n\n\n\n\n\n\nYou can click on the name given on the top-right corner from the header.\n\n\n\n\n\n\nSelect \nEdit Password\n.\n\n\n\n\n\n\n\n\nSettings\n\n\nFrom the settings, you can generate \nAPI Keys\n. Also, you can \nrevoke\n those keys when not required.\n\n\n\n\n\n\nYou can click on the name given on the top-right corner from the header.\n\n\n\n\n\n\nSelect \nSettings\n.", 
            "title": "User Management"
        }, 
        {
            "location": "/user-guide/user-management/#change-password", 
            "text": "You can change your Batchly password at anytime.    Sign in to your Bacthly account.    Click on the name given on the top-right corner.    Select  Change Password .", 
            "title": "Change Password"
        }, 
        {
            "location": "/user-guide/user-management/#edit-profile-settings", 
            "text": "You can change your profile settings (  Email ,  Name ,  Role  ) at any time.    You can click on the name given on the top-right corner from the header.    Select  Edit Password .", 
            "title": "Edit Profile Settings"
        }, 
        {
            "location": "/user-guide/user-management/#settings", 
            "text": "From the settings, you can generate  API Keys . Also, you can  revoke  those keys when not required.    You can click on the name given on the top-right corner from the header.    Select  Settings .", 
            "title": "Settings"
        }, 
        {
            "location": "/iam-access/iam-access/", 
            "text": "Steps to create IAM Access\n\n\nBatchly provides an easy way to add your AWS account. All it needs is for you to provide your AWS Master Key and Access Key, and Batchly automatically creates a separate IAM access with privileges to launch EC2 instances (Spot and On-demand), as well as monitor them.\n\n\nHowever, if for some reason you don\u2019t want to provide your AWS keys, you can create an IAM user for Batchly by following the steps given below:\n\n\n\n\n\n\nGo to AWS IAM dashboard\n\n\n\n\n\n\nIn the left hand pane, click on user.\n\n\n\n\n\n\nGive the user a name, then select Generate to give an access key to each user and click Create.\n\n\n\n\n\n\n\n\nDownload the credentials. At the bottom right corner, there\u2019s a download credentials button.\n\n\n\n\nIn the Users listing, click on the user and in the next screen at the bottom, click on Inline Policies.\n\n\n\n\n\n\nSelect Custom Policy and it opens up a policy editor. Give the policy a name, such as  Batchly-Policy, and add the policy below.\n\n\n\n\n\n\n{\n    \nVersion\n: \n2012-10-17\n,\n    \nStatement\n: [\n        {\n            \nSid\n: \nStmt1455011811000\n,\n            \nEffect\n: \nAllow\n,\n            \nAction\n: [\n                \niam:GetUser\n,\n                \niam:AddRoleToInstanceProfile\n,\n                \niam:CreateInstanceProfile\n,\n                \niam:CreateRole\n,\n                \niam:GetRole\n,\n                \niam:ListInstanceProfiles\n,\n                \niam:ListRoles\n,\n                \niam:PutRolePolicy\n,\n                \niam:RemoveRoleFromInstanceProfile\n,\n                \niam:UpdateAssumeRolePolicy\n,\n                \niam:PassRole\n,\n                \niam:GetInstanceProfile\n\n            ],\n            \nResource\n: [\n        \narn:aws:iam::XXXXXXXXXXXXX:role/batchly/\n\n                \narn:aws:iam::XXXXXXXXXXXXX:role/batchly/batchly-trusted-role\n,\n        \narn:aws:iam::XXXXXXXXXXXXX:instance-profile/*\n\n        \narn:aws:iam::XXXXXXXXXXXXX:policy/batchly-access-policy\n,\n        \narn:aws:iam::XXXXXXXXXXXXX:user/USERNAME\n\n            ]\n        }\n    ]\n}\n\n\n\n\nNote:\n In the JSON, replace the XXXXXXXXXXXXX with the account number of the account in which you are creating the user. Replace the USERNAME with the username in the above policy statement.\n\n\nNext, apply the policy and use the credentials to add the AWS account in Batchly.\n\n\nPlease execute the above steps and share the credential information with us. This would enable us to add the role. The permission list clearly specifies the permission expected for adding the new role.\n\n\nOnce the account is added in Batchly, you can safely remove the user. In addition, once the account is added, you can also validate the newly added role and its permission list.", 
            "title": "Create IAM Access"
        }, 
        {
            "location": "/iam-access/iam-access/#steps-to-create-iam-access", 
            "text": "Batchly provides an easy way to add your AWS account. All it needs is for you to provide your AWS Master Key and Access Key, and Batchly automatically creates a separate IAM access with privileges to launch EC2 instances (Spot and On-demand), as well as monitor them.  However, if for some reason you don\u2019t want to provide your AWS keys, you can create an IAM user for Batchly by following the steps given below:    Go to AWS IAM dashboard    In the left hand pane, click on user.    Give the user a name, then select Generate to give an access key to each user and click Create.     Download the credentials. At the bottom right corner, there\u2019s a download credentials button.   In the Users listing, click on the user and in the next screen at the bottom, click on Inline Policies.    Select Custom Policy and it opens up a policy editor. Give the policy a name, such as  Batchly-Policy, and add the policy below.    {\n     Version :  2012-10-17 ,\n     Statement : [\n        {\n             Sid :  Stmt1455011811000 ,\n             Effect :  Allow ,\n             Action : [\n                 iam:GetUser ,\n                 iam:AddRoleToInstanceProfile ,\n                 iam:CreateInstanceProfile ,\n                 iam:CreateRole ,\n                 iam:GetRole ,\n                 iam:ListInstanceProfiles ,\n                 iam:ListRoles ,\n                 iam:PutRolePolicy ,\n                 iam:RemoveRoleFromInstanceProfile ,\n                 iam:UpdateAssumeRolePolicy ,\n                 iam:PassRole ,\n                 iam:GetInstanceProfile \n            ],\n             Resource : [\n         arn:aws:iam::XXXXXXXXXXXXX:role/batchly/ \n                 arn:aws:iam::XXXXXXXXXXXXX:role/batchly/batchly-trusted-role ,\n         arn:aws:iam::XXXXXXXXXXXXX:instance-profile/* \n         arn:aws:iam::XXXXXXXXXXXXX:policy/batchly-access-policy ,\n         arn:aws:iam::XXXXXXXXXXXXX:user/USERNAME \n            ]\n        }\n    ]\n}  Note:  In the JSON, replace the XXXXXXXXXXXXX with the account number of the account in which you are creating the user. Replace the USERNAME with the username in the above policy statement.  Next, apply the policy and use the credentials to add the AWS account in Batchly.  Please execute the above steps and share the credential information with us. This would enable us to add the role. The permission list clearly specifies the permission expected for adding the new role.  Once the account is added in Batchly, you can safely remove the user. In addition, once the account is added, you can also validate the newly added role and its permission list.", 
            "title": "Steps to create IAM Access"
        }, 
        {
            "location": "/jmeter/jmeter/", 
            "text": "Let's get started with JMeter\n\n\nStep 1:\n  Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.\n\n\nStep 2:\n You will be redirected to Batchly Dashboard. Next, click on the \nApp Store\n located in the header.\n\n\n\n\nStep 3:\n You will be redirected to the App store which has the apps supported on Batchly. To run JMeter, click the \nGet Started\n button in the JMeter App.\n\n\n\n\nStep 4:\n Now, to run JMeter job, fill all the required given text fields. There are following text fields to be filled: \n\n\n\n\n\n\nJob Name:\n You can give any desired name to your job.\n\n\n\n\n\n\nProject:\n Select the associated project to run the JMeter job.\n\n\n\n\n\n\nTest Plan:\n Upload the JMeter test plan (JMX file) from your local system which you want to execute.\n\n\n\n\n\n\nOutput Location:\n Give the Amazon S3 bucket output location.\n\n\n\n\n\n\nInstance Count:\n Give the number of instance on which you want to run your test job.\n\n\n\n\n\n\nInstance Type:\n Select the Amazon EC2 instance type on which you want to run your test plan.\n\n\n\n\n\n\n\n\nStep 5:\n Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.\n\n\nStep 6:\n On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).\n\n\n\n\nStep 7:\n You can monitor the job progress using the Job Run Details page.", 
            "title": "JMeter"
        }, 
        {
            "location": "/jmeter/jmeter/#lets-get-started-with-jmeter", 
            "text": "Step 1:   Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.  Step 2:  You will be redirected to Batchly Dashboard. Next, click on the  App Store  located in the header.   Step 3:  You will be redirected to the App store which has the apps supported on Batchly. To run JMeter, click the  Get Started  button in the JMeter App.   Step 4:  Now, to run JMeter job, fill all the required given text fields. There are following text fields to be filled:     Job Name:  You can give any desired name to your job.    Project:  Select the associated project to run the JMeter job.    Test Plan:  Upload the JMeter test plan (JMX file) from your local system which you want to execute.    Output Location:  Give the Amazon S3 bucket output location.    Instance Count:  Give the number of instance on which you want to run your test job.    Instance Type:  Select the Amazon EC2 instance type on which you want to run your test plan.     Step 5:  Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.  Step 6:  On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).   Step 7:  You can monitor the job progress using the Job Run Details page.", 
            "title": "Let's get started with JMeter"
        }, 
        {
            "location": "/ffmpeg/ffmpeg/", 
            "text": "Let's get started with FFmpeg\n\n\nStep 1:\n  Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.\n\n\nStep 2:\n You will be redirected to Batchly Dashboard. Next, click on the \nApp Store\n located in the header.\n\n\n\n\nStep 3:\n You will be redirected to the App store which has the apps supported on Batchly. To run JMeter, click the \nGet Started\n button in the FFmpeg App.\n\n\n\n\nStep 4:\n Now, to run FFmpeg job, fill all the required given text fields. There are following text fields to be filled:\n\n\nJob Name:\n You can give any desired name to your job.\n\n\nProject:\n Select the associated project to run the job.\n\n\nDefault AWS Region:\n Select the AWS region.\n\n\nTime Limit:\n Give the time limit to run a job in hours.\n\n\nProfile:\n Select the profile type in which you want to transcode.\n\n\nFilename Suffix:\n Give any desired suffix to differentiate input file from the output file.\n\n\nSource S3 Bucket Name:\n Give the Amazon S3 bucket Bucket name.\n\n\nFolder Name:\n Give the folder name, if available.\n\n\nDestination S3 Bucket Name:\n Give the Amazon S3 Bucket name.\n\n\nFolder Name:\n Give the folder name, if available.\n\n\n\n\nStep 5:\n Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.\n\n\nStep 6:\n On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).\n\n\n\n\nStep 7:\n You can monitor the job progress using the Job Run Details page.", 
            "title": "FFmpeg"
        }, 
        {
            "location": "/ffmpeg/ffmpeg/#lets-get-started-with-ffmpeg", 
            "text": "Step 1:   Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.  Step 2:  You will be redirected to Batchly Dashboard. Next, click on the  App Store  located in the header.   Step 3:  You will be redirected to the App store which has the apps supported on Batchly. To run JMeter, click the  Get Started  button in the FFmpeg App.   Step 4:  Now, to run FFmpeg job, fill all the required given text fields. There are following text fields to be filled:  Job Name:  You can give any desired name to your job.  Project:  Select the associated project to run the job.  Default AWS Region:  Select the AWS region.  Time Limit:  Give the time limit to run a job in hours.  Profile:  Select the profile type in which you want to transcode.  Filename Suffix:  Give any desired suffix to differentiate input file from the output file.  Source S3 Bucket Name:  Give the Amazon S3 bucket Bucket name.  Folder Name:  Give the folder name, if available.  Destination S3 Bucket Name:  Give the Amazon S3 Bucket name.  Folder Name:  Give the folder name, if available.   Step 5:  Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the \u2018Jobs\u2019 page.  Step 6:  On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).   Step 7:  You can monitor the job progress using the Job Run Details page.", 
            "title": "Let's get started with FFmpeg"
        }, 
        {
            "location": "/imagemagick/imagemagick/", 
            "text": "Let's get started with ImageMagick\n\n\nStep 1:\n  Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.\n\n\nStep 2:\n You will be redirected to Batchly Dashboard. Next, click on the \nApp Store\n located in the header.\n\n\n\n\nStep 3:\n You will be redirected to the App store which has the apps supported on Batchly. To run ImageMagick, click the \nGet Started\n button in the \nImageMagick\n App.\n\n\n\n\nStep 4:\n Now, to run ImageMagick job, fill all the required given text fields.\n\n\nJob Name:\n You can give any desired name to your job.\n\n\nProject:\n Select the associated project to run the job.\n\n\n\n\nDefault AWS Region:\n Select the AWS region.\n\n\nSource S3 Bucket Name:\n Give the Amazon S3 bucket Bucket name.\n\n\nFolder Name:\n Give the folder name, if available.\n\n\nDestination S3 Bucket Name:\n Give the Amazon S3 Bucket name.\n\n\nFolder Name:\n Give the folder name, if available.\n\n\n\n\nTime Limit:\n Give the time limit to run a job in hours.\n\n\nDefault AWS Region:\n Select the AWS region.\n\n\nRESIZE:\n Give any desired size for the output file.\n\n\n\n\nStep 5:\n Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the \nJobs\n page.\n\n\nStep 6:\n On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).\n\n\nStep 7:\n You can monitor the job progress using the Job Run Details page.", 
            "title": "ImageMagick"
        }, 
        {
            "location": "/imagemagick/imagemagick/#lets-get-started-with-imagemagick", 
            "text": "Step 1:   Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.  Step 2:  You will be redirected to Batchly Dashboard. Next, click on the  App Store  located in the header.   Step 3:  You will be redirected to the App store which has the apps supported on Batchly. To run ImageMagick, click the  Get Started  button in the  ImageMagick  App.   Step 4:  Now, to run ImageMagick job, fill all the required given text fields.  Job Name:  You can give any desired name to your job.  Project:  Select the associated project to run the job.   Default AWS Region:  Select the AWS region.  Source S3 Bucket Name:  Give the Amazon S3 bucket Bucket name.  Folder Name:  Give the folder name, if available.  Destination S3 Bucket Name:  Give the Amazon S3 Bucket name.  Folder Name:  Give the folder name, if available.   Time Limit:  Give the time limit to run a job in hours.  Default AWS Region:  Select the AWS region.  RESIZE:  Give any desired size for the output file.   Step 5:  Click on the \u2018Add Job\u2019 button once you are done with filling all the details. This action will save your job and is available to see later on the  Jobs  page.  Step 6:  On successful job addition, you would get a popup where you can either start your job immediately (by clicking \u2018Execute the Job\u2019) or schedule your job to run later (by clicking on the button \u2018Schedule the Job\u2019).  Step 7:  You can monitor the job progress using the Job Run Details page.", 
            "title": "Let's get started with ImageMagick"
        }, 
        {
            "location": "/bigdata/spark/", 
            "text": "Let's get started with Big Data Apps\n\n\nBig Data has following apps which are almost similar to each other in term of using it (or information required to run these apps): \n\nHive\n, \nPig\n, \nSpark\n, \nHadoop\n\n\nThis demonstration will show on how to use Spark app.You may select any other Big data app instead of Spark.(since , all these apps are similar)\n\n\nStep 1:\n  Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.\n\n\nStep 2:\n You will be redirected to Batchly Dashboard. Next, click on the \nApp Store\n located in the header.\n\n\n\n\nStep 3:\n You will be redirected to the App store which has the apps supported on Batchly. To run Spark, click the \nGet Started\n button in the Spark App.\n\n\n\n\nStep 4:\n Now, to run Spark job, fill all the required given text fields. There are following text fields to be filled: \n\n\nJob Name:\n You can give any desired name to your job. \n\n\nProject:\n Select the associated project to run the job.\n\n\nCluster Name:\n Select the cluster name. \n\n\nDeploy Mode:\n Select the deploy mode.\n\n\nSpark-submit Options:\n Enter Spark-submit if you have any.\n\n\nApplication Location:\n Enter Application Location.\n\n\nArguments:\n Enter Arguments if you have any.\n\n\nAction on Failure:\n Select the Action on Failure.\n\n\nInstance Type:\n Select the Instance Type\n\n\n\n\nStep 5:\n Click on the \nAdd Job\n button once you are done with filling all the details. This action will save your job and is available to see later on the \nJobs\n page.\n\n\nStep 6:\n On successful job addition, you would get a popup where you can either start your job immediately (by clicking \nExecute the Job\n) or schedule your job to run later (by clicking on the button \nSchedule the Job\n).\n\n\n\n\nStep 7:\n You can monitor the job progress using the \nJob Run Details\n page.", 
            "title": "Big Data Apps"
        }, 
        {
            "location": "/bigdata/spark/#lets-get-started-with-big-data-apps", 
            "text": "Big Data has following apps which are almost similar to each other in term of using it (or information required to run these apps):  Hive ,  Pig ,  Spark ,  Hadoop  This demonstration will show on how to use Spark app.You may select any other Big data app instead of Spark.(since , all these apps are similar)  Step 1:   Login to your Batchly Console Application (your-domain.batchly.net) using registered Email Id and Password.  Step 2:  You will be redirected to Batchly Dashboard. Next, click on the  App Store  located in the header.   Step 3:  You will be redirected to the App store which has the apps supported on Batchly. To run Spark, click the  Get Started  button in the Spark App.   Step 4:  Now, to run Spark job, fill all the required given text fields. There are following text fields to be filled:   Job Name:  You can give any desired name to your job.   Project:  Select the associated project to run the job.  Cluster Name:  Select the cluster name.   Deploy Mode:  Select the deploy mode.  Spark-submit Options:  Enter Spark-submit if you have any.  Application Location:  Enter Application Location.  Arguments:  Enter Arguments if you have any.  Action on Failure:  Select the Action on Failure.  Instance Type:  Select the Instance Type   Step 5:  Click on the  Add Job  button once you are done with filling all the details. This action will save your job and is available to see later on the  Jobs  page.  Step 6:  On successful job addition, you would get a popup where you can either start your job immediately (by clicking  Execute the Job ) or schedule your job to run later (by clicking on the button  Schedule the Job ).   Step 7:  You can monitor the job progress using the  Job Run Details  page.", 
            "title": "Let's get started with Big Data Apps"
        }, 
        {
            "location": "/api/overview/", 
            "text": "Batchly REST API\n\n\nEach customer of batchly gets their own customized URL for access. Using the web console, you can generate new set of API access keys.  You need to safe keep the keys.  Batchly API SDK doesn\u2019t send the secret key in any request. Batchly supports two types of API authentication mechanisms -  HMAC which validates calls using request signature and ApiKey based.\n\n\nBefore making calls to the API, setup the base URI, access key and secret key in the configuration.\n\n\nEnd Point\n\n\nTo call API requests to Batchly.net, please send HTTP(S) post requests to:\n\n\nhttps://\ncustomerdomain\n.batchly.net/api/\n\n\n\n\nAuthentication\n\n\n ApiKey \n\n\nA user's unique authentication key string. This authentication key will be shared with your team when the system is configured for your usage. This key is mandatory and needs to be sent as part of every API call. The Key should be sent along in the Request Header as below.\n\n\nAuthorization: ApiKey AccessKey:SecretKey\n\n\n\n\nSupport\n\n\nPlease reach out to \nsupport@batchy.net\n to get your API Keys if you do not have the keys and/or receiving authentication errors.", 
            "title": "API Overview"
        }, 
        {
            "location": "/api/overview/#batchly-rest-api", 
            "text": "Each customer of batchly gets their own customized URL for access. Using the web console, you can generate new set of API access keys.  You need to safe keep the keys.  Batchly API SDK doesn\u2019t send the secret key in any request. Batchly supports two types of API authentication mechanisms -  HMAC which validates calls using request signature and ApiKey based.  Before making calls to the API, setup the base URI, access key and secret key in the configuration.", 
            "title": "Batchly REST API"
        }, 
        {
            "location": "/api/overview/#end-point", 
            "text": "To call API requests to Batchly.net, please send HTTP(S) post requests to:  https:// customerdomain .batchly.net/api/", 
            "title": "End Point"
        }, 
        {
            "location": "/api/overview/#authentication", 
            "text": "ApiKey   A user's unique authentication key string. This authentication key will be shared with your team when the system is configured for your usage. This key is mandatory and needs to be sent as part of every API call. The Key should be sent along in the Request Header as below.  Authorization: ApiKey AccessKey:SecretKey", 
            "title": "Authentication"
        }, 
        {
            "location": "/api/overview/#support", 
            "text": "Please reach out to  support@batchy.net  to get your API Keys if you do not have the keys and/or receiving authentication errors.", 
            "title": "Support"
        }, 
        {
            "location": "/api/definition/", 
            "text": "Batchly API supports the following entities:\n\n\n\n\nJobs\n\n\nRuns\n\n\nProjects\n\n\nAccounts\n\n\n\n\nJobs API\n\n\nJobs are calls to perform various operations on the actual dataset. This call varies from one app to another. Though it is app-specific, each job requires jobname, Project, SLA, input location and output location to be mentioned.\n\n\nBatchly responds immediately with a job ID so your application can track the progress of the job.\n\n\nActions Supported\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGET\n\n\napi/Jobs\n\n\nReturns all the jobs associated with your Batchly account\n\n\n\n\n\n\n\n\nTo run individual jobs, call the API associated with the respective app. \nApp API Documentation\n\n\nRuns API\n\n\nRuns are individual instances of the job in Batchly. Runs API allows you to list and delete the runs in your Batchly account.\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGET\n\n\n/api/Runs\n\n\nGet all the runs associated with your batchly account\n\n\n\n\n\n\nGET\n\n\n/api/Runs/{id}\n\n\nGet the details of the run for the given id\n\n\n\n\n\n\nDELETE\n\n\n/api/Runs/{id}\n\n\nDelete the run details from your account\n\n\n\n\n\n\n\n\nProjects API\n\n\nProjects define the boundary for your jobs to run in. Projects creates a sandbox environment associated with an AWS account, a cloud region within that AWS account and (optionally) associate a virtual private cloud with it.\n\n\nProjects\u2019 calls allow you to create, edit or view projects associated with your Batchly account.\n\n\nActions Supported\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDELETE\n\n\n/api/Projects/{id}\n\n\nDelete the project with the given id\n\n\n\n\n\n\nGET\n\n\n/api/Projects/{id}\n\n\nGet the details of the project for the given id\n\n\n\n\n\n\nPUT\n\n\n/api/Projects/{id}\n\n\nUpdate the project details for the given id\n\n\n\n\n\n\nPOST\n\n\n/api/Projects/Add\n\n\nCreate a new project\n\n\n\n\n\n\nGET\n\n\n/api/Projects\n\n\nGet all the projects associated with your batchly account\n\n\n\n\n\n\n\n\nAccounts API\n\n\nAttach your AWS account to batchly using the accounts API. Accounts calls allow you to create, edit or view AWS accounts associated with your Batchly account.\n\n\nActions Supported\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPOST\n\n\n/api/Accounts/AWS\n\n\nAdd new AWS account to your batchly account\n\n\n\n\n\n\nDELETE\n\n\n/api/Accounts/{id}\n\n\nDelete the AWS account associated with this id\n\n\n\n\n\n\nGET\n\n\n/api/Accounts/{id}\n\n\nGet the AWS account details related to this id\n\n\n\n\n\n\nPUT\n\n\n/api/Accounts/{id}\n\n\nUpdate the details for this id\n\n\n\n\n\n\nGET\n\n\n/api/Accounts\n\n\nGet all the AWS accounts associated with your batchly account", 
            "title": "Definition"
        }, 
        {
            "location": "/api/definition/#jobs-api", 
            "text": "Jobs are calls to perform various operations on the actual dataset. This call varies from one app to another. Though it is app-specific, each job requires jobname, Project, SLA, input location and output location to be mentioned.  Batchly responds immediately with a job ID so your application can track the progress of the job.  Actions Supported     HTTP Method  Endpoint  Description      GET  api/Jobs  Returns all the jobs associated with your Batchly account     To run individual jobs, call the API associated with the respective app.  App API Documentation", 
            "title": "Jobs API"
        }, 
        {
            "location": "/api/definition/#runs-api", 
            "text": "Runs are individual instances of the job in Batchly. Runs API allows you to list and delete the runs in your Batchly account.     HTTP Method  Endpoint  Description      GET  /api/Runs  Get all the runs associated with your batchly account    GET  /api/Runs/{id}  Get the details of the run for the given id    DELETE  /api/Runs/{id}  Delete the run details from your account", 
            "title": "Runs API"
        }, 
        {
            "location": "/api/definition/#projects-api", 
            "text": "Projects define the boundary for your jobs to run in. Projects creates a sandbox environment associated with an AWS account, a cloud region within that AWS account and (optionally) associate a virtual private cloud with it.  Projects\u2019 calls allow you to create, edit or view projects associated with your Batchly account.  Actions Supported     HTTP Method  Endpoint  Description      DELETE  /api/Projects/{id}  Delete the project with the given id    GET  /api/Projects/{id}  Get the details of the project for the given id    PUT  /api/Projects/{id}  Update the project details for the given id    POST  /api/Projects/Add  Create a new project    GET  /api/Projects  Get all the projects associated with your batchly account", 
            "title": "Projects API"
        }, 
        {
            "location": "/api/definition/#accounts-api", 
            "text": "Attach your AWS account to batchly using the accounts API. Accounts calls allow you to create, edit or view AWS accounts associated with your Batchly account.  Actions Supported     HTTP Method  Endpoint  Description      POST  /api/Accounts/AWS  Add new AWS account to your batchly account    DELETE  /api/Accounts/{id}  Delete the AWS account associated with this id    GET  /api/Accounts/{id}  Get the AWS account details related to this id    PUT  /api/Accounts/{id}  Update the details for this id    GET  /api/Accounts  Get all the AWS accounts associated with your batchly account", 
            "title": "Accounts API"
        }, 
        {
            "location": "/api/app-api/", 
            "text": "App Specific APIs\n\n\nBatchly works through Apps. There are marketplace apps such as FFMpeg for video transcoding, and JMeter for load testing. You may also upload your own private app to run in your Batchly account.\n\n\nNevertheless, each app has its own set of APIs to perform the basic operations. The request headers, however, differ based on the app you want to create.\n\n\nCommon Actions Supported By All Apps\n\n\n\n\n\n\n\n\nHTTP Method\n\n\nEndpoint\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPOST\n\n\napi/apps/APP_NAME/{id}/Execute\n\n\nExecute the job\n\n\n\n\n\n\nPOST\n\n\napi/apps/APP_NAME\n\n\nCreate a new job\n\n\n\n\n\n\nPUT\n\n\napi/apps/APP_NAME/{id}\n\n\nEdit a job with updated information.\n\n\n\n\n\n\nGET\n\n\napi/apps/APP_NAME/{id}\n\n\nGet details for a job\n\n\n\n\n\n\nDELETE\n\n\napi/apps/APP_NAME/{id}\n\n\nDelete a job\n\n\n\n\n\n\nGET\n\n\napi/apps/APP_NAME/jobs\n\n\nLists all jobs\n\n\n\n\n\n\n\n\nApp Specific Request headers\n\n\n\n\nFFMpeg - Video Transcoding App\n\n\nRequest Header\n\n\n{\n  \nName\n: \nstring\n,\n  \nProjectId\n: \nstring\n,\n  \nRegion\n: \nstring\n,\n  \nTimeLimit\n: 0,\n  \nProfileId\n: \nstring\n,\n  \nFileNameSuffix\n: \nstring\n,\n  \nSourceLocation\n: \nstring\n,\n  \nDestinationLocation\n: \nstring\n\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nRegion\n\n\nString\n\n\nAWS region where your job will run\n\n\nYes\n\n\n\n\n\n\nTimeLimit\n\n\nInteger\n\n\nTime limit when your job needs to complete\n\n\nYes\n\n\n\n\n\n\nProfileId\n\n\nString\n\n\nCommon video profiles for MP4 and HLS\n\n\nYes\n\n\n\n\n\n\nFileNameSuffix\n\n\nString\n\n\nSuffix to identify the transcoded output file\n\n\nNo\n\n\n\n\n\n\nSourceLocation\n\n\nString\n\n\nLocation where all the input files reside\n\n\nYes\n\n\n\n\n\n\nDestinationLocation\n\n\nString\n\n\nLocation where you want the output files to go to\n\n\nYes\n\n\n\n\n\n\n\n\nExample result\n\n\n{\n  \nRequestId\n: \nstring\n,\n  \nData\n: {\n    \nRuns\n: 0,\n    \nIsScheduled\n: true,\n    \nIsExecuting\n: true,\n    \nEngine\n: \nAll\n,\n    \nAppName\n: \nstring\n,\n    \nIsPrivateApp\n: true,\n    \nId\n: \nstring\n,\n    \nName\n: \nstring\n\n  },\n  \nErrors\n: {},\n  \nContinuationToken\n: \nstring\n\n}\n\n\n\n\n\n\nImageMagick - Image processing App\n\n\nRequest Header\n\n\n{\n  \nName\n: \nstring\n,\n  \nProjectId\n: \nstring\n,\n  \nSourceLocation\n: \nstring\n,\n  \nDestinationLocation\n: \nstring\n,\n  \nRegion\n: \nstring\n,\n  \nTimeLimit\n: 0,\n  \nResize\n: \nstring\n\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nRegion\n\n\nString\n\n\nAWS region where your job will run\n\n\nYes\n\n\n\n\n\n\nTimeLimit\n\n\nInteger\n\n\nTime limit when your job needs to complete\n\n\nYes\n\n\n\n\n\n\nResize\n\n\nString\n\n\nImage height and width to which the image needs to be converted to e.g. 100x100\n\n\nYes\n\n\n\n\n\n\nSourceLocation\n\n\nString\n\n\nLocation where all the input files reside\n\n\nYes\n\n\n\n\n\n\nDestinationLocation\n\n\nString\n\n\nLocation where you want the output files to go to\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n  \nRequestId\n: \nstring\n,\n  \nData\n: {\n    \nRuns\n: 0,\n    \nIsScheduled\n: true,\n    \nIsExecuting\n: true,\n    \nEngine\n: \nAll\n,\n    \nAppName\n: \nstring\n,\n    \nIsPrivateApp\n: true,\n    \nId\n: \nstring\n,\n    \nName\n: \nstring\n\n  },\n  \nErrors\n: {},\n  \nContinuationToken\n: \nstring\n\n}\n\n\n\n\n\n\nTesseract - Optical Character Recognition App\n\n\nRequest Header\n\n\n{\n  \nName\n: \nstring\n,\n  \nProjectId\n: \nstring\n,\n  \nSourceLocation\n: \nstring\n,\n  \nDestinationLocation\n: \nstring\n,\n  \nRegion\n: \nstring\n,\n  \nTimeLimit\n: 0\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nRegion\n\n\nString\n\n\nAWS region where your job will run\n\n\nYes\n\n\n\n\n\n\nTimeLimit\n\n\nInteger\n\n\nTime limit when your job needs to complete\n\n\nYes\n\n\n\n\n\n\nSourceLocation\n\n\nString\n\n\nLocation where all the input files reside\n\n\nYes\n\n\n\n\n\n\nDestinationLocation\n\n\nString\n\n\nLocation where you want the output files to go to\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n  \nRuns\n: 0,\n  \nIsScheduled\n: true,\n  \nIsExecuting\n: true,\n  \nEngine\n: \nAll\n,\n  \nAppName\n: \nstring\n,\n  \nIsPrivateApp\n: true,\n  \nId\n: \nstring\n,\n  \nName\n: \nstring\n\n}\n\n\n\n\n\n\nJMeter - Load Testing App\n\n\nRequest Header\n\n\n{\n  \nName\n: \nstring\n,\n  \nProjectId\n: \nstring\n,\n  \nDestinationLocation\n: \nstring\n,\n  \nTestPlan\n: \nstring\n,\n  \nInstanceType\n: \nstring\n,\n  \nInstanceCount\n: 0\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nTestPlan\n\n\nInteger\n\n\nJMX test plan you want to execture\n\n\nYes\n\n\n\n\n\n\nDestinationLocation\n\n\nString\n\n\nLocation where you want the output files to go to\n\n\nYes\n\n\n\n\n\n\nInstanceType\n\n\nString\n\n\nInstance type you want Batchly to launch\n\n\nYes\n\n\n\n\n\n\nInstanceCount\n\n\nInteger\n\n\nNumber of instances of the InstanceType you want Batchly to launch and manage\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n  \nRequestId\n: \nstring\n,\n  \nData\n: {\n    \nRuns\n: 0,\n    \nIsScheduled\n: true,\n    \nIsExecuting\n: true,\n    \nEngine\n: \nAll\n,\n    \nAppName\n: \nstring\n,\n    \nIsPrivateApp\n: true,\n    \nId\n: \nstring\n,\n    \nName\n: \nstring\n\n  },\n  \nErrors\n: {},\n  \nContinuationToken\n: \nstring\n\n}\n\n\n\n\n\n\nHive App\n\n\nRequest Header\n\n\n{\n  \nName\n: \u201cHive Job\u201d,\n  \nProjectId\n: \u201cP-XXXXX\u201d,\n  \nClusterId\n: \u201cj-XXXXX\u201d,\n  \nScriptLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_SCRIPT\u201d,\n  \nInputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n  \nOutputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n  \nArguments\n: \u201carg1 arg2 arg3\u201d\n  \nActionOnFailure\n: \u201cCONTINUE\u201d\n  \nInstanceTypes\n: [\nc4.large\n, \nm3.large\n]\n}    \n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nClusterId\n\n\nString\n\n\nCluster Id where the job should execute\n\n\nYes\n\n\n\n\n\n\nScriptLocation\n\n\nString\n\n\nS3 location where the script resides\n\n\nYes\n\n\n\n\n\n\nInputLocation\n\n\nString\n\n\nS3 location where the dataset resides\n\n\nNo\n\n\n\n\n\n\nOutputLocation\n\n\nString\n\n\nS3 location where results are written\n\n\nNo\n\n\n\n\n\n\nArguments\n\n\nString\n\n\nAny arguments that the script requires\n\n\nNo\n\n\n\n\n\n\nActionOnFailure\n\n\nString\n\n\nCONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER\n\n\nYes\n\n\n\n\n\n\nInstanceTypes\n\n\nString[]\n\n\nThe instance types to be launched\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n   Id: \nW-XXXXX\n,\n   Name: \nHive Job\n,\n   Runs: 1,\n   IsScheduled: \nFalse\n,\n   IsExecuting: \nTrue | False\n,\n   Engine: \nMapReduce\n\n}   \n\n\n\n\n\n\nPig App\n\n\nRequest Header\n\n\n{\n  \nName\n: \u201cPig Job\u201d,\n  \nProjectId\n: \u201cP-XXXXX\u201d,\n  \nClusterId\n: \u201cj-XXXXX\u201d,\n  \nScriptLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_SCRIPT\u201d,\n  \nInputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n  \nOutputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n  \nArguments\n: \u201carg1 arg2 arg3\u201d\n  \nActionOnFailure\n: \u201cCONTINUE\u201d\n  \nInstanceTypes\n: [\nc4.large\n, \nm3.large\n]\n} \n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nClusterId\n\n\nString\n\n\nCluster Id where the job should execute\n\n\nYes\n\n\n\n\n\n\nScriptLocation\n\n\nString\n\n\nS3 location where the script resides\n\n\nYes\n\n\n\n\n\n\nInputLocation\n\n\nString\n\n\nS3 location where the dataset resides\n\n\nNo\n\n\n\n\n\n\nOutputLocation\n\n\nString\n\n\nS3 location where results are written\n\n\nNo\n\n\n\n\n\n\nArguments\n\n\nString\n\n\nAny arguments that the script requires\n\n\nNo\n\n\n\n\n\n\nActionOnFailure\n\n\nString\n\n\nCONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER\n\n\nYes\n\n\n\n\n\n\nInstanceTypes\n\n\nString[]\n\n\nThe instance types to be launched\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n   Id: \nW-XXXXX\n,\n   Name: \nPig Job\n,\n   Runs: 1,\n   IsScheduled: \nFalse\n,\n   IsExecuting: \nTrue | False\n,\n   Engine: \nMapReduce\n\n}   \n\n\n\n\n\n\nSpark App\n\n\nRequest Header\n\n\n{\n  \nName\n: \u201cSpark Job\u201d,\n  \nProjectId\n: \u201cP-XXXXX\u201d,\n  \nClusterId\n: \u201cj-XXXXX\u201d,\n  \nDeployMode\n: \u201cCluster\u201d,\n  \nSparkSubmitOptions\n: \u201copt1 opt2 opt3\u201d,\n  \nApplicationLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_SPARK_JAR\u201d,\n  \nArguments\n: \u201carg1 arg2 arg3\u201d\n  \nActionOnFailure\n: \u201cCONTINUE\u201d\n  \nInstanceTypes\n: [\nc4.large\n, \nm3.large\n]\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nClusterId\n\n\nString\n\n\nCluster Id where the job should execute\n\n\nYes\n\n\n\n\n\n\nDeployMode\n\n\nInteger\n\n\n1 (for Cluster mode), 2 (for Client mode)\n\n\nYes\n\n\n\n\n\n\nSparkSubmitOptions\n\n\nString\n\n\nSpecify other options for spark-submit\n\n\nNo\n\n\n\n\n\n\nApplicationLocation\n\n\nString\n\n\nS3 location where the Spark jar resides\n\n\nYes\n\n\n\n\n\n\nArguments\n\n\nString\n\n\nAny arguments that the script requires\n\n\nNo\n\n\n\n\n\n\nActionOnFailure\n\n\nString\n\n\nCONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER\n\n\nYes\n\n\n\n\n\n\nInstanceTypes\n\n\nString[]\n\n\nThe instance types to be launched\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n   Id: \nW-XXXXX\n,\n   Name: \nSpark Job\n,\n   Runs: 1,\n   IsScheduled: \nFalse\n,\n   IsExecuting: \nTrue | False\n,\n   Engine: \nMapReduce\n\n}   \n\n\n\n\n\n\nHadoopStreaming App\n\n\nRequest Header\n\n\n{\n  \nName\n: \u201cHadoopStreaming Job\u201d,\n  \nProjectId\n: \u201cP-XXXXX\u201d,\n  \nClusterId\n: \u201cj-XXXXX\u201d,\n  \nMapperLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_MAP_FUNCTION\u201d,\n  \nReducerLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_REDUCE_FUNCTION\u201d,\n  \nInputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n  \nOutputLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n  \nArguments\n: \u201carg1 arg2 arg3\u201d\n  \nActionOnFailure\n: \u201cCONTINUE\u201d\n  \nInstanceTypes\n: [\nc4.large\n, \nm3.large\n]\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nClusterId\n\n\nString\n\n\nCluster Id where the job should execute\n\n\nYes\n\n\n\n\n\n\nMapperLocation\n\n\nString\n\n\nS3 location of the map function or the name of the Hadoop streaming command to run\n\n\nYes\n\n\n\n\n\n\nReducerLocation\n\n\nString\n\n\nS3 location of the reduce function or the name of the Hadoop streaming command to run\n\n\nYes\n\n\n\n\n\n\nInputLocation\n\n\nString\n\n\nS3 location where the dataset resides\n\n\nYes\n\n\n\n\n\n\nOutputLocation\n\n\nString\n\n\nS3 location where the results are written\n\n\nYes\n\n\n\n\n\n\nArguments\n\n\nString\n\n\nAny arguments that the script requires\n\n\nNo\n\n\n\n\n\n\nActionOnFailure\n\n\nString\n\n\nCONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER\n\n\nYes\n\n\n\n\n\n\nInstanceTypes\n\n\nString[]\n\n\nThe instance types to be launched\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n   Id: \nW-XXXXX\n,\n   Name: \nHadoopStreaming Job\n,\n   Runs: 1,\n   IsScheduled: \nFalse\n,\n   IsExecuting: \nTrue | False\n,\n   Engine: \nMapReduce\n\n}   \n\n\n\n\n\n\nHadoopCustom App\n\n\nRequest Header\n\n\n{\n  \nName\n: \u201cHadoop Job\u201d,\n  \nProjectId\n: \u201cP-XXXXX\u201d,\n  \nClusterId\n: \u201cj-XXXXX\u201d,\n  \nJarLocation\n: \u201cs3://BUCKET_NAME/PATH_TO_JAR\u201d,\n  \nArguments\n: \u201carg1 arg2 arg3\u201d\n  \nActionOnFailure\n: \u201cCONTINUE\u201d\n  \nInstanceTypes\n: [\nc4.large\n, \nm3.large\n]\n}\n\n\n\n\n\n\n\n\n\n\nField Name\n\n\nData Type\n\n\nDescription\n\n\nMandatory\n\n\n\n\n\n\n\n\n\n\nName\n\n\nString\n\n\nName for the job\n\n\nYes\n\n\n\n\n\n\nProjectId\n\n\nString\n\n\nIdentifier of the Project under which the job should execute\n\n\nYes\n\n\n\n\n\n\nClusterId\n\n\nString\n\n\nCluster Id where the job should execute\n\n\nYes\n\n\n\n\n\n\nJarLocation\n\n\nString\n\n\nA path into S3 or a fully qualified java class in the classpath\n\n\nYes\n\n\n\n\n\n\nArguments\n\n\nString\n\n\nAny arguments that the script requires\n\n\nNo\n\n\n\n\n\n\nActionOnFailure\n\n\nString\n\n\nCONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER\n\n\nYes\n\n\n\n\n\n\nInstanceTypes\n\n\nString\n\n\nThe instance types to be launched\n\n\nYes\n\n\n\n\n\n\n\n\nSuccess Response\n\n\n{\n   Id: \nW-XXXXX\n,\n   Name: \nHadoop Job\n,\n   Runs: 1,\n   IsScheduled: \nFalse\n,\n   IsExecuting: \nTrue | False\n,\n   Engine: \nMapReduce\n\n}", 
            "title": "App Specific APIs"
        }, 
        {
            "location": "/api/app-api/#app-specific-apis", 
            "text": "Batchly works through Apps. There are marketplace apps such as FFMpeg for video transcoding, and JMeter for load testing. You may also upload your own private app to run in your Batchly account.  Nevertheless, each app has its own set of APIs to perform the basic operations. The request headers, however, differ based on the app you want to create.  Common Actions Supported By All Apps     HTTP Method  Endpoint  Description      POST  api/apps/APP_NAME/{id}/Execute  Execute the job    POST  api/apps/APP_NAME  Create a new job    PUT  api/apps/APP_NAME/{id}  Edit a job with updated information.    GET  api/apps/APP_NAME/{id}  Get details for a job    DELETE  api/apps/APP_NAME/{id}  Delete a job    GET  api/apps/APP_NAME/jobs  Lists all jobs", 
            "title": "App Specific APIs"
        }, 
        {
            "location": "/api/app-api/#app-specific-request-headers", 
            "text": "", 
            "title": "App Specific Request headers"
        }, 
        {
            "location": "/api/app-api/#ffmpeg-video-transcoding-app", 
            "text": "Request Header  {\n   Name :  string ,\n   ProjectId :  string ,\n   Region :  string ,\n   TimeLimit : 0,\n   ProfileId :  string ,\n   FileNameSuffix :  string ,\n   SourceLocation :  string ,\n   DestinationLocation :  string \n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    Region  String  AWS region where your job will run  Yes    TimeLimit  Integer  Time limit when your job needs to complete  Yes    ProfileId  String  Common video profiles for MP4 and HLS  Yes    FileNameSuffix  String  Suffix to identify the transcoded output file  No    SourceLocation  String  Location where all the input files reside  Yes    DestinationLocation  String  Location where you want the output files to go to  Yes     Example result  {\n   RequestId :  string ,\n   Data : {\n     Runs : 0,\n     IsScheduled : true,\n     IsExecuting : true,\n     Engine :  All ,\n     AppName :  string ,\n     IsPrivateApp : true,\n     Id :  string ,\n     Name :  string \n  },\n   Errors : {},\n   ContinuationToken :  string \n}", 
            "title": "FFMpeg - Video Transcoding App"
        }, 
        {
            "location": "/api/app-api/#imagemagick-image-processing-app", 
            "text": "Request Header  {\n   Name :  string ,\n   ProjectId :  string ,\n   SourceLocation :  string ,\n   DestinationLocation :  string ,\n   Region :  string ,\n   TimeLimit : 0,\n   Resize :  string \n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    Region  String  AWS region where your job will run  Yes    TimeLimit  Integer  Time limit when your job needs to complete  Yes    Resize  String  Image height and width to which the image needs to be converted to e.g. 100x100  Yes    SourceLocation  String  Location where all the input files reside  Yes    DestinationLocation  String  Location where you want the output files to go to  Yes     Success Response  {\n   RequestId :  string ,\n   Data : {\n     Runs : 0,\n     IsScheduled : true,\n     IsExecuting : true,\n     Engine :  All ,\n     AppName :  string ,\n     IsPrivateApp : true,\n     Id :  string ,\n     Name :  string \n  },\n   Errors : {},\n   ContinuationToken :  string \n}", 
            "title": "ImageMagick - Image processing App"
        }, 
        {
            "location": "/api/app-api/#tesseract-optical-character-recognition-app", 
            "text": "Request Header  {\n   Name :  string ,\n   ProjectId :  string ,\n   SourceLocation :  string ,\n   DestinationLocation :  string ,\n   Region :  string ,\n   TimeLimit : 0\n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    Region  String  AWS region where your job will run  Yes    TimeLimit  Integer  Time limit when your job needs to complete  Yes    SourceLocation  String  Location where all the input files reside  Yes    DestinationLocation  String  Location where you want the output files to go to  Yes     Success Response  {\n   Runs : 0,\n   IsScheduled : true,\n   IsExecuting : true,\n   Engine :  All ,\n   AppName :  string ,\n   IsPrivateApp : true,\n   Id :  string ,\n   Name :  string \n}", 
            "title": "Tesseract - Optical Character Recognition App"
        }, 
        {
            "location": "/api/app-api/#jmeter-load-testing-app", 
            "text": "Request Header  {\n   Name :  string ,\n   ProjectId :  string ,\n   DestinationLocation :  string ,\n   TestPlan :  string ,\n   InstanceType :  string ,\n   InstanceCount : 0\n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    TestPlan  Integer  JMX test plan you want to execture  Yes    DestinationLocation  String  Location where you want the output files to go to  Yes    InstanceType  String  Instance type you want Batchly to launch  Yes    InstanceCount  Integer  Number of instances of the InstanceType you want Batchly to launch and manage  Yes     Success Response  {\n   RequestId :  string ,\n   Data : {\n     Runs : 0,\n     IsScheduled : true,\n     IsExecuting : true,\n     Engine :  All ,\n     AppName :  string ,\n     IsPrivateApp : true,\n     Id :  string ,\n     Name :  string \n  },\n   Errors : {},\n   ContinuationToken :  string \n}", 
            "title": "JMeter - Load Testing App"
        }, 
        {
            "location": "/api/app-api/#hive-app", 
            "text": "Request Header  {\n   Name : \u201cHive Job\u201d,\n   ProjectId : \u201cP-XXXXX\u201d,\n   ClusterId : \u201cj-XXXXX\u201d,\n   ScriptLocation : \u201cs3://BUCKET_NAME/PATH_TO_SCRIPT\u201d,\n   InputLocation : \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n   OutputLocation : \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n   Arguments : \u201carg1 arg2 arg3\u201d\n   ActionOnFailure : \u201cCONTINUE\u201d\n   InstanceTypes : [ c4.large ,  m3.large ]\n}         Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    ClusterId  String  Cluster Id where the job should execute  Yes    ScriptLocation  String  S3 location where the script resides  Yes    InputLocation  String  S3 location where the dataset resides  No    OutputLocation  String  S3 location where results are written  No    Arguments  String  Any arguments that the script requires  No    ActionOnFailure  String  CONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER  Yes    InstanceTypes  String[]  The instance types to be launched  Yes     Success Response  {\n   Id:  W-XXXXX ,\n   Name:  Hive Job ,\n   Runs: 1,\n   IsScheduled:  False ,\n   IsExecuting:  True | False ,\n   Engine:  MapReduce \n}", 
            "title": "Hive App"
        }, 
        {
            "location": "/api/app-api/#pig-app", 
            "text": "Request Header  {\n   Name : \u201cPig Job\u201d,\n   ProjectId : \u201cP-XXXXX\u201d,\n   ClusterId : \u201cj-XXXXX\u201d,\n   ScriptLocation : \u201cs3://BUCKET_NAME/PATH_TO_SCRIPT\u201d,\n   InputLocation : \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n   OutputLocation : \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n   Arguments : \u201carg1 arg2 arg3\u201d\n   ActionOnFailure : \u201cCONTINUE\u201d\n   InstanceTypes : [ c4.large ,  m3.large ]\n}      Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    ClusterId  String  Cluster Id where the job should execute  Yes    ScriptLocation  String  S3 location where the script resides  Yes    InputLocation  String  S3 location where the dataset resides  No    OutputLocation  String  S3 location where results are written  No    Arguments  String  Any arguments that the script requires  No    ActionOnFailure  String  CONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER  Yes    InstanceTypes  String[]  The instance types to be launched  Yes     Success Response  {\n   Id:  W-XXXXX ,\n   Name:  Pig Job ,\n   Runs: 1,\n   IsScheduled:  False ,\n   IsExecuting:  True | False ,\n   Engine:  MapReduce \n}", 
            "title": "Pig App"
        }, 
        {
            "location": "/api/app-api/#spark-app", 
            "text": "Request Header  {\n   Name : \u201cSpark Job\u201d,\n   ProjectId : \u201cP-XXXXX\u201d,\n   ClusterId : \u201cj-XXXXX\u201d,\n   DeployMode : \u201cCluster\u201d,\n   SparkSubmitOptions : \u201copt1 opt2 opt3\u201d,\n   ApplicationLocation : \u201cs3://BUCKET_NAME/PATH_TO_SPARK_JAR\u201d,\n   Arguments : \u201carg1 arg2 arg3\u201d\n   ActionOnFailure : \u201cCONTINUE\u201d\n   InstanceTypes : [ c4.large ,  m3.large ]\n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    ClusterId  String  Cluster Id where the job should execute  Yes    DeployMode  Integer  1 (for Cluster mode), 2 (for Client mode)  Yes    SparkSubmitOptions  String  Specify other options for spark-submit  No    ApplicationLocation  String  S3 location where the Spark jar resides  Yes    Arguments  String  Any arguments that the script requires  No    ActionOnFailure  String  CONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER  Yes    InstanceTypes  String[]  The instance types to be launched  Yes     Success Response  {\n   Id:  W-XXXXX ,\n   Name:  Spark Job ,\n   Runs: 1,\n   IsScheduled:  False ,\n   IsExecuting:  True | False ,\n   Engine:  MapReduce \n}", 
            "title": "Spark App"
        }, 
        {
            "location": "/api/app-api/#hadoopstreaming-app", 
            "text": "Request Header  {\n   Name : \u201cHadoopStreaming Job\u201d,\n   ProjectId : \u201cP-XXXXX\u201d,\n   ClusterId : \u201cj-XXXXX\u201d,\n   MapperLocation : \u201cs3://BUCKET_NAME/PATH_TO_MAP_FUNCTION\u201d,\n   ReducerLocation : \u201cs3://BUCKET_NAME/PATH_TO_REDUCE_FUNCTION\u201d,\n   InputLocation : \u201cs3://BUCKET_NAME/PATH_TO_DATASET\u201d,\n   OutputLocation : \u201cs3://BUCKET_NAME/PATH_TO_OUTPUT_LOCATION\u201d,\n   Arguments : \u201carg1 arg2 arg3\u201d\n   ActionOnFailure : \u201cCONTINUE\u201d\n   InstanceTypes : [ c4.large ,  m3.large ]\n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    ClusterId  String  Cluster Id where the job should execute  Yes    MapperLocation  String  S3 location of the map function or the name of the Hadoop streaming command to run  Yes    ReducerLocation  String  S3 location of the reduce function or the name of the Hadoop streaming command to run  Yes    InputLocation  String  S3 location where the dataset resides  Yes    OutputLocation  String  S3 location where the results are written  Yes    Arguments  String  Any arguments that the script requires  No    ActionOnFailure  String  CONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER  Yes    InstanceTypes  String[]  The instance types to be launched  Yes     Success Response  {\n   Id:  W-XXXXX ,\n   Name:  HadoopStreaming Job ,\n   Runs: 1,\n   IsScheduled:  False ,\n   IsExecuting:  True | False ,\n   Engine:  MapReduce \n}", 
            "title": "HadoopStreaming App"
        }, 
        {
            "location": "/api/app-api/#hadoopcustom-app", 
            "text": "Request Header  {\n   Name : \u201cHadoop Job\u201d,\n   ProjectId : \u201cP-XXXXX\u201d,\n   ClusterId : \u201cj-XXXXX\u201d,\n   JarLocation : \u201cs3://BUCKET_NAME/PATH_TO_JAR\u201d,\n   Arguments : \u201carg1 arg2 arg3\u201d\n   ActionOnFailure : \u201cCONTINUE\u201d\n   InstanceTypes : [ c4.large ,  m3.large ]\n}     Field Name  Data Type  Description  Mandatory      Name  String  Name for the job  Yes    ProjectId  String  Identifier of the Project under which the job should execute  Yes    ClusterId  String  Cluster Id where the job should execute  Yes    JarLocation  String  A path into S3 or a fully qualified java class in the classpath  Yes    Arguments  String  Any arguments that the script requires  No    ActionOnFailure  String  CONTINUE, CANCEL_AND_WAIT, TERMINATE_CLUSTER  Yes    InstanceTypes  String  The instance types to be launched  Yes     Success Response  {\n   Id:  W-XXXXX ,\n   Name:  Hadoop Job ,\n   Runs: 1,\n   IsScheduled:  False ,\n   IsExecuting:  True | False ,\n   Engine:  MapReduce \n}", 
            "title": "HadoopCustom App"
        }
    ]
}